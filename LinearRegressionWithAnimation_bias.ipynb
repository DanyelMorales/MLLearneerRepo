{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJr2BtCXQYp8MEhQZvX7KM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanyelMorales/MLLearneerRepo/blob/main/LinearRegressionWithAnimation_bias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "blepHTTmuPmq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "8267acde-8c29-4c1c-c48d-a160f57c2139"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import seaborn as sns\n",
        "\n",
        "plt.ion()\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "X,Y =np.loadtxt(\"https://raw.githubusercontent.com/DanyelMorales/MLLearneerRepo/main/dataset/pizza.txt\",skiprows=1, unpack=True)\n",
        "\n",
        "ax.plot(X,Y, \"bo\")\n",
        "\n",
        "def graph(X1,Y1, id):\n",
        "    ax.set_xlim(0, id)\n",
        "    ax.cla()\n",
        "    ax.plot(X1,Y1, \"bo\")\n",
        "    line1, = ax.plot(X1,X1, 'b-')\n",
        "    display(fig)\n",
        "    clear_output(wait = True)\n",
        "    plt.pause(0.5)\n",
        "    return line1\n",
        "\n",
        "\n",
        "def updateLine(X1,Y1,theLine):\n",
        "    theLine.set_ydata(Y1)\n",
        "    theLine.set_xdata(X1)\n",
        "    display(fig)\n",
        "    clear_output(wait = True)\n",
        "    #plt.pause(0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MsXmVcG-0kHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showHistory(X,Y,historyW, step=2):\n",
        "  theLine=graph(X,Y,1)\n",
        "  updateLineL = lambda w :  updateLine(X,predict(X,w[0],w[1]),theLine)\n",
        "  for i in range(0,len(historyW),step):\n",
        "    w=historyW[i]\n",
        "    updateLineL(w)\n",
        "  updateLineL(historyW[-1])\n",
        "\n",
        "def predict(X, w,b):\n",
        "  return X * w + b\n",
        "\n",
        "def loss(X,Y,w,b):\n",
        "  error = predict(X,w,b)-Y\n",
        "  squared_error = error ** 2\n",
        "  return np.average(squared_error)\n",
        "\n",
        "def train(iterations, X,Y, lr):\n",
        "  w=b=0\n",
        "  historyW=[]\n",
        "  for i in range(iterations):\n",
        "    historyW.append([w,b])\n",
        "    tmpLoss=loss(X,Y,w,b)\n",
        "    print(f\"\\n iteration={i} bias={b} loss={tmpLoss}\")\n",
        "    if(loss(X,Y,w+lr,b)<tmpLoss):\n",
        "      w+=lr\n",
        "    elif(loss(X,Y,w-lr,b)<tmpLoss):\n",
        "      w-=lr\n",
        "    elif(loss(X,Y,w,b+lr)<tmpLoss):\n",
        "      b+=lr\n",
        "    elif(loss(X,Y,w,b-lr)<tmpLoss):\n",
        "      b-=lr\n",
        "    else:\n",
        "      return [w,b, historyW]\n",
        "  raise Exception(f\"cannot convey after {iterations} iterations\")\n",
        "\n",
        "w,b, history =train(20000, X,Y, 0.01)\n",
        "print(f\"\\nw={w}, b={b}\")\n",
        "print(f\"Prediction: x=20, Y=>{predict(20, w,b)}\")\n"
      ],
      "metadata": {
        "id": "8PAq5GGAQvqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92fb1c97-527b-4211-b08f-c045b2892be8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " iteration=0 bias=0 loss=812.8666666666667\n",
            "\n",
            " iteration=1 bias=0 loss=804.8205466666666\n",
            "\n",
            " iteration=2 bias=0 loss=796.8181866666666\n",
            "\n",
            " iteration=3 bias=0 loss=788.8595866666668\n",
            "\n",
            " iteration=4 bias=0 loss=780.9447466666668\n",
            "\n",
            " iteration=5 bias=0 loss=773.0736666666668\n",
            "\n",
            " iteration=6 bias=0 loss=765.2463466666667\n",
            "\n",
            " iteration=7 bias=0 loss=757.4627866666666\n",
            "\n",
            " iteration=8 bias=0 loss=749.7229866666669\n",
            "\n",
            " iteration=9 bias=0 loss=742.0269466666667\n",
            "\n",
            " iteration=10 bias=0 loss=734.3746666666665\n",
            "\n",
            " iteration=11 bias=0 loss=726.7661466666667\n",
            "\n",
            " iteration=12 bias=0 loss=719.2013866666668\n",
            "\n",
            " iteration=13 bias=0 loss=711.6803866666667\n",
            "\n",
            " iteration=14 bias=0 loss=704.2031466666667\n",
            "\n",
            " iteration=15 bias=0 loss=696.7696666666667\n",
            "\n",
            " iteration=16 bias=0 loss=689.3799466666668\n",
            "\n",
            " iteration=17 bias=0 loss=682.0339866666666\n",
            "\n",
            " iteration=18 bias=0 loss=674.7317866666667\n",
            "\n",
            " iteration=19 bias=0 loss=667.4733466666668\n",
            "\n",
            " iteration=20 bias=0 loss=660.2586666666667\n",
            "\n",
            " iteration=21 bias=0 loss=653.0877466666666\n",
            "\n",
            " iteration=22 bias=0 loss=645.9605866666667\n",
            "\n",
            " iteration=23 bias=0 loss=638.8771866666665\n",
            "\n",
            " iteration=24 bias=0 loss=631.8375466666665\n",
            "\n",
            " iteration=25 bias=0 loss=624.8416666666667\n",
            "\n",
            " iteration=26 bias=0 loss=617.8895466666668\n",
            "\n",
            " iteration=27 bias=0 loss=610.9811866666666\n",
            "\n",
            " iteration=28 bias=0 loss=604.1165866666666\n",
            "\n",
            " iteration=29 bias=0 loss=597.2957466666664\n",
            "\n",
            " iteration=30 bias=0 loss=590.5186666666666\n",
            "\n",
            " iteration=31 bias=0 loss=583.7853466666667\n",
            "\n",
            " iteration=32 bias=0 loss=577.0957866666666\n",
            "\n",
            " iteration=33 bias=0 loss=570.4499866666666\n",
            "\n",
            " iteration=34 bias=0 loss=563.8479466666666\n",
            "\n",
            " iteration=35 bias=0 loss=557.2896666666664\n",
            "\n",
            " iteration=36 bias=0 loss=550.7751466666665\n",
            "\n",
            " iteration=37 bias=0 loss=544.3043866666665\n",
            "\n",
            " iteration=38 bias=0 loss=537.8773866666666\n",
            "\n",
            " iteration=39 bias=0 loss=531.4941466666666\n",
            "\n",
            " iteration=40 bias=0 loss=525.1546666666666\n",
            "\n",
            " iteration=41 bias=0 loss=518.8589466666666\n",
            "\n",
            " iteration=42 bias=0 loss=512.6069866666666\n",
            "\n",
            " iteration=43 bias=0 loss=506.3987866666665\n",
            "\n",
            " iteration=44 bias=0 loss=500.2343466666667\n",
            "\n",
            " iteration=45 bias=0 loss=494.1136666666665\n",
            "\n",
            " iteration=46 bias=0 loss=488.03674666666655\n",
            "\n",
            " iteration=47 bias=0 loss=482.0035866666665\n",
            "\n",
            " iteration=48 bias=0 loss=476.01418666666655\n",
            "\n",
            " iteration=49 bias=0 loss=470.06854666666646\n",
            "\n",
            " iteration=50 bias=0 loss=464.1666666666666\n",
            "\n",
            " iteration=51 bias=0 loss=458.30854666666653\n",
            "\n",
            " iteration=52 bias=0 loss=452.4941866666665\n",
            "\n",
            " iteration=53 bias=0 loss=446.72358666666656\n",
            "\n",
            " iteration=54 bias=0 loss=440.9967466666665\n",
            "\n",
            " iteration=55 bias=0 loss=435.3136666666665\n",
            "\n",
            " iteration=56 bias=0 loss=429.67434666666645\n",
            "\n",
            " iteration=57 bias=0 loss=424.0787866666666\n",
            "\n",
            " iteration=58 bias=0 loss=418.5269866666666\n",
            "\n",
            " iteration=59 bias=0 loss=413.01894666666647\n",
            "\n",
            " iteration=60 bias=0 loss=407.5546666666665\n",
            "\n",
            " iteration=61 bias=0 loss=402.1341466666665\n",
            "\n",
            " iteration=62 bias=0 loss=396.7573866666665\n",
            "\n",
            " iteration=63 bias=0 loss=391.4243866666665\n",
            "\n",
            " iteration=64 bias=0 loss=386.1351466666665\n",
            "\n",
            " iteration=65 bias=0 loss=380.8896666666664\n",
            "\n",
            " iteration=66 bias=0 loss=375.68794666666656\n",
            "\n",
            " iteration=67 bias=0 loss=370.52998666666645\n",
            "\n",
            " iteration=68 bias=0 loss=365.41578666666646\n",
            "\n",
            " iteration=69 bias=0 loss=360.3453466666664\n",
            "\n",
            " iteration=70 bias=0 loss=355.3186666666664\n",
            "\n",
            " iteration=71 bias=0 loss=350.3357466666665\n",
            "\n",
            " iteration=72 bias=0 loss=345.39658666666645\n",
            "\n",
            " iteration=73 bias=0 loss=340.50118666666646\n",
            "\n",
            " iteration=74 bias=0 loss=335.64954666666654\n",
            "\n",
            " iteration=75 bias=0 loss=330.84166666666647\n",
            "\n",
            " iteration=76 bias=0 loss=326.0775466666664\n",
            "\n",
            " iteration=77 bias=0 loss=321.35718666666645\n",
            "\n",
            " iteration=78 bias=0 loss=316.6805866666665\n",
            "\n",
            " iteration=79 bias=0 loss=312.0477466666665\n",
            "\n",
            " iteration=80 bias=0 loss=307.45866666666643\n",
            "\n",
            " iteration=81 bias=0 loss=302.9133466666663\n",
            "\n",
            " iteration=82 bias=0 loss=298.41178666666644\n",
            "\n",
            " iteration=83 bias=0 loss=293.9539866666664\n",
            "\n",
            " iteration=84 bias=0 loss=289.5399466666665\n",
            "\n",
            " iteration=85 bias=0 loss=285.16966666666644\n",
            "\n",
            " iteration=86 bias=0 loss=280.8431466666664\n",
            "\n",
            " iteration=87 bias=0 loss=276.56038666666643\n",
            "\n",
            " iteration=88 bias=0 loss=272.32138666666646\n",
            "\n",
            " iteration=89 bias=0 loss=268.12614666666644\n",
            "\n",
            " iteration=90 bias=0 loss=263.97466666666645\n",
            "\n",
            " iteration=91 bias=0 loss=259.8669466666664\n",
            "\n",
            " iteration=92 bias=0 loss=255.80298666666644\n",
            "\n",
            " iteration=93 bias=0 loss=251.78278666666645\n",
            "\n",
            " iteration=94 bias=0 loss=247.8063466666664\n",
            "\n",
            " iteration=95 bias=0 loss=243.87366666666645\n",
            "\n",
            " iteration=96 bias=0 loss=239.98474666666644\n",
            "\n",
            " iteration=97 bias=0 loss=236.1395866666664\n",
            "\n",
            " iteration=98 bias=0 loss=232.33818666666647\n",
            "\n",
            " iteration=99 bias=0 loss=228.58054666666644\n",
            "\n",
            " iteration=100 bias=0 loss=224.86666666666648\n",
            "\n",
            " iteration=101 bias=0 loss=221.19654666666642\n",
            "\n",
            " iteration=102 bias=0 loss=217.57018666666642\n",
            "\n",
            " iteration=103 bias=0 loss=213.98758666666643\n",
            "\n",
            " iteration=104 bias=0 loss=210.4487466666664\n",
            "\n",
            " iteration=105 bias=0 loss=206.95366666666646\n",
            "\n",
            " iteration=106 bias=0 loss=203.50234666666645\n",
            "\n",
            " iteration=107 bias=0 loss=200.0947866666664\n",
            "\n",
            " iteration=108 bias=0 loss=196.73098666666644\n",
            "\n",
            " iteration=109 bias=0 loss=193.41094666666643\n",
            "\n",
            " iteration=110 bias=0 loss=190.13466666666642\n",
            "\n",
            " iteration=111 bias=0 loss=186.9021466666664\n",
            "\n",
            " iteration=112 bias=0 loss=183.71338666666642\n",
            "\n",
            " iteration=113 bias=0 loss=180.56838666666644\n",
            "\n",
            " iteration=114 bias=0 loss=177.46714666666642\n",
            "\n",
            " iteration=115 bias=0 loss=174.40966666666645\n",
            "\n",
            " iteration=116 bias=0 loss=171.39594666666642\n",
            "\n",
            " iteration=117 bias=0 loss=168.42598666666646\n",
            "\n",
            " iteration=118 bias=0 loss=165.4997866666664\n",
            "\n",
            " iteration=119 bias=0 loss=162.6173466666664\n",
            "\n",
            " iteration=120 bias=0 loss=159.77866666666648\n",
            "\n",
            " iteration=121 bias=0 loss=156.98374666666643\n",
            "\n",
            " iteration=122 bias=0 loss=154.23258666666644\n",
            "\n",
            " iteration=123 bias=0 loss=151.5251866666664\n",
            "\n",
            " iteration=124 bias=0 loss=148.86154666666647\n",
            "\n",
            " iteration=125 bias=0 loss=146.24166666666645\n",
            "\n",
            " iteration=126 bias=0 loss=143.66554666666644\n",
            "\n",
            " iteration=127 bias=0 loss=141.13318666666643\n",
            "\n",
            " iteration=128 bias=0 loss=138.64458666666644\n",
            "\n",
            " iteration=129 bias=0 loss=136.19974666666647\n",
            "\n",
            " iteration=130 bias=0 loss=133.79866666666646\n",
            "\n",
            " iteration=131 bias=0 loss=131.44134666666645\n",
            "\n",
            " iteration=132 bias=0 loss=129.12778666666648\n",
            "\n",
            " iteration=133 bias=0 loss=126.85798666666645\n",
            "\n",
            " iteration=134 bias=0 loss=124.63194666666645\n",
            "\n",
            " iteration=135 bias=0 loss=122.44966666666646\n",
            "\n",
            " iteration=136 bias=0 loss=120.31114666666647\n",
            "\n",
            " iteration=137 bias=0 loss=118.21638666666648\n",
            "\n",
            " iteration=138 bias=0 loss=116.16538666666646\n",
            "\n",
            " iteration=139 bias=0 loss=114.15814666666647\n",
            "\n",
            " iteration=140 bias=0 loss=112.19466666666646\n",
            "\n",
            " iteration=141 bias=0 loss=110.27494666666648\n",
            "\n",
            " iteration=142 bias=0 loss=108.39898666666645\n",
            "\n",
            " iteration=143 bias=0 loss=106.56678666666647\n",
            "\n",
            " iteration=144 bias=0 loss=104.77834666666648\n",
            "\n",
            " iteration=145 bias=0 loss=103.03366666666648\n",
            "\n",
            " iteration=146 bias=0 loss=101.33274666666648\n",
            "\n",
            " iteration=147 bias=0 loss=99.6755866666665\n",
            "\n",
            " iteration=148 bias=0 loss=98.06218666666649\n",
            "\n",
            " iteration=149 bias=0 loss=96.49254666666651\n",
            "\n",
            " iteration=150 bias=0 loss=94.96666666666648\n",
            "\n",
            " iteration=151 bias=0 loss=93.4845466666665\n",
            "\n",
            " iteration=152 bias=0 loss=92.04618666666649\n",
            "\n",
            " iteration=153 bias=0 loss=90.65158666666652\n",
            "\n",
            " iteration=154 bias=0 loss=89.30074666666656\n",
            "\n",
            " iteration=155 bias=0 loss=87.99366666666653\n",
            "\n",
            " iteration=156 bias=0 loss=86.7303466666665\n",
            "\n",
            " iteration=157 bias=0 loss=85.51078666666652\n",
            "\n",
            " iteration=158 bias=0 loss=84.33498666666654\n",
            "\n",
            " iteration=159 bias=0 loss=83.20294666666652\n",
            "\n",
            " iteration=160 bias=0 loss=82.11466666666658\n",
            "\n",
            " iteration=161 bias=0 loss=81.07014666666653\n",
            "\n",
            " iteration=162 bias=0 loss=80.06938666666656\n",
            "\n",
            " iteration=163 bias=0 loss=79.11238666666654\n",
            "\n",
            " iteration=164 bias=0 loss=78.19914666666655\n",
            "\n",
            " iteration=165 bias=0 loss=77.32966666666657\n",
            "\n",
            " iteration=166 bias=0 loss=76.50394666666658\n",
            "\n",
            " iteration=167 bias=0 loss=75.72198666666654\n",
            "\n",
            " iteration=168 bias=0 loss=74.9837866666666\n",
            "\n",
            " iteration=169 bias=0 loss=74.28934666666656\n",
            "\n",
            " iteration=170 bias=0 loss=73.63866666666657\n",
            "\n",
            " iteration=171 bias=0 loss=73.03174666666659\n",
            "\n",
            " iteration=172 bias=0 loss=72.4685866666666\n",
            "\n",
            " iteration=173 bias=0 loss=71.94918666666659\n",
            "\n",
            " iteration=174 bias=0 loss=71.47354666666664\n",
            "\n",
            " iteration=175 bias=0 loss=71.04166666666661\n",
            "\n",
            " iteration=176 bias=0 loss=70.65354666666663\n",
            "\n",
            " iteration=177 bias=0 loss=70.30918666666663\n",
            "\n",
            " iteration=178 bias=0 loss=70.00858666666663\n",
            "\n",
            " iteration=179 bias=0 loss=69.75174666666662\n",
            "\n",
            " iteration=180 bias=0 loss=69.53866666666661\n",
            "\n",
            " iteration=181 bias=0 loss=69.36934666666664\n",
            "\n",
            " iteration=182 bias=0 loss=69.24378666666665\n",
            "\n",
            " iteration=183 bias=0 loss=69.16198666666668\n",
            "\n",
            " iteration=184 bias=0 loss=69.12394666666667\n",
            "\n",
            " iteration=185 bias=0.01 loss=69.05284666666665\n",
            "\n",
            " iteration=186 bias=0.02 loss=68.98194666666669\n",
            "\n",
            " iteration=187 bias=0.03 loss=68.91124666666668\n",
            "\n",
            " iteration=188 bias=0.04 loss=68.84074666666669\n",
            "\n",
            " iteration=189 bias=0.05 loss=68.77044666666667\n",
            "\n",
            " iteration=190 bias=0.060000000000000005 loss=68.70034666666669\n",
            "\n",
            " iteration=191 bias=0.07 loss=68.63044666666669\n",
            "\n",
            " iteration=192 bias=0.08 loss=68.56074666666667\n",
            "\n",
            " iteration=193 bias=0.09 loss=68.49124666666668\n",
            "\n",
            " iteration=194 bias=0.09999999999999999 loss=68.42194666666667\n",
            "\n",
            " iteration=195 bias=0.10999999999999999 loss=68.35284666666666\n",
            "\n",
            " iteration=196 bias=0.11999999999999998 loss=68.28394666666665\n",
            "\n",
            " iteration=197 bias=0.12999999999999998 loss=68.21524666666667\n",
            "\n",
            " iteration=198 bias=0.13999999999999999 loss=68.14674666666666\n",
            "\n",
            " iteration=199 bias=0.15 loss=68.07844666666668\n",
            "\n",
            " iteration=200 bias=0.16 loss=68.01034666666666\n",
            "\n",
            " iteration=201 bias=0.16 loss=68.00785333333332\n",
            "\n",
            " iteration=202 bias=0.17 loss=67.93742\n",
            "\n",
            " iteration=203 bias=0.18000000000000002 loss=67.86718666666667\n",
            "\n",
            " iteration=204 bias=0.19000000000000003 loss=67.79715333333333\n",
            "\n",
            " iteration=205 bias=0.20000000000000004 loss=67.72731999999999\n",
            "\n",
            " iteration=206 bias=0.21000000000000005 loss=67.65768666666665\n",
            "\n",
            " iteration=207 bias=0.22000000000000006 loss=67.58825333333333\n",
            "\n",
            " iteration=208 bias=0.23000000000000007 loss=67.51901999999998\n",
            "\n",
            " iteration=209 bias=0.24000000000000007 loss=67.44998666666667\n",
            "\n",
            " iteration=210 bias=0.25000000000000006 loss=67.38115333333332\n",
            "\n",
            " iteration=211 bias=0.26000000000000006 loss=67.31251999999999\n",
            "\n",
            " iteration=212 bias=0.2700000000000001 loss=67.24408666666668\n",
            "\n",
            " iteration=213 bias=0.2800000000000001 loss=67.17585333333334\n",
            "\n",
            " iteration=214 bias=0.2900000000000001 loss=67.10781999999999\n",
            "\n",
            " iteration=215 bias=0.3000000000000001 loss=67.03998666666665\n",
            "\n",
            " iteration=216 bias=0.3100000000000001 loss=66.97235333333334\n",
            "\n",
            " iteration=217 bias=0.3200000000000001 loss=66.90491999999999\n",
            "\n",
            " iteration=218 bias=0.3300000000000001 loss=66.83768666666666\n",
            "\n",
            " iteration=219 bias=0.3300000000000001 loss=66.83588666666664\n",
            "\n",
            " iteration=220 bias=0.34000000000000014 loss=66.76632000000001\n",
            "\n",
            " iteration=221 bias=0.35000000000000014 loss=66.69695333333333\n",
            "\n",
            " iteration=222 bias=0.36000000000000015 loss=66.62778666666667\n",
            "\n",
            " iteration=223 bias=0.37000000000000016 loss=66.55881999999998\n",
            "\n",
            " iteration=224 bias=0.38000000000000017 loss=66.49005333333334\n",
            "\n",
            " iteration=225 bias=0.3900000000000002 loss=66.42148666666665\n",
            "\n",
            " iteration=226 bias=0.4000000000000002 loss=66.35312\n",
            "\n",
            " iteration=227 bias=0.4100000000000002 loss=66.28495333333333\n",
            "\n",
            " iteration=228 bias=0.4200000000000002 loss=66.21698666666666\n",
            "\n",
            " iteration=229 bias=0.4300000000000002 loss=66.14922\n",
            "\n",
            " iteration=230 bias=0.4400000000000002 loss=66.08165333333332\n",
            "\n",
            " iteration=231 bias=0.45000000000000023 loss=66.01428666666666\n",
            "\n",
            " iteration=232 bias=0.46000000000000024 loss=65.94712\n",
            "\n",
            " iteration=233 bias=0.47000000000000025 loss=65.88015333333333\n",
            "\n",
            " iteration=234 bias=0.48000000000000026 loss=65.81338666666666\n",
            "\n",
            " iteration=235 bias=0.49000000000000027 loss=65.74682\n",
            "\n",
            " iteration=236 bias=0.5000000000000002 loss=65.68045333333333\n",
            "\n",
            " iteration=237 bias=0.5000000000000002 loss=65.67934666666666\n",
            "\n",
            " iteration=238 bias=0.5100000000000002 loss=65.61064666666665\n",
            "\n",
            " iteration=239 bias=0.5200000000000002 loss=65.54214666666668\n",
            "\n",
            " iteration=240 bias=0.5300000000000002 loss=65.47384666666667\n",
            "\n",
            " iteration=241 bias=0.5400000000000003 loss=65.40574666666667\n",
            "\n",
            " iteration=242 bias=0.5500000000000003 loss=65.33784666666665\n",
            "\n",
            " iteration=243 bias=0.5600000000000003 loss=65.27014666666668\n",
            "\n",
            " iteration=244 bias=0.5700000000000003 loss=65.20264666666667\n",
            "\n",
            " iteration=245 bias=0.5800000000000003 loss=65.13534666666665\n",
            "\n",
            " iteration=246 bias=0.5900000000000003 loss=65.06824666666668\n",
            "\n",
            " iteration=247 bias=0.6000000000000003 loss=65.00134666666665\n",
            "\n",
            " iteration=248 bias=0.6100000000000003 loss=64.93464666666667\n",
            "\n",
            " iteration=249 bias=0.6200000000000003 loss=64.86814666666666\n",
            "\n",
            " iteration=250 bias=0.6300000000000003 loss=64.80184666666668\n",
            "\n",
            " iteration=251 bias=0.6400000000000003 loss=64.73574666666666\n",
            "\n",
            " iteration=252 bias=0.6500000000000004 loss=64.66984666666667\n",
            "\n",
            " iteration=253 bias=0.6600000000000004 loss=64.60414666666668\n",
            "\n",
            " iteration=254 bias=0.6700000000000004 loss=64.53864666666666\n",
            "\n",
            " iteration=255 bias=0.6700000000000004 loss=64.53823333333331\n",
            "\n",
            " iteration=256 bias=0.6800000000000004 loss=64.4704\n",
            "\n",
            " iteration=257 bias=0.6900000000000004 loss=64.40276666666664\n",
            "\n",
            " iteration=258 bias=0.7000000000000004 loss=64.33533333333334\n",
            "\n",
            " iteration=259 bias=0.7100000000000004 loss=64.26809999999998\n",
            "\n",
            " iteration=260 bias=0.7200000000000004 loss=64.20106666666666\n",
            "\n",
            " iteration=261 bias=0.7300000000000004 loss=64.1342333333333\n",
            "\n",
            " iteration=262 bias=0.7400000000000004 loss=64.06759999999998\n",
            "\n",
            " iteration=263 bias=0.7500000000000004 loss=64.00116666666666\n",
            "\n",
            " iteration=264 bias=0.7600000000000005 loss=63.93493333333332\n",
            "\n",
            " iteration=265 bias=0.7700000000000005 loss=63.86889999999998\n",
            "\n",
            " iteration=266 bias=0.7800000000000005 loss=63.80306666666664\n",
            "\n",
            " iteration=267 bias=0.7900000000000005 loss=63.737433333333314\n",
            "\n",
            " iteration=268 bias=0.8000000000000005 loss=63.671999999999976\n",
            "\n",
            " iteration=269 bias=0.8100000000000005 loss=63.60676666666666\n",
            "\n",
            " iteration=270 bias=0.8200000000000005 loss=63.541733333333326\n",
            "\n",
            " iteration=271 bias=0.8300000000000005 loss=63.47689999999998\n",
            "\n",
            " iteration=272 bias=0.8400000000000005 loss=63.412266666666675\n",
            "\n",
            " iteration=273 bias=0.8500000000000005 loss=63.347833333333334\n",
            "\n",
            " iteration=274 bias=0.8500000000000005 loss=63.34558\n",
            "\n",
            " iteration=275 bias=0.8600000000000005 loss=63.27881333333333\n",
            "\n",
            " iteration=276 bias=0.8700000000000006 loss=63.212246666666644\n",
            "\n",
            " iteration=277 bias=0.8800000000000006 loss=63.145880000000005\n",
            "\n",
            " iteration=278 bias=0.8900000000000006 loss=63.07971333333333\n",
            "\n",
            " iteration=279 bias=0.9000000000000006 loss=63.01374666666665\n",
            "\n",
            " iteration=280 bias=0.9100000000000006 loss=62.94798\n",
            "\n",
            " iteration=281 bias=0.9200000000000006 loss=62.88241333333333\n",
            "\n",
            " iteration=282 bias=0.9300000000000006 loss=62.81704666666667\n",
            "\n",
            " iteration=283 bias=0.9400000000000006 loss=62.75188\n",
            "\n",
            " iteration=284 bias=0.9500000000000006 loss=62.68691333333334\n",
            "\n",
            " iteration=285 bias=0.9600000000000006 loss=62.622146666666666\n",
            "\n",
            " iteration=286 bias=0.9700000000000006 loss=62.557579999999994\n",
            "\n",
            " iteration=287 bias=0.9800000000000006 loss=62.493213333333344\n",
            "\n",
            " iteration=288 bias=0.9900000000000007 loss=62.42904666666666\n",
            "\n",
            " iteration=289 bias=1.0000000000000007 loss=62.36507999999999\n",
            "\n",
            " iteration=290 bias=1.0100000000000007 loss=62.30131333333332\n",
            "\n",
            " iteration=291 bias=1.0200000000000007 loss=62.237746666666666\n",
            "\n",
            " iteration=292 bias=1.0200000000000007 loss=62.23618666666666\n",
            "\n",
            " iteration=293 bias=1.0300000000000007 loss=62.170286666666655\n",
            "\n",
            " iteration=294 bias=1.0400000000000007 loss=62.10458666666664\n",
            "\n",
            " iteration=295 bias=1.0500000000000007 loss=62.03908666666667\n",
            "\n",
            " iteration=296 bias=1.0600000000000007 loss=61.97378666666666\n",
            "\n",
            " iteration=297 bias=1.0700000000000007 loss=61.90868666666667\n",
            "\n",
            " iteration=298 bias=1.0800000000000007 loss=61.84378666666666\n",
            "\n",
            " iteration=299 bias=1.0900000000000007 loss=61.779086666666664\n",
            "\n",
            " iteration=300 bias=1.1000000000000008 loss=61.71458666666665\n",
            "\n",
            " iteration=301 bias=1.1100000000000008 loss=61.65028666666665\n",
            "\n",
            " iteration=302 bias=1.1200000000000008 loss=61.58618666666665\n",
            "\n",
            " iteration=303 bias=1.1300000000000008 loss=61.52228666666667\n",
            "\n",
            " iteration=304 bias=1.1400000000000008 loss=61.45858666666666\n",
            "\n",
            " iteration=305 bias=1.1500000000000008 loss=61.39508666666665\n",
            "\n",
            " iteration=306 bias=1.1600000000000008 loss=61.33178666666667\n",
            "\n",
            " iteration=307 bias=1.1700000000000008 loss=61.26868666666667\n",
            "\n",
            " iteration=308 bias=1.1800000000000008 loss=61.20578666666666\n",
            "\n",
            " iteration=309 bias=1.1900000000000008 loss=61.143086666666655\n",
            "\n",
            " iteration=310 bias=1.1900000000000008 loss=61.14221999999998\n",
            "\n",
            " iteration=311 bias=1.2000000000000008 loss=61.07718666666667\n",
            "\n",
            " iteration=312 bias=1.2100000000000009 loss=61.01235333333334\n",
            "\n",
            " iteration=313 bias=1.2200000000000009 loss=60.94771999999998\n",
            "\n",
            " iteration=314 bias=1.2300000000000009 loss=60.883286666666685\n",
            "\n",
            " iteration=315 bias=1.2400000000000009 loss=60.819053333333336\n",
            "\n",
            " iteration=316 bias=1.2500000000000009 loss=60.75501999999999\n",
            "\n",
            " iteration=317 bias=1.260000000000001 loss=60.69118666666667\n",
            "\n",
            " iteration=318 bias=1.270000000000001 loss=60.627553333333346\n",
            "\n",
            " iteration=319 bias=1.280000000000001 loss=60.56412\n",
            "\n",
            " iteration=320 bias=1.290000000000001 loss=60.50088666666667\n",
            "\n",
            " iteration=321 bias=1.300000000000001 loss=60.437853333333344\n",
            "\n",
            " iteration=322 bias=1.310000000000001 loss=60.37501999999999\n",
            "\n",
            " iteration=323 bias=1.320000000000001 loss=60.312386666666654\n",
            "\n",
            " iteration=324 bias=1.330000000000001 loss=60.24995333333333\n",
            "\n",
            " iteration=325 bias=1.340000000000001 loss=60.18772000000001\n",
            "\n",
            " iteration=326 bias=1.350000000000001 loss=60.125686666666674\n",
            "\n",
            " iteration=327 bias=1.360000000000001 loss=60.063853333333334\n",
            "\n",
            " iteration=328 bias=1.360000000000001 loss=60.06368\n",
            "\n",
            " iteration=329 bias=1.370000000000001 loss=59.999513333333326\n",
            "\n",
            " iteration=330 bias=1.380000000000001 loss=59.93554666666666\n",
            "\n",
            " iteration=331 bias=1.390000000000001 loss=59.87177999999999\n",
            "\n",
            " iteration=332 bias=1.400000000000001 loss=59.80821333333332\n",
            "\n",
            " iteration=333 bias=1.410000000000001 loss=59.74484666666665\n",
            "\n",
            " iteration=334 bias=1.420000000000001 loss=59.68167999999999\n",
            "\n",
            " iteration=335 bias=1.430000000000001 loss=59.618713333333325\n",
            "\n",
            " iteration=336 bias=1.440000000000001 loss=59.55594666666665\n",
            "\n",
            " iteration=337 bias=1.450000000000001 loss=59.493379999999995\n",
            "\n",
            " iteration=338 bias=1.460000000000001 loss=59.431013333333325\n",
            "\n",
            " iteration=339 bias=1.470000000000001 loss=59.36884666666664\n",
            "\n",
            " iteration=340 bias=1.480000000000001 loss=59.30688000000001\n",
            "\n",
            " iteration=341 bias=1.490000000000001 loss=59.24511333333333\n",
            "\n",
            " iteration=342 bias=1.500000000000001 loss=59.183546666666665\n",
            "\n",
            " iteration=343 bias=1.5100000000000011 loss=59.122179999999986\n",
            "\n",
            " iteration=344 bias=1.5200000000000011 loss=59.061013333333335\n",
            "\n",
            " iteration=345 bias=1.5300000000000011 loss=59.00004666666665\n",
            "\n",
            " iteration=346 bias=1.5400000000000011 loss=58.93927999999998\n",
            "\n",
            " iteration=347 bias=1.5400000000000011 loss=58.937266666666645\n",
            "\n",
            " iteration=348 bias=1.5500000000000012 loss=58.874166666666675\n",
            "\n",
            " iteration=349 bias=1.5600000000000012 loss=58.81126666666666\n",
            "\n",
            " iteration=350 bias=1.5700000000000012 loss=58.74856666666666\n",
            "\n",
            " iteration=351 bias=1.5800000000000012 loss=58.68606666666665\n",
            "\n",
            " iteration=352 bias=1.5900000000000012 loss=58.62376666666667\n",
            "\n",
            " iteration=353 bias=1.6000000000000012 loss=58.56166666666665\n",
            "\n",
            " iteration=354 bias=1.6100000000000012 loss=58.49976666666665\n",
            "\n",
            " iteration=355 bias=1.6200000000000012 loss=58.43806666666667\n",
            "\n",
            " iteration=356 bias=1.6300000000000012 loss=58.37656666666667\n",
            "\n",
            " iteration=357 bias=1.6400000000000012 loss=58.315266666666666\n",
            "\n",
            " iteration=358 bias=1.6500000000000012 loss=58.25416666666665\n",
            "\n",
            " iteration=359 bias=1.6600000000000013 loss=58.193266666666666\n",
            "\n",
            " iteration=360 bias=1.6700000000000013 loss=58.132566666666655\n",
            "\n",
            " iteration=361 bias=1.6800000000000013 loss=58.072066666666665\n",
            "\n",
            " iteration=362 bias=1.6900000000000013 loss=58.011766666666674\n",
            "\n",
            " iteration=363 bias=1.7000000000000013 loss=57.95166666666667\n",
            "\n",
            " iteration=364 bias=1.7100000000000013 loss=57.89176666666666\n",
            "\n",
            " iteration=365 bias=1.7100000000000013 loss=57.89044666666665\n",
            "\n",
            " iteration=366 bias=1.7200000000000013 loss=57.82821333333332\n",
            "\n",
            " iteration=367 bias=1.7300000000000013 loss=57.76617999999999\n",
            "\n",
            " iteration=368 bias=1.7400000000000013 loss=57.70434666666667\n",
            "\n",
            " iteration=369 bias=1.7500000000000013 loss=57.64271333333332\n",
            "\n",
            " iteration=370 bias=1.7600000000000013 loss=57.58127999999999\n",
            "\n",
            " iteration=371 bias=1.7700000000000014 loss=57.520046666666666\n",
            "\n",
            " iteration=372 bias=1.7800000000000014 loss=57.459013333333324\n",
            "\n",
            " iteration=373 bias=1.7900000000000014 loss=57.39817999999999\n",
            "\n",
            " iteration=374 bias=1.8000000000000014 loss=57.33754666666667\n",
            "\n",
            " iteration=375 bias=1.8100000000000014 loss=57.277113333333325\n",
            "\n",
            " iteration=376 bias=1.8200000000000014 loss=57.21687999999999\n",
            "\n",
            " iteration=377 bias=1.8300000000000014 loss=57.15684666666665\n",
            "\n",
            " iteration=378 bias=1.8400000000000014 loss=57.097013333333344\n",
            "\n",
            " iteration=379 bias=1.8500000000000014 loss=57.03738\n",
            "\n",
            " iteration=380 bias=1.8600000000000014 loss=56.97794666666665\n",
            "\n",
            " iteration=381 bias=1.8700000000000014 loss=56.918713333333336\n",
            "\n",
            " iteration=382 bias=1.8800000000000014 loss=56.859680000000004\n",
            "\n",
            " iteration=383 bias=1.8800000000000014 loss=56.85905333333332\n",
            "\n",
            " iteration=384 bias=1.8900000000000015 loss=56.79768666666665\n",
            "\n",
            " iteration=385 bias=1.9000000000000015 loss=56.73651999999998\n",
            "\n",
            " iteration=386 bias=1.9100000000000015 loss=56.675553333333326\n",
            "\n",
            " iteration=387 bias=1.9200000000000015 loss=56.614786666666646\n",
            "\n",
            " iteration=388 bias=1.9300000000000015 loss=56.55421999999998\n",
            "\n",
            " iteration=389 bias=1.9400000000000015 loss=56.49385333333334\n",
            "\n",
            " iteration=390 bias=1.9500000000000015 loss=56.433686666666645\n",
            "\n",
            " iteration=391 bias=1.9600000000000015 loss=56.373719999999985\n",
            "\n",
            " iteration=392 bias=1.9700000000000015 loss=56.313953333333316\n",
            "\n",
            " iteration=393 bias=1.9800000000000015 loss=56.254386666666655\n",
            "\n",
            " iteration=394 bias=1.9900000000000015 loss=56.19501999999998\n",
            "\n",
            " iteration=395 bias=2.0000000000000013 loss=56.13585333333333\n",
            "\n",
            " iteration=396 bias=2.010000000000001 loss=56.07688666666665\n",
            "\n",
            " iteration=397 bias=2.020000000000001 loss=56.018119999999996\n",
            "\n",
            " iteration=398 bias=2.0300000000000007 loss=55.95955333333332\n",
            "\n",
            " iteration=399 bias=2.0400000000000005 loss=55.90118666666667\n",
            "\n",
            " iteration=400 bias=2.0500000000000003 loss=55.84301999999999\n",
            "\n",
            " iteration=401 bias=2.06 loss=55.78505333333333\n",
            "\n",
            " iteration=402 bias=2.06 loss=55.78258666666668\n",
            "\n",
            " iteration=403 bias=2.07 loss=55.72228666666667\n",
            "\n",
            " iteration=404 bias=2.0799999999999996 loss=55.66218666666666\n",
            "\n",
            " iteration=405 bias=2.0899999999999994 loss=55.60228666666667\n",
            "\n",
            " iteration=406 bias=2.099999999999999 loss=55.542586666666665\n",
            "\n",
            " iteration=407 bias=2.109999999999999 loss=55.483086666666665\n",
            "\n",
            " iteration=408 bias=2.1199999999999988 loss=55.42378666666668\n",
            "\n",
            " iteration=409 bias=2.1299999999999986 loss=55.364686666666664\n",
            "\n",
            " iteration=410 bias=2.1399999999999983 loss=55.305786666666684\n",
            "\n",
            " iteration=411 bias=2.149999999999998 loss=55.247086666666675\n",
            "\n",
            " iteration=412 bias=2.159999999999998 loss=55.18858666666667\n",
            "\n",
            " iteration=413 bias=2.1699999999999977 loss=55.13028666666667\n",
            "\n",
            " iteration=414 bias=2.1799999999999975 loss=55.07218666666669\n",
            "\n",
            " iteration=415 bias=2.1899999999999973 loss=55.0142866666667\n",
            "\n",
            " iteration=416 bias=2.199999999999997 loss=54.95658666666669\n",
            "\n",
            " iteration=417 bias=2.209999999999997 loss=54.899086666666676\n",
            "\n",
            " iteration=418 bias=2.2199999999999966 loss=54.8417866666667\n",
            "\n",
            " iteration=419 bias=2.2299999999999964 loss=54.784686666666694\n",
            "\n",
            " iteration=420 bias=2.2299999999999964 loss=54.78291333333336\n",
            "\n",
            " iteration=421 bias=2.239999999999996 loss=54.723480000000016\n",
            "\n",
            " iteration=422 bias=2.249999999999996 loss=54.664246666666685\n",
            "\n",
            " iteration=423 bias=2.259999999999996 loss=54.605213333333374\n",
            "\n",
            " iteration=424 bias=2.2699999999999956 loss=54.54638000000002\n",
            "\n",
            " iteration=425 bias=2.2799999999999954 loss=54.4877466666667\n",
            "\n",
            " iteration=426 bias=2.289999999999995 loss=54.42931333333335\n",
            "\n",
            " iteration=427 bias=2.299999999999995 loss=54.37108000000004\n",
            "\n",
            " iteration=428 bias=2.3099999999999947 loss=54.313046666666686\n",
            "\n",
            " iteration=429 bias=2.3199999999999945 loss=54.255213333333366\n",
            "\n",
            " iteration=430 bias=2.3299999999999943 loss=54.19758000000004\n",
            "\n",
            " iteration=431 bias=2.339999999999994 loss=54.14014666666671\n",
            "\n",
            " iteration=432 bias=2.349999999999994 loss=54.08291333333337\n",
            "\n",
            " iteration=433 bias=2.3599999999999937 loss=54.02588000000004\n",
            "\n",
            " iteration=434 bias=2.3699999999999934 loss=53.969046666666706\n",
            "\n",
            " iteration=435 bias=2.3799999999999932 loss=53.91241333333338\n",
            "\n",
            " iteration=436 bias=2.389999999999993 loss=53.855980000000045\n",
            "\n",
            " iteration=437 bias=2.399999999999993 loss=53.799746666666714\n",
            "\n",
            " iteration=438 bias=2.399999999999993 loss=53.798666666666705\n",
            "\n",
            " iteration=439 bias=2.4099999999999926 loss=53.74010000000002\n",
            "\n",
            " iteration=440 bias=2.4199999999999924 loss=53.68173333333338\n",
            "\n",
            " iteration=441 bias=2.429999999999992 loss=53.6235666666667\n",
            "\n",
            " iteration=442 bias=2.439999999999992 loss=53.56560000000004\n",
            "\n",
            " iteration=443 bias=2.4499999999999917 loss=53.50783333333336\n",
            "\n",
            " iteration=444 bias=2.4599999999999915 loss=53.45026666666672\n",
            "\n",
            " iteration=445 bias=2.4699999999999913 loss=53.39290000000003\n",
            "\n",
            " iteration=446 bias=2.479999999999991 loss=53.335733333333366\n",
            "\n",
            " iteration=447 bias=2.489999999999991 loss=53.27876666666671\n",
            "\n",
            " iteration=448 bias=2.4999999999999907 loss=53.22200000000006\n",
            "\n",
            " iteration=449 bias=2.5099999999999905 loss=53.16543333333338\n",
            "\n",
            " iteration=450 bias=2.5199999999999902 loss=53.10906666666671\n",
            "\n",
            " iteration=451 bias=2.52999999999999 loss=53.052900000000044\n",
            "\n",
            " iteration=452 bias=2.53999999999999 loss=52.99693333333339\n",
            "\n",
            " iteration=453 bias=2.5499999999999896 loss=52.94116666666671\n",
            "\n",
            " iteration=454 bias=2.5599999999999894 loss=52.88560000000005\n",
            "\n",
            " iteration=455 bias=2.569999999999989 loss=52.83023333333338\n",
            "\n",
            " iteration=456 bias=2.569999999999989 loss=52.829846666666725\n",
            "\n",
            " iteration=457 bias=2.579999999999989 loss=52.77214666666673\n",
            "\n",
            " iteration=458 bias=2.5899999999999888 loss=52.71464666666673\n",
            "\n",
            " iteration=459 bias=2.5999999999999885 loss=52.657346666666726\n",
            "\n",
            " iteration=460 bias=2.6099999999999883 loss=52.60024666666672\n",
            "\n",
            " iteration=461 bias=2.619999999999988 loss=52.543346666666736\n",
            "\n",
            " iteration=462 bias=2.629999999999988 loss=52.48664666666673\n",
            "\n",
            " iteration=463 bias=2.6399999999999877 loss=52.43014666666672\n",
            "\n",
            " iteration=464 bias=2.6499999999999875 loss=52.37384666666672\n",
            "\n",
            " iteration=465 bias=2.6599999999999873 loss=52.31774666666673\n",
            "\n",
            " iteration=466 bias=2.669999999999987 loss=52.26184666666673\n",
            "\n",
            " iteration=467 bias=2.679999999999987 loss=52.20614666666673\n",
            "\n",
            " iteration=468 bias=2.6899999999999866 loss=52.15064666666673\n",
            "\n",
            " iteration=469 bias=2.6999999999999864 loss=52.09534666666674\n",
            "\n",
            " iteration=470 bias=2.709999999999986 loss=52.04024666666674\n",
            "\n",
            " iteration=471 bias=2.719999999999986 loss=51.98534666666673\n",
            "\n",
            " iteration=472 bias=2.7299999999999858 loss=51.930646666666725\n",
            "\n",
            " iteration=473 bias=2.7399999999999856 loss=51.87614666666674\n",
            "\n",
            " iteration=474 bias=2.7499999999999853 loss=51.82184666666675\n",
            "\n",
            " iteration=475 bias=2.7499999999999853 loss=51.81962000000009\n",
            "\n",
            " iteration=476 bias=2.759999999999985 loss=51.762986666666755\n",
            "\n",
            " iteration=477 bias=2.769999999999985 loss=51.70655333333342\n",
            "\n",
            " iteration=478 bias=2.7799999999999847 loss=51.6503200000001\n",
            "\n",
            " iteration=479 bias=2.7899999999999845 loss=51.594286666666754\n",
            "\n",
            " iteration=480 bias=2.7999999999999843 loss=51.53845333333342\n",
            "\n",
            " iteration=481 bias=2.809999999999984 loss=51.48282000000008\n",
            "\n",
            " iteration=482 bias=2.819999999999984 loss=51.42738666666676\n",
            "\n",
            " iteration=483 bias=2.8299999999999836 loss=51.37215333333343\n",
            "\n",
            " iteration=484 bias=2.8399999999999834 loss=51.31712000000011\n",
            "\n",
            " iteration=485 bias=2.849999999999983 loss=51.26228666666676\n",
            "\n",
            " iteration=486 bias=2.859999999999983 loss=51.20765333333345\n",
            "\n",
            " iteration=487 bias=2.869999999999983 loss=51.1532200000001\n",
            "\n",
            " iteration=488 bias=2.8799999999999826 loss=51.09898666666678\n",
            "\n",
            " iteration=489 bias=2.8899999999999824 loss=51.04495333333343\n",
            "\n",
            " iteration=490 bias=2.899999999999982 loss=50.991120000000116\n",
            "\n",
            " iteration=491 bias=2.909999999999982 loss=50.93748666666677\n",
            "\n",
            " iteration=492 bias=2.9199999999999817 loss=50.88405333333344\n",
            "\n",
            " iteration=493 bias=2.9199999999999817 loss=50.882520000000085\n",
            "\n",
            " iteration=494 bias=2.9299999999999815 loss=50.826753333333414\n",
            "\n",
            " iteration=495 bias=2.9399999999999813 loss=50.771186666666786\n",
            "\n",
            " iteration=496 bias=2.949999999999981 loss=50.71582000000011\n",
            "\n",
            " iteration=497 bias=2.959999999999981 loss=50.66065333333344\n",
            "\n",
            " iteration=498 bias=2.9699999999999807 loss=50.60568666666676\n",
            "\n",
            " iteration=499 bias=2.9799999999999804 loss=50.55092000000011\n",
            "\n",
            " iteration=500 bias=2.9899999999999802 loss=50.49635333333343\n",
            "\n",
            " iteration=501 bias=2.99999999999998 loss=50.44198666666678\n",
            "\n",
            " iteration=502 bias=3.00999999999998 loss=50.38782000000008\n",
            "\n",
            " iteration=503 bias=3.0199999999999796 loss=50.333853333333444\n",
            "\n",
            " iteration=504 bias=3.0299999999999794 loss=50.28008666666677\n",
            "\n",
            " iteration=505 bias=3.039999999999979 loss=50.226520000000114\n",
            "\n",
            " iteration=506 bias=3.049999999999979 loss=50.17315333333343\n",
            "\n",
            " iteration=507 bias=3.0599999999999787 loss=50.11998666666679\n",
            "\n",
            " iteration=508 bias=3.0699999999999785 loss=50.067020000000106\n",
            "\n",
            " iteration=509 bias=3.0799999999999783 loss=50.01425333333345\n",
            "\n",
            " iteration=510 bias=3.089999999999978 loss=49.96168666666677\n",
            "\n",
            " iteration=511 bias=3.089999999999978 loss=49.96084666666677\n",
            "\n",
            " iteration=512 bias=3.099999999999978 loss=49.90594666666679\n",
            "\n",
            " iteration=513 bias=3.1099999999999777 loss=49.85124666666679\n",
            "\n",
            " iteration=514 bias=3.1199999999999775 loss=49.79674666666677\n",
            "\n",
            " iteration=515 bias=3.1299999999999772 loss=49.74244666666678\n",
            "\n",
            " iteration=516 bias=3.139999999999977 loss=49.6883466666668\n",
            "\n",
            " iteration=517 bias=3.149999999999977 loss=49.63444666666679\n",
            "\n",
            " iteration=518 bias=3.1599999999999766 loss=49.58074666666678\n",
            "\n",
            " iteration=519 bias=3.1699999999999764 loss=49.527246666666784\n",
            "\n",
            " iteration=520 bias=3.179999999999976 loss=49.47394666666681\n",
            "\n",
            " iteration=521 bias=3.189999999999976 loss=49.42084666666679\n",
            "\n",
            " iteration=522 bias=3.1999999999999758 loss=49.36794666666679\n",
            "\n",
            " iteration=523 bias=3.2099999999999755 loss=49.315246666666795\n",
            "\n",
            " iteration=524 bias=3.2199999999999753 loss=49.26274666666679\n",
            "\n",
            " iteration=525 bias=3.229999999999975 loss=49.210446666666805\n",
            "\n",
            " iteration=526 bias=3.239999999999975 loss=49.1583466666668\n",
            "\n",
            " iteration=527 bias=3.2499999999999747 loss=49.10644666666679\n",
            "\n",
            " iteration=528 bias=3.2599999999999745 loss=49.054746666666794\n",
            "\n",
            " iteration=529 bias=3.2599999999999745 loss=49.05460000000016\n",
            "\n",
            " iteration=530 bias=3.2699999999999743 loss=49.000566666666806\n",
            "\n",
            " iteration=531 bias=3.279999999999974 loss=48.94673333333348\n",
            "\n",
            " iteration=532 bias=3.289999999999974 loss=48.89310000000013\n",
            "\n",
            " iteration=533 bias=3.2999999999999736 loss=48.839666666666815\n",
            "\n",
            " iteration=534 bias=3.3099999999999734 loss=48.78643333333347\n",
            "\n",
            " iteration=535 bias=3.319999999999973 loss=48.733400000000145\n",
            "\n",
            " iteration=536 bias=3.329999999999973 loss=48.68056666666679\n",
            "\n",
            " iteration=537 bias=3.3399999999999728 loss=48.627933333333495\n",
            "\n",
            " iteration=538 bias=3.3499999999999726 loss=48.57550000000015\n",
            "\n",
            " iteration=539 bias=3.3599999999999723 loss=48.52326666666681\n",
            "\n",
            " iteration=540 bias=3.369999999999972 loss=48.471233333333494\n",
            "\n",
            " iteration=541 bias=3.379999999999972 loss=48.41940000000015\n",
            "\n",
            " iteration=542 bias=3.3899999999999717 loss=48.36776666666681\n",
            "\n",
            " iteration=543 bias=3.3999999999999715 loss=48.316333333333475\n",
            "\n",
            " iteration=544 bias=3.4099999999999713 loss=48.265100000000146\n",
            "\n",
            " iteration=545 bias=3.419999999999971 loss=48.214066666666824\n",
            "\n",
            " iteration=546 bias=3.429999999999971 loss=48.16323333333349\n",
            "\n",
            " iteration=547 bias=3.4399999999999706 loss=48.11260000000015\n",
            "\n",
            " iteration=548 bias=3.4399999999999706 loss=48.11061333333349\n",
            "\n",
            " iteration=549 bias=3.4499999999999704 loss=48.05764666666679\n",
            "\n",
            " iteration=550 bias=3.45999999999997 loss=48.004880000000156\n",
            "\n",
            " iteration=551 bias=3.46999999999997 loss=47.95231333333349\n",
            "\n",
            " iteration=552 bias=3.47999999999997 loss=47.899946666666814\n",
            "\n",
            " iteration=553 bias=3.4899999999999696 loss=47.84778000000016\n",
            "\n",
            " iteration=554 bias=3.4999999999999694 loss=47.79581333333349\n",
            "\n",
            " iteration=555 bias=3.509999999999969 loss=47.744046666666826\n",
            "\n",
            " iteration=556 bias=3.519999999999969 loss=47.692480000000145\n",
            "\n",
            " iteration=557 bias=3.5299999999999687 loss=47.64111333333348\n",
            "\n",
            " iteration=558 bias=3.5399999999999685 loss=47.58994666666683\n",
            "\n",
            " iteration=559 bias=3.5499999999999683 loss=47.53898000000016\n",
            "\n",
            " iteration=560 bias=3.559999999999968 loss=47.4882133333335\n",
            "\n",
            " iteration=561 bias=3.569999999999968 loss=47.43764666666682\n",
            "\n",
            " iteration=562 bias=3.5799999999999677 loss=47.38728000000016\n",
            "\n",
            " iteration=563 bias=3.5899999999999674 loss=47.3371133333335\n",
            "\n",
            " iteration=564 bias=3.5999999999999672 loss=47.287146666666814\n",
            "\n",
            " iteration=565 bias=3.609999999999967 loss=47.23738000000016\n",
            "\n",
            " iteration=566 bias=3.609999999999967 loss=47.23608666666682\n",
            "\n",
            " iteration=567 bias=3.619999999999967 loss=47.18398666666686\n",
            "\n",
            " iteration=568 bias=3.6299999999999666 loss=47.13208666666684\n",
            "\n",
            " iteration=569 bias=3.6399999999999664 loss=47.08038666666684\n",
            "\n",
            " iteration=570 bias=3.649999999999966 loss=47.02888666666683\n",
            "\n",
            " iteration=571 bias=3.659999999999966 loss=46.977586666666845\n",
            "\n",
            " iteration=572 bias=3.6699999999999657 loss=46.92648666666684\n",
            "\n",
            " iteration=573 bias=3.6799999999999655 loss=46.87558666666684\n",
            "\n",
            " iteration=574 bias=3.6899999999999653 loss=46.824886666666835\n",
            "\n",
            " iteration=575 bias=3.699999999999965 loss=46.77438666666684\n",
            "\n",
            " iteration=576 bias=3.709999999999965 loss=46.72408666666685\n",
            "\n",
            " iteration=577 bias=3.7199999999999647 loss=46.673986666666835\n",
            "\n",
            " iteration=578 bias=3.7299999999999645 loss=46.62408666666683\n",
            "\n",
            " iteration=579 bias=3.7399999999999642 loss=46.57438666666686\n",
            "\n",
            " iteration=580 bias=3.749999999999964 loss=46.524886666666845\n",
            "\n",
            " iteration=581 bias=3.759999999999964 loss=46.47558666666684\n",
            "\n",
            " iteration=582 bias=3.7699999999999636 loss=46.42648666666685\n",
            "\n",
            " iteration=583 bias=3.7799999999999634 loss=46.37758666666686\n",
            "\n",
            " iteration=584 bias=3.7799999999999634 loss=46.37698666666686\n",
            "\n",
            " iteration=585 bias=3.789999999999963 loss=46.32575333333352\n",
            "\n",
            " iteration=586 bias=3.799999999999963 loss=46.274720000000194\n",
            "\n",
            " iteration=587 bias=3.8099999999999627 loss=46.22388666666684\n",
            "\n",
            " iteration=588 bias=3.8199999999999625 loss=46.173253333333534\n",
            "\n",
            " iteration=589 bias=3.8299999999999623 loss=46.12282000000019\n",
            "\n",
            " iteration=590 bias=3.839999999999962 loss=46.07258666666685\n",
            "\n",
            " iteration=591 bias=3.849999999999962 loss=46.022553333333505\n",
            "\n",
            " iteration=592 bias=3.8599999999999617 loss=45.97272000000019\n",
            "\n",
            " iteration=593 bias=3.8699999999999615 loss=45.92308666666686\n",
            "\n",
            " iteration=594 bias=3.8799999999999613 loss=45.873653333333536\n",
            "\n",
            " iteration=595 bias=3.889999999999961 loss=45.82442000000019\n",
            "\n",
            " iteration=596 bias=3.899999999999961 loss=45.775386666666876\n",
            "\n",
            " iteration=597 bias=3.9099999999999606 loss=45.726553333333534\n",
            "\n",
            " iteration=598 bias=3.9199999999999604 loss=45.6779200000002\n",
            "\n",
            " iteration=599 bias=3.92999999999996 loss=45.62948666666686\n",
            "\n",
            " iteration=600 bias=3.93999999999996 loss=45.581253333333535\n",
            "\n",
            " iteration=601 bias=3.9499999999999598 loss=45.533220000000206\n",
            "\n",
            " iteration=602 bias=3.9599999999999596 loss=45.485386666666855\n",
            "\n",
            " iteration=603 bias=3.9599999999999596 loss=45.48294666666685\n",
            "\n",
            " iteration=604 bias=3.9699999999999593 loss=45.432780000000186\n",
            "\n",
            " iteration=605 bias=3.979999999999959 loss=45.38281333333353\n",
            "\n",
            " iteration=606 bias=3.989999999999959 loss=45.33304666666686\n",
            "\n",
            " iteration=607 bias=3.9999999999999587 loss=45.28348000000019\n",
            "\n",
            " iteration=608 bias=4.009999999999959 loss=45.234113333333525\n",
            "\n",
            " iteration=609 bias=4.019999999999959 loss=45.18494666666686\n",
            "\n",
            " iteration=610 bias=4.0299999999999585 loss=45.135980000000195\n",
            "\n",
            " iteration=611 bias=4.039999999999958 loss=45.08721333333351\n",
            "\n",
            " iteration=612 bias=4.049999999999958 loss=45.03864666666685\n",
            "\n",
            " iteration=613 bias=4.059999999999958 loss=44.9902800000002\n",
            "\n",
            " iteration=614 bias=4.069999999999958 loss=44.94211333333352\n",
            "\n",
            " iteration=615 bias=4.079999999999957 loss=44.894146666666856\n",
            "\n",
            " iteration=616 bias=4.089999999999957 loss=44.846380000000195\n",
            "\n",
            " iteration=617 bias=4.099999999999957 loss=44.798813333333534\n",
            "\n",
            " iteration=618 bias=4.109999999999957 loss=44.751446666666865\n",
            "\n",
            " iteration=619 bias=4.119999999999957 loss=44.7042800000002\n",
            "\n",
            " iteration=620 bias=4.129999999999956 loss=44.65731333333352\n",
            "\n",
            " iteration=621 bias=4.129999999999956 loss=44.655566666666886\n",
            "\n",
            " iteration=622 bias=4.139999999999956 loss=44.60626666666688\n",
            "\n",
            " iteration=623 bias=4.149999999999956 loss=44.55716666666686\n",
            "\n",
            " iteration=624 bias=4.159999999999956 loss=44.50826666666689\n",
            "\n",
            " iteration=625 bias=4.1699999999999555 loss=44.459566666666866\n",
            "\n",
            " iteration=626 bias=4.179999999999955 loss=44.41106666666689\n",
            "\n",
            " iteration=627 bias=4.189999999999955 loss=44.36276666666687\n",
            "\n",
            " iteration=628 bias=4.199999999999955 loss=44.31466666666687\n",
            "\n",
            " iteration=629 bias=4.209999999999955 loss=44.26676666666688\n",
            "\n",
            " iteration=630 bias=4.2199999999999545 loss=44.21906666666688\n",
            "\n",
            " iteration=631 bias=4.229999999999954 loss=44.171566666666884\n",
            "\n",
            " iteration=632 bias=4.239999999999954 loss=44.124266666666884\n",
            "\n",
            " iteration=633 bias=4.249999999999954 loss=44.077166666666876\n",
            "\n",
            " iteration=634 bias=4.259999999999954 loss=44.03026666666689\n",
            "\n",
            " iteration=635 bias=4.269999999999953 loss=43.98356666666687\n",
            "\n",
            " iteration=636 bias=4.279999999999953 loss=43.93706666666687\n",
            "\n",
            " iteration=637 bias=4.289999999999953 loss=43.890766666666885\n",
            "\n",
            " iteration=638 bias=4.299999999999953 loss=43.84466666666689\n",
            "\n",
            " iteration=639 bias=4.299999999999953 loss=43.843613333333565\n",
            "\n",
            " iteration=640 bias=4.3099999999999525 loss=43.79518000000022\n",
            "\n",
            " iteration=641 bias=4.319999999999952 loss=43.74694666666688\n",
            "\n",
            " iteration=642 bias=4.329999999999952 loss=43.69891333333356\n",
            "\n",
            " iteration=643 bias=4.339999999999952 loss=43.65108000000023\n",
            "\n",
            " iteration=644 bias=4.349999999999952 loss=43.60344666666688\n",
            "\n",
            " iteration=645 bias=4.3599999999999515 loss=43.55601333333355\n",
            "\n",
            " iteration=646 bias=4.369999999999951 loss=43.50878000000022\n",
            "\n",
            " iteration=647 bias=4.379999999999951 loss=43.461746666666905\n",
            "\n",
            " iteration=648 bias=4.389999999999951 loss=43.41491333333355\n",
            "\n",
            " iteration=649 bias=4.399999999999951 loss=43.368280000000226\n",
            "\n",
            " iteration=650 bias=4.40999999999995 loss=43.3218466666669\n",
            "\n",
            " iteration=651 bias=4.41999999999995 loss=43.275613333333574\n",
            "\n",
            " iteration=652 bias=4.42999999999995 loss=43.22958000000024\n",
            "\n",
            " iteration=653 bias=4.43999999999995 loss=43.18374666666689\n",
            "\n",
            " iteration=654 bias=4.4499999999999496 loss=43.13811333333356\n",
            "\n",
            " iteration=655 bias=4.459999999999949 loss=43.09268000000023\n",
            "\n",
            " iteration=656 bias=4.469999999999949 loss=43.0474466666669\n",
            "\n",
            " iteration=657 bias=4.469999999999949 loss=43.047086666666914\n",
            "\n",
            " iteration=658 bias=4.479999999999949 loss=42.99952000000023\n",
            "\n",
            " iteration=659 bias=4.489999999999949 loss=42.95215333333356\n",
            "\n",
            " iteration=660 bias=4.4999999999999485 loss=42.90498666666692\n",
            "\n",
            " iteration=661 bias=4.509999999999948 loss=42.858020000000245\n",
            "\n",
            " iteration=662 bias=4.519999999999948 loss=42.811253333333596\n",
            "\n",
            " iteration=663 bias=4.529999999999948 loss=42.76468666666691\n",
            "\n",
            " iteration=664 bias=4.539999999999948 loss=42.71832000000026\n",
            "\n",
            " iteration=665 bias=4.549999999999947 loss=42.67215333333358\n",
            "\n",
            " iteration=666 bias=4.559999999999947 loss=42.626186666666904\n",
            "\n",
            " iteration=667 bias=4.569999999999947 loss=42.58042000000024\n",
            "\n",
            " iteration=668 bias=4.579999999999947 loss=42.534853333333594\n",
            "\n",
            " iteration=669 bias=4.589999999999947 loss=42.48948666666692\n",
            "\n",
            " iteration=670 bias=4.599999999999946 loss=42.44432000000025\n",
            "\n",
            " iteration=671 bias=4.609999999999946 loss=42.39935333333359\n",
            "\n",
            " iteration=672 bias=4.619999999999946 loss=42.35458666666692\n",
            "\n",
            " iteration=673 bias=4.629999999999946 loss=42.31002000000025\n",
            "\n",
            " iteration=674 bias=4.6399999999999455 loss=42.26565333333359\n",
            "\n",
            " iteration=675 bias=4.649999999999945 loss=42.22148666666691\n",
            "\n",
            " iteration=676 bias=4.649999999999945 loss=42.2192866666669\n",
            "\n",
            " iteration=677 bias=4.659999999999945 loss=42.17278666666691\n",
            "\n",
            " iteration=678 bias=4.669999999999945 loss=42.12648666666693\n",
            "\n",
            " iteration=679 bias=4.679999999999945 loss=42.08038666666693\n",
            "\n",
            " iteration=680 bias=4.689999999999944 loss=42.03448666666693\n",
            "\n",
            " iteration=681 bias=4.699999999999944 loss=41.98878666666691\n",
            "\n",
            " iteration=682 bias=4.709999999999944 loss=41.943286666666914\n",
            "\n",
            " iteration=683 bias=4.719999999999944 loss=41.897986666666924\n",
            "\n",
            " iteration=684 bias=4.729999999999944 loss=41.85288666666691\n",
            "\n",
            " iteration=685 bias=4.739999999999943 loss=41.80798666666692\n",
            "\n",
            " iteration=686 bias=4.749999999999943 loss=41.76328666666691\n",
            "\n",
            " iteration=687 bias=4.759999999999943 loss=41.71878666666691\n",
            "\n",
            " iteration=688 bias=4.769999999999943 loss=41.67448666666692\n",
            "\n",
            " iteration=689 bias=4.7799999999999425 loss=41.630386666666915\n",
            "\n",
            " iteration=690 bias=4.789999999999942 loss=41.58648666666692\n",
            "\n",
            " iteration=691 bias=4.799999999999942 loss=41.54278666666691\n",
            "\n",
            " iteration=692 bias=4.809999999999942 loss=41.4992866666669\n",
            "\n",
            " iteration=693 bias=4.819999999999942 loss=41.455986666666924\n",
            "\n",
            " iteration=694 bias=4.819999999999942 loss=41.45448000000028\n",
            "\n",
            " iteration=695 bias=4.8299999999999415 loss=41.40884666666694\n",
            "\n",
            " iteration=696 bias=4.839999999999941 loss=41.363413333333604\n",
            "\n",
            " iteration=697 bias=4.849999999999941 loss=41.318180000000275\n",
            "\n",
            " iteration=698 bias=4.859999999999941 loss=41.27314666666695\n",
            "\n",
            " iteration=699 bias=4.869999999999941 loss=41.2283133333336\n",
            "\n",
            " iteration=700 bias=4.87999999999994 loss=41.18368000000027\n",
            "\n",
            " iteration=701 bias=4.88999999999994 loss=41.13924666666694\n",
            "\n",
            " iteration=702 bias=4.89999999999994 loss=41.095013333333604\n",
            "\n",
            " iteration=703 bias=4.90999999999994 loss=41.05098000000026\n",
            "\n",
            " iteration=704 bias=4.9199999999999395 loss=41.00714666666693\n",
            "\n",
            " iteration=705 bias=4.929999999999939 loss=40.963513333333594\n",
            "\n",
            " iteration=706 bias=4.939999999999939 loss=40.92008000000028\n",
            "\n",
            " iteration=707 bias=4.949999999999939 loss=40.87684666666693\n",
            "\n",
            " iteration=708 bias=4.959999999999939 loss=40.8338133333336\n",
            "\n",
            " iteration=709 bias=4.9699999999999385 loss=40.79098000000028\n",
            "\n",
            " iteration=710 bias=4.979999999999938 loss=40.748346666666954\n",
            "\n",
            " iteration=711 bias=4.989999999999938 loss=40.7059133333336\n",
            "\n",
            " iteration=712 bias=4.989999999999938 loss=40.70510000000028\n",
            "\n",
            " iteration=713 bias=4.999999999999938 loss=40.66033333333361\n",
            "\n",
            " iteration=714 bias=5.009999999999938 loss=40.61576666666694\n",
            "\n",
            " iteration=715 bias=5.019999999999937 loss=40.57140000000029\n",
            "\n",
            " iteration=716 bias=5.029999999999937 loss=40.52723333333361\n",
            "\n",
            " iteration=717 bias=5.039999999999937 loss=40.48326666666695\n",
            "\n",
            " iteration=718 bias=5.049999999999937 loss=40.43950000000027\n",
            "\n",
            " iteration=719 bias=5.0599999999999365 loss=40.39593333333361\n",
            "\n",
            " iteration=720 bias=5.069999999999936 loss=40.352566666666945\n",
            "\n",
            " iteration=721 bias=5.079999999999936 loss=40.30940000000028\n",
            "\n",
            " iteration=722 bias=5.089999999999936 loss=40.26643333333361\n",
            "\n",
            " iteration=723 bias=5.099999999999936 loss=40.22366666666696\n",
            "\n",
            " iteration=724 bias=5.1099999999999355 loss=40.181100000000285\n",
            "\n",
            " iteration=725 bias=5.119999999999935 loss=40.138733333333604\n",
            "\n",
            " iteration=726 bias=5.129999999999935 loss=40.09656666666694\n",
            "\n",
            " iteration=727 bias=5.139999999999935 loss=40.05460000000027\n",
            "\n",
            " iteration=728 bias=5.149999999999935 loss=40.0128333333336\n",
            "\n",
            " iteration=729 bias=5.159999999999934 loss=39.97126666666694\n",
            "\n",
            " iteration=730 bias=5.159999999999934 loss=39.97114666666696\n",
            "\n",
            " iteration=731 bias=5.169999999999934 loss=39.92724666666695\n",
            "\n",
            " iteration=732 bias=5.179999999999934 loss=39.883546666666945\n",
            "\n",
            " iteration=733 bias=5.189999999999934 loss=39.84004666666695\n",
            "\n",
            " iteration=734 bias=5.199999999999934 loss=39.796746666666955\n",
            "\n",
            " iteration=735 bias=5.209999999999933 loss=39.753646666666945\n",
            "\n",
            " iteration=736 bias=5.219999999999933 loss=39.71074666666696\n",
            "\n",
            " iteration=737 bias=5.229999999999933 loss=39.66804666666696\n",
            "\n",
            " iteration=738 bias=5.239999999999933 loss=39.62554666666696\n",
            "\n",
            " iteration=739 bias=5.2499999999999325 loss=39.58324666666695\n",
            "\n",
            " iteration=740 bias=5.259999999999932 loss=39.54114666666695\n",
            "\n",
            " iteration=741 bias=5.269999999999932 loss=39.499246666666956\n",
            "\n",
            " iteration=742 bias=5.279999999999932 loss=39.45754666666695\n",
            "\n",
            " iteration=743 bias=5.289999999999932 loss=39.416046666666944\n",
            "\n",
            " iteration=744 bias=5.299999999999931 loss=39.37474666666696\n",
            "\n",
            " iteration=745 bias=5.309999999999931 loss=39.333646666666944\n",
            "\n",
            " iteration=746 bias=5.319999999999931 loss=39.29274666666695\n",
            "\n",
            " iteration=747 bias=5.329999999999931 loss=39.252046666666956\n",
            "\n",
            " iteration=748 bias=5.339999999999931 loss=39.21154666666696\n",
            "\n",
            " iteration=749 bias=5.339999999999931 loss=39.20958666666696\n",
            "\n",
            " iteration=750 bias=5.34999999999993 loss=39.166753333333624\n",
            "\n",
            " iteration=751 bias=5.35999999999993 loss=39.1241200000003\n",
            "\n",
            " iteration=752 bias=5.36999999999993 loss=39.081686666666975\n",
            "\n",
            " iteration=753 bias=5.37999999999993 loss=39.039453333333626\n",
            "\n",
            " iteration=754 bias=5.3899999999999295 loss=38.99742000000029\n",
            "\n",
            " iteration=755 bias=5.399999999999929 loss=38.95558666666696\n",
            "\n",
            " iteration=756 bias=5.409999999999929 loss=38.91395333333362\n",
            "\n",
            " iteration=757 bias=5.419999999999929 loss=38.872520000000286\n",
            "\n",
            " iteration=758 bias=5.429999999999929 loss=38.83128666666696\n",
            "\n",
            " iteration=759 bias=5.4399999999999284 loss=38.79025333333362\n",
            "\n",
            " iteration=760 bias=5.449999999999928 loss=38.74942000000029\n",
            "\n",
            " iteration=761 bias=5.459999999999928 loss=38.70878666666695\n",
            "\n",
            " iteration=762 bias=5.469999999999928 loss=38.66835333333363\n",
            "\n",
            " iteration=763 bias=5.479999999999928 loss=38.6281200000003\n",
            "\n",
            " iteration=764 bias=5.489999999999927 loss=38.588086666666946\n",
            "\n",
            " iteration=765 bias=5.499999999999927 loss=38.548253333333626\n",
            "\n",
            " iteration=766 bias=5.509999999999927 loss=38.50862000000028\n",
            "\n",
            " iteration=767 bias=5.509999999999927 loss=38.50735333333363\n",
            "\n",
            " iteration=768 bias=5.519999999999927 loss=38.465386666666966\n",
            "\n",
            " iteration=769 bias=5.5299999999999265 loss=38.4236200000003\n",
            "\n",
            " iteration=770 bias=5.539999999999926 loss=38.38205333333364\n",
            "\n",
            " iteration=771 bias=5.549999999999926 loss=38.340686666666976\n",
            "\n",
            " iteration=772 bias=5.559999999999926 loss=38.2995200000003\n",
            "\n",
            " iteration=773 bias=5.569999999999926 loss=38.25855333333365\n",
            "\n",
            " iteration=774 bias=5.5799999999999255 loss=38.21778666666697\n",
            "\n",
            " iteration=775 bias=5.589999999999925 loss=38.1772200000003\n",
            "\n",
            " iteration=776 bias=5.599999999999925 loss=38.136853333333626\n",
            "\n",
            " iteration=777 bias=5.609999999999925 loss=38.09668666666696\n",
            "\n",
            " iteration=778 bias=5.619999999999925 loss=38.056720000000304\n",
            "\n",
            " iteration=779 bias=5.629999999999924 loss=38.01695333333363\n",
            "\n",
            " iteration=780 bias=5.639999999999924 loss=37.97738666666695\n",
            "\n",
            " iteration=781 bias=5.649999999999924 loss=37.93802000000029\n",
            "\n",
            " iteration=782 bias=5.659999999999924 loss=37.89885333333364\n",
            "\n",
            " iteration=783 bias=5.6699999999999235 loss=37.85988666666697\n",
            "\n",
            " iteration=784 bias=5.679999999999923 loss=37.8211200000003\n",
            "\n",
            " iteration=785 bias=5.679999999999923 loss=37.82054666666697\n",
            "\n",
            " iteration=786 bias=5.689999999999923 loss=37.77944666666697\n",
            "\n",
            " iteration=787 bias=5.699999999999923 loss=37.73854666666696\n",
            "\n",
            " iteration=788 bias=5.709999999999923 loss=37.69784666666698\n",
            "\n",
            " iteration=789 bias=5.7199999999999225 loss=37.657346666666974\n",
            "\n",
            " iteration=790 bias=5.729999999999922 loss=37.617046666666994\n",
            "\n",
            " iteration=791 bias=5.739999999999922 loss=37.576946666666984\n",
            "\n",
            " iteration=792 bias=5.749999999999922 loss=37.53704666666696\n",
            "\n",
            " iteration=793 bias=5.759999999999922 loss=37.497346666666985\n",
            "\n",
            " iteration=794 bias=5.769999999999921 loss=37.45784666666696\n",
            "\n",
            " iteration=795 bias=5.779999999999921 loss=37.41854666666698\n",
            "\n",
            " iteration=796 bias=5.789999999999921 loss=37.37944666666697\n",
            "\n",
            " iteration=797 bias=5.799999999999921 loss=37.340546666666974\n",
            "\n",
            " iteration=798 bias=5.809999999999921 loss=37.301846666666975\n",
            "\n",
            " iteration=799 bias=5.81999999999992 loss=37.263346666666976\n",
            "\n",
            " iteration=800 bias=5.82999999999992 loss=37.22504666666698\n",
            "\n",
            " iteration=801 bias=5.83999999999992 loss=37.18694666666696\n",
            "\n",
            " iteration=802 bias=5.84999999999992 loss=37.14904666666697\n",
            "\n",
            " iteration=803 bias=5.8599999999999195 loss=37.11134666666696\n",
            "\n",
            " iteration=804 bias=5.8599999999999195 loss=37.10893333333365\n",
            "\n",
            " iteration=805 bias=5.869999999999919 loss=37.06890000000033\n",
            "\n",
            " iteration=806 bias=5.879999999999919 loss=37.02906666666698\n",
            "\n",
            " iteration=807 bias=5.889999999999919 loss=36.98943333333364\n",
            "\n",
            " iteration=808 bias=5.899999999999919 loss=36.95000000000031\n",
            "\n",
            " iteration=809 bias=5.909999999999918 loss=36.91076666666698\n",
            "\n",
            " iteration=810 bias=5.919999999999918 loss=36.87173333333365\n",
            "\n",
            " iteration=811 bias=5.929999999999918 loss=36.83290000000033\n",
            "\n",
            " iteration=812 bias=5.939999999999918 loss=36.79426666666698\n",
            "\n",
            " iteration=813 bias=5.949999999999918 loss=36.75583333333364\n",
            "\n",
            " iteration=814 bias=5.959999999999917 loss=36.717600000000296\n",
            "\n",
            " iteration=815 bias=5.969999999999917 loss=36.679566666666986\n",
            "\n",
            " iteration=816 bias=5.979999999999917 loss=36.64173333333364\n",
            "\n",
            " iteration=817 bias=5.989999999999917 loss=36.60410000000031\n",
            "\n",
            " iteration=818 bias=5.9999999999999165 loss=36.566666666666976\n",
            "\n",
            " iteration=819 bias=6.009999999999916 loss=36.52943333333364\n",
            "\n",
            " iteration=820 bias=6.019999999999916 loss=36.4924000000003\n",
            "\n",
            " iteration=821 bias=6.029999999999916 loss=36.45556666666698\n",
            "\n",
            " iteration=822 bias=6.029999999999916 loss=36.453846666667\n",
            "\n",
            " iteration=823 bias=6.039999999999916 loss=36.41468000000033\n",
            "\n",
            " iteration=824 bias=6.0499999999999154 loss=36.375713333333664\n",
            "\n",
            " iteration=825 bias=6.059999999999915 loss=36.336946666667\n",
            "\n",
            " iteration=826 bias=6.069999999999915 loss=36.29838000000032\n",
            "\n",
            " iteration=827 bias=6.079999999999915 loss=36.26001333333366\n",
            "\n",
            " iteration=828 bias=6.089999999999915 loss=36.22184666666697\n",
            "\n",
            " iteration=829 bias=6.099999999999914 loss=36.183880000000336\n",
            "\n",
            " iteration=830 bias=6.109999999999914 loss=36.14611333333366\n",
            "\n",
            " iteration=831 bias=6.119999999999914 loss=36.10854666666699\n",
            "\n",
            " iteration=832 bias=6.129999999999914 loss=36.071180000000346\n",
            "\n",
            " iteration=833 bias=6.1399999999999135 loss=36.03401333333366\n",
            "\n",
            " iteration=834 bias=6.149999999999913 loss=35.99704666666699\n",
            "\n",
            " iteration=835 bias=6.159999999999913 loss=35.96028000000033\n",
            "\n",
            " iteration=836 bias=6.169999999999913 loss=35.92371333333365\n",
            "\n",
            " iteration=837 bias=6.179999999999913 loss=35.887346666666986\n",
            "\n",
            " iteration=838 bias=6.1899999999999125 loss=35.85118000000033\n",
            "\n",
            " iteration=839 bias=6.199999999999912 loss=35.81521333333365\n",
            "\n",
            " iteration=840 bias=6.199999999999912 loss=35.814186666666984\n",
            "\n",
            " iteration=841 bias=6.209999999999912 loss=35.775886666667\n",
            "\n",
            " iteration=842 bias=6.219999999999912 loss=35.73778666666699\n",
            "\n",
            " iteration=843 bias=6.229999999999912 loss=35.69988666666699\n",
            "\n",
            " iteration=844 bias=6.239999999999911 loss=35.66218666666699\n",
            "\n",
            " iteration=845 bias=6.249999999999911 loss=35.62468666666699\n",
            "\n",
            " iteration=846 bias=6.259999999999911 loss=35.58738666666698\n",
            "\n",
            " iteration=847 bias=6.269999999999911 loss=35.550286666667\n",
            "\n",
            " iteration=848 bias=6.2799999999999105 loss=35.51338666666698\n",
            "\n",
            " iteration=849 bias=6.28999999999991 loss=35.476686666667\n",
            "\n",
            " iteration=850 bias=6.29999999999991 loss=35.440186666667\n",
            "\n",
            " iteration=851 bias=6.30999999999991 loss=35.403886666666985\n",
            "\n",
            " iteration=852 bias=6.31999999999991 loss=35.36778666666698\n",
            "\n",
            " iteration=853 bias=6.3299999999999095 loss=35.331886666666975\n",
            "\n",
            " iteration=854 bias=6.339999999999909 loss=35.296186666666976\n",
            "\n",
            " iteration=855 bias=6.349999999999909 loss=35.26068666666699\n",
            "\n",
            " iteration=856 bias=6.359999999999909 loss=35.22538666666697\n",
            "\n",
            " iteration=857 bias=6.369999999999909 loss=35.19028666666698\n",
            "\n",
            " iteration=858 bias=6.369999999999909 loss=35.189953333333655\n",
            "\n",
            " iteration=859 bias=6.379999999999908 loss=35.15252000000034\n",
            "\n",
            " iteration=860 bias=6.389999999999908 loss=35.115286666667004\n",
            "\n",
            " iteration=861 bias=6.399999999999908 loss=35.07825333333367\n",
            "\n",
            " iteration=862 bias=6.409999999999908 loss=35.04142000000034\n",
            "\n",
            " iteration=863 bias=6.419999999999908 loss=35.00478666666701\n",
            "\n",
            " iteration=864 bias=6.429999999999907 loss=34.96835333333367\n",
            "\n",
            " iteration=865 bias=6.439999999999907 loss=34.93212000000033\n",
            "\n",
            " iteration=866 bias=6.449999999999907 loss=34.89608666666699\n",
            "\n",
            " iteration=867 bias=6.459999999999907 loss=34.86025333333367\n",
            "\n",
            " iteration=868 bias=6.4699999999999065 loss=34.82462000000034\n",
            "\n",
            " iteration=869 bias=6.479999999999906 loss=34.78918666666699\n",
            "\n",
            " iteration=870 bias=6.489999999999906 loss=34.75395333333368\n",
            "\n",
            " iteration=871 bias=6.499999999999906 loss=34.71892000000033\n",
            "\n",
            " iteration=872 bias=6.509999999999906 loss=34.684086666666985\n",
            "\n",
            " iteration=873 bias=6.519999999999905 loss=34.64945333333366\n",
            "\n",
            " iteration=874 bias=6.529999999999905 loss=34.61502000000033\n",
            "\n",
            " iteration=875 bias=6.539999999999905 loss=34.580786666667\n",
            "\n",
            " iteration=876 bias=6.549999999999905 loss=34.54675333333366\n",
            "\n",
            " iteration=877 bias=6.549999999999905 loss=34.54458000000035\n",
            "\n",
            " iteration=878 bias=6.559999999999905 loss=34.50821333333368\n",
            "\n",
            " iteration=879 bias=6.569999999999904 loss=34.472046666667005\n",
            "\n",
            " iteration=880 bias=6.579999999999904 loss=34.43608000000035\n",
            "\n",
            " iteration=881 bias=6.589999999999904 loss=34.40031333333368\n",
            "\n",
            " iteration=882 bias=6.599999999999904 loss=34.364746666667\n",
            "\n",
            " iteration=883 bias=6.6099999999999035 loss=34.32938000000035\n",
            "\n",
            " iteration=884 bias=6.619999999999903 loss=34.29421333333368\n",
            "\n",
            " iteration=885 bias=6.629999999999903 loss=34.259246666667\n",
            "\n",
            " iteration=886 bias=6.639999999999903 loss=34.224480000000334\n",
            "\n",
            " iteration=887 bias=6.649999999999903 loss=34.18991333333367\n",
            "\n",
            " iteration=888 bias=6.659999999999902 loss=34.155546666667\n",
            "\n",
            " iteration=889 bias=6.669999999999902 loss=34.12138000000034\n",
            "\n",
            " iteration=890 bias=6.679999999999902 loss=34.087413333333664\n",
            "\n",
            " iteration=891 bias=6.689999999999902 loss=34.053646666667014\n",
            "\n",
            " iteration=892 bias=6.699999999999902 loss=34.020080000000334\n",
            "\n",
            " iteration=893 bias=6.709999999999901 loss=33.98671333333366\n",
            "\n",
            " iteration=894 bias=6.719999999999901 loss=33.95354666666699\n",
            "\n",
            " iteration=895 bias=6.719999999999901 loss=33.95206666666703\n",
            "\n",
            " iteration=896 bias=6.729999999999901 loss=33.91656666666703\n",
            "\n",
            " iteration=897 bias=6.739999999999901 loss=33.881266666667024\n",
            "\n",
            " iteration=898 bias=6.7499999999999005 loss=33.846166666667024\n",
            "\n",
            " iteration=899 bias=6.7599999999999 loss=33.81126666666702\n",
            "\n",
            " iteration=900 bias=6.7699999999999 loss=33.77656666666702\n",
            "\n",
            " iteration=901 bias=6.7799999999999 loss=33.74206666666702\n",
            "\n",
            " iteration=902 bias=6.7899999999999 loss=33.70776666666702\n",
            "\n",
            " iteration=903 bias=6.7999999999998995 loss=33.67366666666702\n",
            "\n",
            " iteration=904 bias=6.809999999999899 loss=33.63976666666702\n",
            "\n",
            " iteration=905 bias=6.819999999999899 loss=33.60606666666702\n",
            "\n",
            " iteration=906 bias=6.829999999999899 loss=33.57256666666702\n",
            "\n",
            " iteration=907 bias=6.839999999999899 loss=33.53926666666701\n",
            "\n",
            " iteration=908 bias=6.849999999999898 loss=33.506166666667006\n",
            "\n",
            " iteration=909 bias=6.859999999999898 loss=33.473266666667016\n",
            "\n",
            " iteration=910 bias=6.869999999999898 loss=33.44056666666702\n",
            "\n",
            " iteration=911 bias=6.879999999999898 loss=33.40806666666701\n",
            "\n",
            " iteration=912 bias=6.8899999999998975 loss=33.375766666667005\n",
            "\n",
            " iteration=913 bias=6.8899999999998975 loss=33.374980000000356\n",
            "\n",
            " iteration=914 bias=6.899999999999897 loss=33.34034666666702\n",
            "\n",
            " iteration=915 bias=6.909999999999897 loss=33.30591333333368\n",
            "\n",
            " iteration=916 bias=6.919999999999897 loss=33.27168000000035\n",
            "\n",
            " iteration=917 bias=6.929999999999897 loss=33.237646666667004\n",
            "\n",
            " iteration=918 bias=6.9399999999998965 loss=33.20381333333368\n",
            "\n",
            " iteration=919 bias=6.949999999999896 loss=33.170180000000336\n",
            "\n",
            " iteration=920 bias=6.959999999999896 loss=33.13674666666701\n",
            "\n",
            " iteration=921 bias=6.969999999999896 loss=33.10351333333368\n",
            "\n",
            " iteration=922 bias=6.979999999999896 loss=33.07048000000035\n",
            "\n",
            " iteration=923 bias=6.989999999999895 loss=33.037646666667\n",
            "\n",
            " iteration=924 bias=6.999999999999895 loss=33.00501333333367\n",
            "\n",
            " iteration=925 bias=7.009999999999895 loss=32.97258000000033\n",
            "\n",
            " iteration=926 bias=7.019999999999895 loss=32.940346666667004\n",
            "\n",
            " iteration=927 bias=7.0299999999998946 loss=32.90831333333367\n",
            "\n",
            " iteration=928 bias=7.039999999999894 loss=32.87648000000033\n",
            "\n",
            " iteration=929 bias=7.049999999999894 loss=32.844846666667\n",
            "\n",
            " iteration=930 bias=7.059999999999894 loss=32.81341333333366\n",
            "\n",
            " iteration=931 bias=7.059999999999894 loss=32.81332000000036\n",
            "\n",
            " iteration=932 bias=7.069999999999894 loss=32.77955333333369\n",
            "\n",
            " iteration=933 bias=7.0799999999998935 loss=32.74598666666702\n",
            "\n",
            " iteration=934 bias=7.089999999999893 loss=32.71262000000036\n",
            "\n",
            " iteration=935 bias=7.099999999999893 loss=32.67945333333369\n",
            "\n",
            " iteration=936 bias=7.109999999999893 loss=32.646486666667016\n",
            "\n",
            " iteration=937 bias=7.119999999999893 loss=32.61372000000035\n",
            "\n",
            " iteration=938 bias=7.129999999999892 loss=32.581153333333674\n",
            "\n",
            " iteration=939 bias=7.139999999999892 loss=32.54878666666702\n",
            "\n",
            " iteration=940 bias=7.149999999999892 loss=32.516620000000344\n",
            "\n",
            " iteration=941 bias=7.159999999999892 loss=32.48465333333368\n",
            "\n",
            " iteration=942 bias=7.169999999999892 loss=32.45288666666702\n",
            "\n",
            " iteration=943 bias=7.179999999999891 loss=32.42132000000034\n",
            "\n",
            " iteration=944 bias=7.189999999999891 loss=32.38995333333368\n",
            "\n",
            " iteration=945 bias=7.199999999999891 loss=32.35878666666701\n",
            "\n",
            " iteration=946 bias=7.209999999999891 loss=32.32782000000034\n",
            "\n",
            " iteration=947 bias=7.2199999999998905 loss=32.29705333333367\n",
            "\n",
            " iteration=948 bias=7.22999999999989 loss=32.266486666667\n",
            "\n",
            " iteration=949 bias=7.23999999999989 loss=32.236120000000334\n",
            "\n",
            " iteration=950 bias=7.23999999999989 loss=32.23418666666702\n",
            "\n",
            " iteration=951 bias=7.24999999999989 loss=32.20148666666702\n",
            "\n",
            " iteration=952 bias=7.25999999999989 loss=32.16898666666703\n",
            "\n",
            " iteration=953 bias=7.269999999999889 loss=32.13668666666702\n",
            "\n",
            " iteration=954 bias=7.279999999999889 loss=32.10458666666702\n",
            "\n",
            " iteration=955 bias=7.289999999999889 loss=32.072686666667025\n",
            "\n",
            " iteration=956 bias=7.299999999999889 loss=32.040986666667024\n",
            "\n",
            " iteration=957 bias=7.309999999999889 loss=32.009486666667016\n",
            "\n",
            " iteration=958 bias=7.319999999999888 loss=31.978186666667018\n",
            "\n",
            " iteration=959 bias=7.329999999999888 loss=31.947086666667012\n",
            "\n",
            " iteration=960 bias=7.339999999999888 loss=31.916186666667024\n",
            "\n",
            " iteration=961 bias=7.349999999999888 loss=31.885486666667013\n",
            "\n",
            " iteration=962 bias=7.3599999999998875 loss=31.85498666666701\n",
            "\n",
            " iteration=963 bias=7.369999999999887 loss=31.82468666666701\n",
            "\n",
            " iteration=964 bias=7.379999999999887 loss=31.79458666666702\n",
            "\n",
            " iteration=965 bias=7.389999999999887 loss=31.764686666667014\n",
            "\n",
            " iteration=966 bias=7.399999999999887 loss=31.73498666666701\n",
            "\n",
            " iteration=967 bias=7.4099999999998865 loss=31.705486666667007\n",
            "\n",
            " iteration=968 bias=7.4099999999998865 loss=31.704246666667025\n",
            "\n",
            " iteration=969 bias=7.419999999999886 loss=31.672413333333694\n",
            "\n",
            " iteration=970 bias=7.429999999999886 loss=31.640780000000362\n",
            "\n",
            " iteration=971 bias=7.439999999999886 loss=31.60934666666702\n",
            "\n",
            " iteration=972 bias=7.449999999999886 loss=31.57811333333369\n",
            "\n",
            " iteration=973 bias=7.459999999999885 loss=31.547080000000356\n",
            "\n",
            " iteration=974 bias=7.469999999999885 loss=31.51624666666702\n",
            "\n",
            " iteration=975 bias=7.479999999999885 loss=31.485613333333678\n",
            "\n",
            " iteration=976 bias=7.489999999999885 loss=31.455180000000347\n",
            "\n",
            " iteration=977 bias=7.4999999999998845 loss=31.424946666667022\n",
            "\n",
            " iteration=978 bias=7.509999999999884 loss=31.39491333333368\n",
            "\n",
            " iteration=979 bias=7.519999999999884 loss=31.36508000000034\n",
            "\n",
            " iteration=980 bias=7.529999999999884 loss=31.33544666666701\n",
            "\n",
            " iteration=981 bias=7.539999999999884 loss=31.306013333333677\n",
            "\n",
            " iteration=982 bias=7.5499999999998835 loss=31.276780000000336\n",
            "\n",
            " iteration=983 bias=7.559999999999883 loss=31.247746666667002\n",
            "\n",
            " iteration=984 bias=7.569999999999883 loss=31.21891333333367\n",
            "\n",
            " iteration=985 bias=7.579999999999883 loss=31.19028000000034\n",
            "\n",
            " iteration=986 bias=7.579999999999883 loss=31.189733333333695\n",
            "\n",
            " iteration=987 bias=7.589999999999883 loss=31.158766666667024\n",
            "\n",
            " iteration=988 bias=7.599999999999882 loss=31.128000000000362\n",
            "\n",
            " iteration=989 bias=7.609999999999882 loss=31.097433333333687\n",
            "\n",
            " iteration=990 bias=7.619999999999882 loss=31.067066666667028\n",
            "\n",
            " iteration=991 bias=7.629999999999882 loss=31.036900000000355\n",
            "\n",
            " iteration=992 bias=7.6399999999998816 loss=31.006933333333684\n",
            "\n",
            " iteration=993 bias=7.649999999999881 loss=30.97716666666702\n",
            "\n",
            " iteration=994 bias=7.659999999999881 loss=30.947600000000353\n",
            "\n",
            " iteration=995 bias=7.669999999999881 loss=30.91823333333368\n",
            "\n",
            " iteration=996 bias=7.679999999999881 loss=30.889066666667013\n",
            "\n",
            " iteration=997 bias=7.6899999999998805 loss=30.860100000000344\n",
            "\n",
            " iteration=998 bias=7.69999999999988 loss=30.831333333333685\n",
            "\n",
            " iteration=999 bias=7.70999999999988 loss=30.802766666667015\n",
            "\n",
            " iteration=1000 bias=7.71999999999988 loss=30.774400000000345\n",
            "\n",
            " iteration=1001 bias=7.72999999999988 loss=30.74623333333368\n",
            "\n",
            " iteration=1002 bias=7.739999999999879 loss=30.718266666667\n",
            "\n",
            " iteration=1003 bias=7.749999999999879 loss=30.69050000000034\n",
            "\n",
            " iteration=1004 bias=7.759999999999879 loss=30.662933333333665\n",
            "\n",
            " iteration=1005 bias=7.759999999999879 loss=30.660546666667035\n",
            "\n",
            " iteration=1006 bias=7.769999999999879 loss=30.63064666666704\n",
            "\n",
            " iteration=1007 bias=7.779999999999879 loss=30.60094666666703\n",
            "\n",
            " iteration=1008 bias=7.789999999999878 loss=30.57144666666703\n",
            "\n",
            " iteration=1009 bias=7.799999999999878 loss=30.542146666667023\n",
            "\n",
            " iteration=1010 bias=7.809999999999878 loss=30.513046666667016\n",
            "\n",
            " iteration=1011 bias=7.819999999999878 loss=30.48414666666702\n",
            "\n",
            " iteration=1012 bias=7.8299999999998775 loss=30.455446666667026\n",
            "\n",
            " iteration=1013 bias=7.839999999999877 loss=30.426946666667014\n",
            "\n",
            " iteration=1014 bias=7.849999999999877 loss=30.398646666667016\n",
            "\n",
            " iteration=1015 bias=7.859999999999877 loss=30.370546666667018\n",
            "\n",
            " iteration=1016 bias=7.869999999999877 loss=30.34264666666701\n",
            "\n",
            " iteration=1017 bias=7.879999999999876 loss=30.314946666667012\n",
            "\n",
            " iteration=1018 bias=7.889999999999876 loss=30.287446666667005\n",
            "\n",
            " iteration=1019 bias=7.899999999999876 loss=30.260146666667012\n",
            "\n",
            " iteration=1020 bias=7.909999999999876 loss=30.23304666666701\n",
            "\n",
            " iteration=1021 bias=7.919999999999876 loss=30.206146666667003\n",
            "\n",
            " iteration=1022 bias=7.929999999999875 loss=30.179446666667015\n",
            "\n",
            " iteration=1023 bias=7.929999999999875 loss=30.17775333333369\n",
            "\n",
            " iteration=1024 bias=7.939999999999875 loss=30.148720000000356\n",
            "\n",
            " iteration=1025 bias=7.949999999999875 loss=30.119886666667018\n",
            "\n",
            " iteration=1026 bias=7.959999999999875 loss=30.091253333333682\n",
            "\n",
            " iteration=1027 bias=7.9699999999998745 loss=30.062820000000354\n",
            "\n",
            " iteration=1028 bias=7.979999999999874 loss=30.03458666666702\n",
            "\n",
            " iteration=1029 bias=7.989999999999874 loss=30.006553333333688\n",
            "\n",
            " iteration=1030 bias=7.999999999999874 loss=29.978720000000347\n",
            "\n",
            " iteration=1031 bias=8.009999999999874 loss=29.95108666666702\n",
            "\n",
            " iteration=1032 bias=8.019999999999873 loss=29.923653333333682\n",
            "\n",
            " iteration=1033 bias=8.029999999999873 loss=29.89642000000034\n",
            "\n",
            " iteration=1034 bias=8.039999999999873 loss=29.869386666667012\n",
            "\n",
            " iteration=1035 bias=8.049999999999873 loss=29.84255333333367\n",
            "\n",
            " iteration=1036 bias=8.059999999999873 loss=29.815920000000336\n",
            "\n",
            " iteration=1037 bias=8.069999999999872 loss=29.789486666667\n",
            "\n",
            " iteration=1038 bias=8.079999999999872 loss=29.763253333333672\n",
            "\n",
            " iteration=1039 bias=8.089999999999872 loss=29.73722000000033\n",
            "\n",
            " iteration=1040 bias=8.099999999999872 loss=29.711386666666996\n",
            "\n",
            " iteration=1041 bias=8.099999999999872 loss=29.71038666666704\n",
            "\n",
            " iteration=1042 bias=8.109999999999872 loss=29.682220000000356\n",
            "\n",
            " iteration=1043 bias=8.119999999999871 loss=29.65425333333369\n",
            "\n",
            " iteration=1044 bias=8.129999999999871 loss=29.626486666667024\n",
            "\n",
            " iteration=1045 bias=8.139999999999871 loss=29.59892000000036\n",
            "\n",
            " iteration=1046 bias=8.14999999999987 loss=29.57155333333368\n",
            "\n",
            " iteration=1047 bias=8.15999999999987 loss=29.544386666667013\n",
            "\n",
            " iteration=1048 bias=8.16999999999987 loss=29.51742000000035\n",
            "\n",
            " iteration=1049 bias=8.17999999999987 loss=29.490653333333686\n",
            "\n",
            " iteration=1050 bias=8.18999999999987 loss=29.464086666667022\n",
            "\n",
            " iteration=1051 bias=8.19999999999987 loss=29.43772000000034\n",
            "\n",
            " iteration=1052 bias=8.20999999999987 loss=29.41155333333368\n",
            "\n",
            " iteration=1053 bias=8.21999999999987 loss=29.38558666666701\n",
            "\n",
            " iteration=1054 bias=8.229999999999869 loss=29.359820000000337\n",
            "\n",
            " iteration=1055 bias=8.239999999999869 loss=29.334253333333663\n",
            "\n",
            " iteration=1056 bias=8.249999999999869 loss=29.308886666666993\n",
            "\n",
            " iteration=1057 bias=8.259999999999868 loss=29.28372000000034\n",
            "\n",
            " iteration=1058 bias=8.269999999999868 loss=29.258753333333665\n",
            "\n",
            " iteration=1059 bias=8.269999999999868 loss=29.25844666666703\n",
            "\n",
            " iteration=1060 bias=8.279999999999868 loss=29.231146666667026\n",
            "\n",
            " iteration=1061 bias=8.289999999999868 loss=29.204046666667015\n",
            "\n",
            " iteration=1062 bias=8.299999999999867 loss=29.177146666667024\n",
            "\n",
            " iteration=1063 bias=8.309999999999867 loss=29.150446666667015\n",
            "\n",
            " iteration=1064 bias=8.319999999999867 loss=29.123946666667017\n",
            "\n",
            " iteration=1065 bias=8.329999999999867 loss=29.097646666667018\n",
            "\n",
            " iteration=1066 bias=8.339999999999867 loss=29.071546666667015\n",
            "\n",
            " iteration=1067 bias=8.349999999999866 loss=29.045646666667007\n",
            "\n",
            " iteration=1068 bias=8.359999999999866 loss=29.01994666666701\n",
            "\n",
            " iteration=1069 bias=8.369999999999866 loss=28.994446666667002\n",
            "\n",
            " iteration=1070 bias=8.379999999999866 loss=28.969146666667008\n",
            "\n",
            " iteration=1071 bias=8.389999999999866 loss=28.944046666667013\n",
            "\n",
            " iteration=1072 bias=8.399999999999865 loss=28.919146666667004\n",
            "\n",
            " iteration=1073 bias=8.409999999999865 loss=28.89444666666701\n",
            "\n",
            " iteration=1074 bias=8.419999999999865 loss=28.869946666666994\n",
            "\n",
            " iteration=1075 bias=8.429999999999865 loss=28.845646666666998\n",
            "\n",
            " iteration=1076 bias=8.439999999999864 loss=28.821546666666997\n",
            "\n",
            " iteration=1077 bias=8.449999999999864 loss=28.79764666666698\n",
            "\n",
            " iteration=1078 bias=8.449999999999864 loss=28.795500000000345\n",
            "\n",
            " iteration=1079 bias=8.459999999999864 loss=28.76926666666702\n",
            "\n",
            " iteration=1080 bias=8.469999999999864 loss=28.743233333333684\n",
            "\n",
            " iteration=1081 bias=8.479999999999864 loss=28.717400000000342\n",
            "\n",
            " iteration=1082 bias=8.489999999999863 loss=28.69176666666702\n",
            "\n",
            " iteration=1083 bias=8.499999999999863 loss=28.666333333333675\n",
            "\n",
            " iteration=1084 bias=8.509999999999863 loss=28.641100000000336\n",
            "\n",
            " iteration=1085 bias=8.519999999999863 loss=28.616066666667002\n",
            "\n",
            " iteration=1086 bias=8.529999999999863 loss=28.59123333333366\n",
            "\n",
            " iteration=1087 bias=8.539999999999862 loss=28.566600000000335\n",
            "\n",
            " iteration=1088 bias=8.549999999999862 loss=28.542166666667\n",
            "\n",
            " iteration=1089 bias=8.559999999999862 loss=28.517933333333662\n",
            "\n",
            " iteration=1090 bias=8.569999999999862 loss=28.493900000000338\n",
            "\n",
            " iteration=1091 bias=8.579999999999862 loss=28.470066666666995\n",
            "\n",
            " iteration=1092 bias=8.589999999999861 loss=28.44643333333365\n",
            "\n",
            " iteration=1093 bias=8.599999999999861 loss=28.42300000000032\n",
            "\n",
            " iteration=1094 bias=8.60999999999986 loss=28.39976666666698\n",
            "\n",
            " iteration=1095 bias=8.61999999999986 loss=28.376733333333654\n",
            "\n",
            " iteration=1096 bias=8.61999999999986 loss=28.37528000000036\n",
            "\n",
            " iteration=1097 bias=8.62999999999986 loss=28.34991333333368\n",
            "\n",
            " iteration=1098 bias=8.63999999999986 loss=28.324746666667018\n",
            "\n",
            " iteration=1099 bias=8.64999999999986 loss=28.299780000000336\n",
            "\n",
            " iteration=1100 bias=8.65999999999986 loss=28.275013333333685\n",
            "\n",
            " iteration=1101 bias=8.66999999999986 loss=28.25044666666701\n",
            "\n",
            " iteration=1102 bias=8.67999999999986 loss=28.226080000000344\n",
            "\n",
            " iteration=1103 bias=8.68999999999986 loss=28.201913333333682\n",
            "\n",
            " iteration=1104 bias=8.699999999999859 loss=28.177946666667\n",
            "\n",
            " iteration=1105 bias=8.709999999999859 loss=28.15418000000033\n",
            "\n",
            " iteration=1106 bias=8.719999999999859 loss=28.130613333333667\n",
            "\n",
            " iteration=1107 bias=8.729999999999858 loss=28.107246666666985\n",
            "\n",
            " iteration=1108 bias=8.739999999999858 loss=28.084080000000323\n",
            "\n",
            " iteration=1109 bias=8.749999999999858 loss=28.06111333333366\n",
            "\n",
            " iteration=1110 bias=8.759999999999858 loss=28.038346666666985\n",
            "\n",
            " iteration=1111 bias=8.769999999999857 loss=28.015780000000333\n",
            "\n",
            " iteration=1112 bias=8.779999999999857 loss=27.993413333333653\n",
            "\n",
            " iteration=1113 bias=8.789999999999857 loss=27.97124666666698\n",
            "\n",
            " iteration=1114 bias=8.789999999999857 loss=27.970486666667014\n",
            "\n",
            " iteration=1115 bias=8.799999999999857 loss=27.945986666667014\n",
            "\n",
            " iteration=1116 bias=8.809999999999857 loss=27.921686666667025\n",
            "\n",
            " iteration=1117 bias=8.819999999999856 loss=27.89758666666702\n",
            "\n",
            " iteration=1118 bias=8.829999999999856 loss=27.873686666667016\n",
            "\n",
            " iteration=1119 bias=8.839999999999856 loss=27.849986666667007\n",
            "\n",
            " iteration=1120 bias=8.849999999999856 loss=27.826486666666995\n",
            "\n",
            " iteration=1121 bias=8.859999999999856 loss=27.803186666667013\n",
            "\n",
            " iteration=1122 bias=8.869999999999855 loss=27.780086666667\n",
            "\n",
            " iteration=1123 bias=8.879999999999855 loss=27.757186666667\n",
            "\n",
            " iteration=1124 bias=8.889999999999855 loss=27.73448666666701\n",
            "\n",
            " iteration=1125 bias=8.899999999999855 loss=27.711986666667\n",
            "\n",
            " iteration=1126 bias=8.909999999999854 loss=27.689686666666997\n",
            "\n",
            " iteration=1127 bias=8.919999999999854 loss=27.66758666666699\n",
            "\n",
            " iteration=1128 bias=8.929999999999854 loss=27.645686666666982\n",
            "\n",
            " iteration=1129 bias=8.939999999999854 loss=27.62398666666699\n",
            "\n",
            " iteration=1130 bias=8.949999999999854 loss=27.602486666666984\n",
            "\n",
            " iteration=1131 bias=8.959999999999853 loss=27.581186666666976\n",
            "\n",
            " iteration=1132 bias=8.959999999999853 loss=27.581120000000336\n",
            "\n",
            " iteration=1133 bias=8.969999999999853 loss=27.55748666666702\n",
            "\n",
            " iteration=1134 bias=8.979999999999853 loss=27.534053333333677\n",
            "\n",
            " iteration=1135 bias=8.989999999999853 loss=27.510820000000326\n",
            "\n",
            " iteration=1136 bias=8.999999999999853 loss=27.487786666666995\n",
            "\n",
            " iteration=1137 bias=9.009999999999852 loss=27.46495333333366\n",
            "\n",
            " iteration=1138 bias=9.019999999999852 loss=27.442320000000336\n",
            "\n",
            " iteration=1139 bias=9.029999999999852 loss=27.419886666666994\n",
            "\n",
            " iteration=1140 bias=9.039999999999852 loss=27.397653333333654\n",
            "\n",
            " iteration=1141 bias=9.049999999999851 loss=27.375620000000332\n",
            "\n",
            " iteration=1142 bias=9.059999999999851 loss=27.353786666666988\n",
            "\n",
            " iteration=1143 bias=9.069999999999851 loss=27.332153333333647\n",
            "\n",
            " iteration=1144 bias=9.07999999999985 loss=27.310720000000313\n",
            "\n",
            " iteration=1145 bias=9.08999999999985 loss=27.28948666666697\n",
            "\n",
            " iteration=1146 bias=9.09999999999985 loss=27.26845333333365\n",
            "\n",
            " iteration=1147 bias=9.10999999999985 loss=27.24762000000031\n",
            "\n",
            " iteration=1148 bias=9.11999999999985 loss=27.22698666666696\n",
            "\n",
            " iteration=1149 bias=9.12999999999985 loss=27.206553333333652\n",
            "\n",
            " iteration=1150 bias=9.13999999999985 loss=27.186320000000304\n",
            "\n",
            " iteration=1151 bias=9.13999999999985 loss=27.184413333333683\n",
            "\n",
            " iteration=1152 bias=9.14999999999985 loss=27.161846666667007\n",
            "\n",
            " iteration=1153 bias=9.15999999999985 loss=27.139480000000326\n",
            "\n",
            " iteration=1154 bias=9.169999999999849 loss=27.117313333333673\n",
            "\n",
            " iteration=1155 bias=9.179999999999849 loss=27.095346666667\n",
            "\n",
            " iteration=1156 bias=9.189999999999849 loss=27.073580000000327\n",
            "\n",
            " iteration=1157 bias=9.199999999999848 loss=27.052013333333647\n",
            "\n",
            " iteration=1158 bias=9.209999999999848 loss=27.030646666666986\n",
            "\n",
            " iteration=1159 bias=9.219999999999848 loss=27.00948000000033\n",
            "\n",
            " iteration=1160 bias=9.229999999999848 loss=26.98851333333365\n",
            "\n",
            " iteration=1161 bias=9.239999999999847 loss=26.96774666666698\n",
            "\n",
            " iteration=1162 bias=9.249999999999847 loss=26.947180000000312\n",
            "\n",
            " iteration=1163 bias=9.259999999999847 loss=26.926813333333648\n",
            "\n",
            " iteration=1164 bias=9.269999999999847 loss=26.906646666666973\n",
            "\n",
            " iteration=1165 bias=9.279999999999847 loss=26.8866800000003\n",
            "\n",
            " iteration=1166 bias=9.289999999999846 loss=26.86691333333363\n",
            "\n",
            " iteration=1167 bias=9.299999999999846 loss=26.84734666666697\n",
            "\n",
            " iteration=1168 bias=9.309999999999846 loss=26.8279800000003\n",
            "\n",
            " iteration=1169 bias=9.309999999999846 loss=26.826766666667\n",
            "\n",
            " iteration=1170 bias=9.319999999999846 loss=26.805066666666995\n",
            "\n",
            " iteration=1171 bias=9.329999999999846 loss=26.783566666667\n",
            "\n",
            " iteration=1172 bias=9.339999999999845 loss=26.762266666667\n",
            "\n",
            " iteration=1173 bias=9.349999999999845 loss=26.741166666666995\n",
            "\n",
            " iteration=1174 bias=9.359999999999845 loss=26.720266666666987\n",
            "\n",
            " iteration=1175 bias=9.369999999999845 loss=26.69956666666699\n",
            "\n",
            " iteration=1176 bias=9.379999999999844 loss=26.679066666666987\n",
            "\n",
            " iteration=1177 bias=9.389999999999844 loss=26.65876666666698\n",
            "\n",
            " iteration=1178 bias=9.399999999999844 loss=26.638666666666975\n",
            "\n",
            " iteration=1179 bias=9.409999999999844 loss=26.61876666666698\n",
            "\n",
            " iteration=1180 bias=9.419999999999844 loss=26.599066666666978\n",
            "\n",
            " iteration=1181 bias=9.429999999999843 loss=26.579566666666974\n",
            "\n",
            " iteration=1182 bias=9.439999999999843 loss=26.560266666666966\n",
            "\n",
            " iteration=1183 bias=9.449999999999843 loss=26.541166666666975\n",
            "\n",
            " iteration=1184 bias=9.459999999999843 loss=26.52226666666697\n",
            "\n",
            " iteration=1185 bias=9.469999999999843 loss=26.50356666666696\n",
            "\n",
            " iteration=1186 bias=9.479999999999842 loss=26.485066666666956\n",
            "\n",
            " iteration=1187 bias=9.479999999999842 loss=26.48454666666698\n",
            "\n",
            " iteration=1188 bias=9.489999999999842 loss=26.463713333333644\n",
            "\n",
            " iteration=1189 bias=9.499999999999842 loss=26.443080000000315\n",
            "\n",
            " iteration=1190 bias=9.509999999999842 loss=26.422646666666978\n",
            "\n",
            " iteration=1191 bias=9.519999999999841 loss=26.40241333333364\n",
            "\n",
            " iteration=1192 bias=9.529999999999841 loss=26.382380000000314\n",
            "\n",
            " iteration=1193 bias=9.539999999999841 loss=26.362546666666972\n",
            "\n",
            " iteration=1194 bias=9.54999999999984 loss=26.342913333333637\n",
            "\n",
            " iteration=1195 bias=9.55999999999984 loss=26.323480000000295\n",
            "\n",
            " iteration=1196 bias=9.56999999999984 loss=26.30424666666695\n",
            "\n",
            " iteration=1197 bias=9.57999999999984 loss=26.285213333333626\n",
            "\n",
            " iteration=1198 bias=9.58999999999984 loss=26.266380000000293\n",
            "\n",
            " iteration=1199 bias=9.59999999999984 loss=26.247746666666956\n",
            "\n",
            " iteration=1200 bias=9.60999999999984 loss=26.229313333333625\n",
            "\n",
            " iteration=1201 bias=9.61999999999984 loss=26.211080000000287\n",
            "\n",
            " iteration=1202 bias=9.62999999999984 loss=26.193046666666948\n",
            "\n",
            " iteration=1203 bias=9.639999999999839 loss=26.175213333333613\n",
            "\n",
            " iteration=1204 bias=9.649999999999839 loss=26.157580000000266\n",
            "\n",
            " iteration=1205 bias=9.659999999999838 loss=26.140146666666944\n",
            "\n",
            " iteration=1206 bias=9.659999999999838 loss=26.13778666666699\n",
            "\n",
            " iteration=1207 bias=9.669999999999838 loss=26.118020000000318\n",
            "\n",
            " iteration=1208 bias=9.679999999999838 loss=26.098453333333637\n",
            "\n",
            " iteration=1209 bias=9.689999999999838 loss=26.079086666666967\n",
            "\n",
            " iteration=1210 bias=9.699999999999838 loss=26.059920000000314\n",
            "\n",
            " iteration=1211 bias=9.709999999999837 loss=26.040953333333622\n",
            "\n",
            " iteration=1212 bias=9.719999999999837 loss=26.022186666666965\n",
            "\n",
            " iteration=1213 bias=9.729999999999837 loss=26.003620000000307\n",
            "\n",
            " iteration=1214 bias=9.739999999999837 loss=25.985253333333635\n",
            "\n",
            " iteration=1215 bias=9.749999999999837 loss=25.967086666666958\n",
            "\n",
            " iteration=1216 bias=9.759999999999836 loss=25.94912000000028\n",
            "\n",
            " iteration=1217 bias=9.769999999999836 loss=25.931353333333615\n",
            "\n",
            " iteration=1218 bias=9.779999999999836 loss=25.91378666666695\n",
            "\n",
            " iteration=1219 bias=9.789999999999836 loss=25.896420000000276\n",
            "\n",
            " iteration=1220 bias=9.799999999999836 loss=25.879253333333608\n",
            "\n",
            " iteration=1221 bias=9.809999999999835 loss=25.862286666666947\n",
            "\n",
            " iteration=1222 bias=9.819999999999835 loss=25.845520000000285\n",
            "\n",
            " iteration=1223 bias=9.829999999999835 loss=25.828953333333608\n",
            "\n",
            " iteration=1224 bias=9.829999999999835 loss=25.827286666666975\n",
            "\n",
            " iteration=1225 bias=9.839999999999835 loss=25.808386666666973\n",
            "\n",
            " iteration=1226 bias=9.849999999999834 loss=25.78968666666697\n",
            "\n",
            " iteration=1227 bias=9.859999999999834 loss=25.771186666666974\n",
            "\n",
            " iteration=1228 bias=9.869999999999834 loss=25.75288666666697\n",
            "\n",
            " iteration=1229 bias=9.879999999999834 loss=25.734786666666952\n",
            "\n",
            " iteration=1230 bias=9.889999999999834 loss=25.71688666666696\n",
            "\n",
            " iteration=1231 bias=9.899999999999833 loss=25.69918666666696\n",
            "\n",
            " iteration=1232 bias=9.909999999999833 loss=25.681686666666952\n",
            "\n",
            " iteration=1233 bias=9.919999999999833 loss=25.66438666666695\n",
            "\n",
            " iteration=1234 bias=9.929999999999833 loss=25.647286666666954\n",
            "\n",
            " iteration=1235 bias=9.939999999999833 loss=25.630386666666947\n",
            "\n",
            " iteration=1236 bias=9.949999999999832 loss=25.613686666666947\n",
            "\n",
            " iteration=1237 bias=9.959999999999832 loss=25.597186666666936\n",
            "\n",
            " iteration=1238 bias=9.969999999999832 loss=25.580886666666935\n",
            "\n",
            " iteration=1239 bias=9.979999999999832 loss=25.564786666666937\n",
            "\n",
            " iteration=1240 bias=9.989999999999831 loss=25.548886666666935\n",
            "\n",
            " iteration=1241 bias=9.999999999999831 loss=25.53318666666693\n",
            "\n",
            " iteration=1242 bias=9.999999999999831 loss=25.532213333333637\n",
            "\n",
            " iteration=1243 bias=10.009999999999831 loss=25.514180000000295\n",
            "\n",
            " iteration=1244 bias=10.01999999999983 loss=25.49634666666697\n",
            "\n",
            " iteration=1245 bias=10.02999999999983 loss=25.478713333333634\n",
            "\n",
            " iteration=1246 bias=10.03999999999983 loss=25.461280000000286\n",
            "\n",
            " iteration=1247 bias=10.04999999999983 loss=25.444046666666964\n",
            "\n",
            " iteration=1248 bias=10.05999999999983 loss=25.427013333333623\n",
            "\n",
            " iteration=1249 bias=10.06999999999983 loss=25.410180000000285\n",
            "\n",
            " iteration=1250 bias=10.07999999999983 loss=25.39354666666695\n",
            "\n",
            " iteration=1251 bias=10.08999999999983 loss=25.377113333333607\n",
            "\n",
            " iteration=1252 bias=10.09999999999983 loss=25.360880000000286\n",
            "\n",
            " iteration=1253 bias=10.109999999999829 loss=25.344846666666943\n",
            "\n",
            " iteration=1254 bias=10.119999999999829 loss=25.3290133333336\n",
            "\n",
            " iteration=1255 bias=10.129999999999828 loss=25.313380000000276\n",
            "\n",
            " iteration=1256 bias=10.139999999999828 loss=25.297946666666938\n",
            "\n",
            " iteration=1257 bias=10.149999999999828 loss=25.282713333333596\n",
            "\n",
            " iteration=1258 bias=10.159999999999828 loss=25.26768000000026\n",
            "\n",
            " iteration=1259 bias=10.169999999999828 loss=25.25284666666692\n",
            "\n",
            " iteration=1260 bias=10.169999999999828 loss=25.25256666666695\n",
            "\n",
            " iteration=1261 bias=10.179999999999827 loss=25.235400000000286\n",
            "\n",
            " iteration=1262 bias=10.189999999999827 loss=25.218433333333618\n",
            "\n",
            " iteration=1263 bias=10.199999999999827 loss=25.201666666666952\n",
            "\n",
            " iteration=1264 bias=10.209999999999827 loss=25.18510000000028\n",
            "\n",
            " iteration=1265 bias=10.219999999999827 loss=25.16873333333361\n",
            "\n",
            " iteration=1266 bias=10.229999999999826 loss=25.152566666666942\n",
            "\n",
            " iteration=1267 bias=10.239999999999826 loss=25.13660000000028\n",
            "\n",
            " iteration=1268 bias=10.249999999999826 loss=25.120833333333596\n",
            "\n",
            " iteration=1269 bias=10.259999999999826 loss=25.105266666666935\n",
            "\n",
            " iteration=1270 bias=10.269999999999825 loss=25.089900000000263\n",
            "\n",
            " iteration=1271 bias=10.279999999999825 loss=25.074733333333594\n",
            "\n",
            " iteration=1272 bias=10.289999999999825 loss=25.059766666666928\n",
            "\n",
            " iteration=1273 bias=10.299999999999825 loss=25.045000000000247\n",
            "\n",
            " iteration=1274 bias=10.309999999999825 loss=25.030433333333587\n",
            "\n",
            " iteration=1275 bias=10.319999999999824 loss=25.016066666666916\n",
            "\n",
            " iteration=1276 bias=10.329999999999824 loss=25.001900000000244\n",
            "\n",
            " iteration=1277 bias=10.339999999999824 loss=24.98793333333357\n",
            "\n",
            " iteration=1278 bias=10.349999999999824 loss=24.974166666666903\n",
            "\n",
            " iteration=1279 bias=10.349999999999824 loss=24.972046666666945\n",
            "\n",
            " iteration=1280 bias=10.359999999999824 loss=24.955946666666946\n",
            "\n",
            " iteration=1281 bias=10.369999999999823 loss=24.940046666666944\n",
            "\n",
            " iteration=1282 bias=10.379999999999823 loss=24.92434666666695\n",
            "\n",
            " iteration=1283 bias=10.389999999999823 loss=24.908846666666935\n",
            "\n",
            " iteration=1284 bias=10.399999999999823 loss=24.893546666666932\n",
            "\n",
            " iteration=1285 bias=10.409999999999823 loss=24.878446666666935\n",
            "\n",
            " iteration=1286 bias=10.419999999999822 loss=24.863546666666938\n",
            "\n",
            " iteration=1287 bias=10.429999999999822 loss=24.848846666666926\n",
            "\n",
            " iteration=1288 bias=10.439999999999822 loss=24.834346666666924\n",
            "\n",
            " iteration=1289 bias=10.449999999999822 loss=24.820046666666922\n",
            "\n",
            " iteration=1290 bias=10.459999999999821 loss=24.805946666666912\n",
            "\n",
            " iteration=1291 bias=10.469999999999821 loss=24.792046666666913\n",
            "\n",
            " iteration=1292 bias=10.479999999999821 loss=24.77834666666691\n",
            "\n",
            " iteration=1293 bias=10.48999999999982 loss=24.764846666666916\n",
            "\n",
            " iteration=1294 bias=10.49999999999982 loss=24.751546666666915\n",
            "\n",
            " iteration=1295 bias=10.50999999999982 loss=24.738446666666906\n",
            "\n",
            " iteration=1296 bias=10.51999999999982 loss=24.725546666666897\n",
            "\n",
            " iteration=1297 bias=10.51999999999982 loss=24.724120000000276\n",
            "\n",
            " iteration=1298 bias=10.52999999999982 loss=24.708886666666942\n",
            "\n",
            " iteration=1299 bias=10.53999999999982 loss=24.693853333333596\n",
            "\n",
            " iteration=1300 bias=10.54999999999982 loss=24.679020000000275\n",
            "\n",
            " iteration=1301 bias=10.55999999999982 loss=24.664386666666932\n",
            "\n",
            " iteration=1302 bias=10.569999999999819 loss=24.649953333333592\n",
            "\n",
            " iteration=1303 bias=10.579999999999819 loss=24.63572000000025\n",
            "\n",
            " iteration=1304 bias=10.589999999999819 loss=24.62168666666691\n",
            "\n",
            " iteration=1305 bias=10.599999999999818 loss=24.607853333333587\n",
            "\n",
            " iteration=1306 bias=10.609999999999818 loss=24.59422000000025\n",
            "\n",
            " iteration=1307 bias=10.619999999999818 loss=24.580786666666903\n",
            "\n",
            " iteration=1308 bias=10.629999999999818 loss=24.56755333333358\n",
            "\n",
            " iteration=1309 bias=10.639999999999818 loss=24.554520000000245\n",
            "\n",
            " iteration=1310 bias=10.649999999999817 loss=24.541686666666894\n",
            "\n",
            " iteration=1311 bias=10.659999999999817 loss=24.529053333333554\n",
            "\n",
            " iteration=1312 bias=10.669999999999817 loss=24.516620000000213\n",
            "\n",
            " iteration=1313 bias=10.679999999999817 loss=24.504386666666896\n",
            "\n",
            " iteration=1314 bias=10.689999999999817 loss=24.492353333333558\n",
            "\n",
            " iteration=1315 bias=10.689999999999817 loss=24.49162000000026\n",
            "\n",
            " iteration=1316 bias=10.699999999999816 loss=24.4772533333336\n",
            "\n",
            " iteration=1317 bias=10.709999999999816 loss=24.463086666666918\n",
            "\n",
            " iteration=1318 bias=10.719999999999816 loss=24.44912000000026\n",
            "\n",
            " iteration=1319 bias=10.729999999999816 loss=24.435353333333587\n",
            "\n",
            " iteration=1320 bias=10.739999999999815 loss=24.42178666666691\n",
            "\n",
            " iteration=1321 bias=10.749999999999815 loss=24.40842000000026\n",
            "\n",
            " iteration=1322 bias=10.759999999999815 loss=24.395253333333585\n",
            "\n",
            " iteration=1323 bias=10.769999999999815 loss=24.382286666666904\n",
            "\n",
            " iteration=1324 bias=10.779999999999815 loss=24.36952000000024\n",
            "\n",
            " iteration=1325 bias=10.789999999999814 loss=24.356953333333557\n",
            "\n",
            " iteration=1326 bias=10.799999999999814 loss=24.344586666666896\n",
            "\n",
            " iteration=1327 bias=10.809999999999814 loss=24.33242000000023\n",
            "\n",
            " iteration=1328 bias=10.819999999999814 loss=24.320453333333553\n",
            "\n",
            " iteration=1329 bias=10.829999999999814 loss=24.308686666666897\n",
            "\n",
            " iteration=1330 bias=10.839999999999813 loss=24.297120000000216\n",
            "\n",
            " iteration=1331 bias=10.849999999999813 loss=24.28575333333355\n",
            "\n",
            " iteration=1332 bias=10.859999999999813 loss=24.274586666666885\n",
            "\n",
            " iteration=1333 bias=10.859999999999813 loss=24.274546666666918\n",
            "\n",
            " iteration=1334 bias=10.869999999999813 loss=24.261046666666918\n",
            "\n",
            " iteration=1335 bias=10.879999999999812 loss=24.247746666666913\n",
            "\n",
            " iteration=1336 bias=10.889999999999812 loss=24.23464666666691\n",
            "\n",
            " iteration=1337 bias=10.899999999999812 loss=24.221746666666903\n",
            "\n",
            " iteration=1338 bias=10.909999999999812 loss=24.209046666666904\n",
            "\n",
            " iteration=1339 bias=10.919999999999812 loss=24.1965466666669\n",
            "\n",
            " iteration=1340 bias=10.929999999999811 loss=24.18424666666689\n",
            "\n",
            " iteration=1341 bias=10.939999999999811 loss=24.172146666666894\n",
            "\n",
            " iteration=1342 bias=10.949999999999811 loss=24.16024666666689\n",
            "\n",
            " iteration=1343 bias=10.95999999999981 loss=24.148546666666892\n",
            "\n",
            " iteration=1344 bias=10.96999999999981 loss=24.13704666666689\n",
            "\n",
            " iteration=1345 bias=10.97999999999981 loss=24.125746666666878\n",
            "\n",
            " iteration=1346 bias=10.98999999999981 loss=24.114646666666875\n",
            "\n",
            " iteration=1347 bias=10.99999999999981 loss=24.103746666666872\n",
            "\n",
            " iteration=1348 bias=11.00999999999981 loss=24.09304666666686\n",
            "\n",
            " iteration=1349 bias=11.01999999999981 loss=24.082546666666868\n",
            "\n",
            " iteration=1350 bias=11.02999999999981 loss=24.072246666666864\n",
            "\n",
            " iteration=1351 bias=11.039999999999809 loss=24.062146666666866\n",
            "\n",
            " iteration=1352 bias=11.039999999999809 loss=24.060266666666905\n",
            "\n",
            " iteration=1353 bias=11.049999999999809 loss=24.047833333333568\n",
            "\n",
            " iteration=1354 bias=11.059999999999809 loss=24.035600000000223\n",
            "\n",
            " iteration=1355 bias=11.069999999999808 loss=24.02356666666689\n",
            "\n",
            " iteration=1356 bias=11.079999999999808 loss=24.011733333333556\n",
            "\n",
            " iteration=1357 bias=11.089999999999808 loss=24.00010000000022\n",
            "\n",
            " iteration=1358 bias=11.099999999999808 loss=23.98866666666688\n",
            "\n",
            " iteration=1359 bias=11.109999999999808 loss=23.977433333333547\n",
            "\n",
            " iteration=1360 bias=11.119999999999807 loss=23.96640000000021\n",
            "\n",
            " iteration=1361 bias=11.129999999999807 loss=23.955566666666872\n",
            "\n",
            " iteration=1362 bias=11.139999999999807 loss=23.944933333333534\n",
            "\n",
            " iteration=1363 bias=11.149999999999807 loss=23.934500000000188\n",
            "\n",
            " iteration=1364 bias=11.159999999999807 loss=23.924266666666867\n",
            "\n",
            " iteration=1365 bias=11.169999999999806 loss=23.91423333333352\n",
            "\n",
            " iteration=1366 bias=11.179999999999806 loss=23.904400000000187\n",
            "\n",
            " iteration=1367 bias=11.189999999999806 loss=23.894766666666857\n",
            "\n",
            " iteration=1368 bias=11.199999999999806 loss=23.88533333333352\n",
            "\n",
            " iteration=1369 bias=11.209999999999805 loss=23.87610000000018\n",
            "\n",
            " iteration=1370 bias=11.209999999999805 loss=23.874913333333556\n",
            "\n",
            " iteration=1371 bias=11.219999999999805 loss=23.86334666666688\n",
            "\n",
            " iteration=1372 bias=11.229999999999805 loss=23.851980000000232\n",
            "\n",
            " iteration=1373 bias=11.239999999999805 loss=23.840813333333553\n",
            "\n",
            " iteration=1374 bias=11.249999999999805 loss=23.82984666666689\n",
            "\n",
            " iteration=1375 bias=11.259999999999804 loss=23.81908000000021\n",
            "\n",
            " iteration=1376 bias=11.269999999999804 loss=23.808513333333533\n",
            "\n",
            " iteration=1377 bias=11.279999999999804 loss=23.79814666666687\n",
            "\n",
            " iteration=1378 bias=11.289999999999804 loss=23.787980000000193\n",
            "\n",
            " iteration=1379 bias=11.299999999999804 loss=23.778013333333526\n",
            "\n",
            " iteration=1380 bias=11.309999999999803 loss=23.76824666666686\n",
            "\n",
            " iteration=1381 bias=11.319999999999803 loss=23.758680000000197\n",
            "\n",
            " iteration=1382 bias=11.329999999999803 loss=23.749313333333525\n",
            "\n",
            " iteration=1383 bias=11.339999999999803 loss=23.740146666666842\n",
            "\n",
            " iteration=1384 bias=11.349999999999802 loss=23.73118000000017\n",
            "\n",
            " iteration=1385 bias=11.359999999999802 loss=23.72241333333351\n",
            "\n",
            " iteration=1386 bias=11.369999999999802 loss=23.713846666666832\n",
            "\n",
            " iteration=1387 bias=11.379999999999802 loss=23.705480000000158\n",
            "\n",
            " iteration=1388 bias=11.379999999999802 loss=23.704986666666887\n",
            "\n",
            " iteration=1389 bias=11.389999999999802 loss=23.694286666666876\n",
            "\n",
            " iteration=1390 bias=11.399999999999801 loss=23.68378666666687\n",
            "\n",
            " iteration=1391 bias=11.409999999999801 loss=23.67348666666686\n",
            "\n",
            " iteration=1392 bias=11.419999999999801 loss=23.66338666666687\n",
            "\n",
            " iteration=1393 bias=11.4299999999998 loss=23.65348666666686\n",
            "\n",
            " iteration=1394 bias=11.4399999999998 loss=23.643786666666852\n",
            "\n",
            " iteration=1395 bias=11.4499999999998 loss=23.63428666666686\n",
            "\n",
            " iteration=1396 bias=11.4599999999998 loss=23.624986666666857\n",
            "\n",
            " iteration=1397 bias=11.4699999999998 loss=23.615886666666846\n",
            "\n",
            " iteration=1398 bias=11.4799999999998 loss=23.606986666666838\n",
            "\n",
            " iteration=1399 bias=11.4899999999998 loss=23.59828666666683\n",
            "\n",
            " iteration=1400 bias=11.4999999999998 loss=23.58978666666684\n",
            "\n",
            " iteration=1401 bias=11.509999999999799 loss=23.58148666666683\n",
            "\n",
            " iteration=1402 bias=11.519999999999799 loss=23.573386666666824\n",
            "\n",
            " iteration=1403 bias=11.529999999999799 loss=23.565486666666832\n",
            "\n",
            " iteration=1404 bias=11.539999999999798 loss=23.557786666666825\n",
            "\n",
            " iteration=1405 bias=11.549999999999798 loss=23.55028666666682\n",
            "\n",
            " iteration=1406 bias=11.559999999999798 loss=23.54298666666681\n",
            "\n",
            " iteration=1407 bias=11.559999999999798 loss=23.54065333333354\n",
            "\n",
            " iteration=1408 bias=11.569999999999798 loss=23.5310200000002\n",
            "\n",
            " iteration=1409 bias=11.579999999999798 loss=23.521586666666856\n",
            "\n",
            " iteration=1410 bias=11.589999999999797 loss=23.51235333333352\n",
            "\n",
            " iteration=1411 bias=11.599999999999797 loss=23.503320000000194\n",
            "\n",
            " iteration=1412 bias=11.609999999999797 loss=23.494486666666845\n",
            "\n",
            " iteration=1413 bias=11.619999999999797 loss=23.485853333333505\n",
            "\n",
            " iteration=1414 bias=11.629999999999797 loss=23.477420000000173\n",
            "\n",
            " iteration=1415 bias=11.639999999999796 loss=23.469186666666836\n",
            "\n",
            " iteration=1416 bias=11.649999999999796 loss=23.461153333333503\n",
            "\n",
            " iteration=1417 bias=11.659999999999796 loss=23.453320000000165\n",
            "\n",
            " iteration=1418 bias=11.669999999999796 loss=23.445686666666823\n",
            "\n",
            " iteration=1419 bias=11.679999999999795 loss=23.43825333333349\n",
            "\n",
            " iteration=1420 bias=11.689999999999795 loss=23.431020000000146\n",
            "\n",
            " iteration=1421 bias=11.699999999999795 loss=23.423986666666806\n",
            "\n",
            " iteration=1422 bias=11.709999999999795 loss=23.417153333333477\n",
            "\n",
            " iteration=1423 bias=11.719999999999795 loss=23.410520000000137\n",
            "\n",
            " iteration=1424 bias=11.729999999999794 loss=23.404086666666803\n",
            "\n",
            " iteration=1425 bias=11.729999999999794 loss=23.40244666666684\n",
            "\n",
            " iteration=1426 bias=11.739999999999794 loss=23.393680000000177\n",
            "\n",
            " iteration=1427 bias=11.749999999999794 loss=23.385113333333507\n",
            "\n",
            " iteration=1428 bias=11.759999999999794 loss=23.37674666666684\n",
            "\n",
            " iteration=1429 bias=11.769999999999794 loss=23.368580000000165\n",
            "\n",
            " iteration=1430 bias=11.779999999999793 loss=23.360613333333497\n",
            "\n",
            " iteration=1431 bias=11.789999999999793 loss=23.35284666666683\n",
            "\n",
            " iteration=1432 bias=11.799999999999793 loss=23.34528000000016\n",
            "\n",
            " iteration=1433 bias=11.809999999999793 loss=23.337913333333482\n",
            "\n",
            " iteration=1434 bias=11.819999999999792 loss=23.330746666666816\n",
            "\n",
            " iteration=1435 bias=11.829999999999792 loss=23.323780000000138\n",
            "\n",
            " iteration=1436 bias=11.839999999999792 loss=23.317013333333474\n",
            "\n",
            " iteration=1437 bias=11.849999999999792 loss=23.31044666666681\n",
            "\n",
            " iteration=1438 bias=11.859999999999792 loss=23.304080000000127\n",
            "\n",
            " iteration=1439 bias=11.869999999999791 loss=23.297913333333465\n",
            "\n",
            " iteration=1440 bias=11.879999999999791 loss=23.291946666666796\n",
            "\n",
            " iteration=1441 bias=11.889999999999791 loss=23.286180000000122\n",
            "\n",
            " iteration=1442 bias=11.89999999999979 loss=23.28061333333345\n",
            "\n",
            " iteration=1443 bias=11.89999999999979 loss=23.279666666666838\n",
            "\n",
            " iteration=1444 bias=11.90999999999979 loss=23.271766666666828\n",
            "\n",
            " iteration=1445 bias=11.91999999999979 loss=23.264066666666825\n",
            "\n",
            " iteration=1446 bias=11.92999999999979 loss=23.256566666666817\n",
            "\n",
            " iteration=1447 bias=11.93999999999979 loss=23.249266666666824\n",
            "\n",
            " iteration=1448 bias=11.94999999999979 loss=23.24216666666681\n",
            "\n",
            " iteration=1449 bias=11.95999999999979 loss=23.235266666666806\n",
            "\n",
            " iteration=1450 bias=11.96999999999979 loss=23.228566666666797\n",
            "\n",
            " iteration=1451 bias=11.979999999999789 loss=23.22206666666681\n",
            "\n",
            " iteration=1452 bias=11.989999999999789 loss=23.2157666666668\n",
            "\n",
            " iteration=1453 bias=11.999999999999789 loss=23.209666666666788\n",
            "\n",
            " iteration=1454 bias=12.009999999999788 loss=23.203766666666795\n",
            "\n",
            " iteration=1455 bias=12.019999999999788 loss=23.198066666666783\n",
            "\n",
            " iteration=1456 bias=12.029999999999788 loss=23.192566666666774\n",
            "\n",
            " iteration=1457 bias=12.039999999999788 loss=23.18726666666677\n",
            "\n",
            " iteration=1458 bias=12.049999999999788 loss=23.182166666666774\n",
            "\n",
            " iteration=1459 bias=12.059999999999787 loss=23.177266666666775\n",
            "\n",
            " iteration=1460 bias=12.069999999999787 loss=23.172566666666764\n",
            "\n",
            " iteration=1461 bias=12.069999999999787 loss=23.17231333333348\n",
            "\n",
            " iteration=1462 bias=12.079999999999787 loss=23.16528000000014\n",
            "\n",
            " iteration=1463 bias=12.089999999999787 loss=23.1584466666668\n",
            "\n",
            " iteration=1464 bias=12.099999999999786 loss=23.151813333333482\n",
            "\n",
            " iteration=1465 bias=12.109999999999786 loss=23.145380000000138\n",
            "\n",
            " iteration=1466 bias=12.119999999999786 loss=23.139146666666793\n",
            "\n",
            " iteration=1467 bias=12.129999999999786 loss=23.13311333333347\n",
            "\n",
            " iteration=1468 bias=12.139999999999786 loss=23.127280000000134\n",
            "\n",
            " iteration=1469 bias=12.149999999999785 loss=23.121646666666784\n",
            "\n",
            " iteration=1470 bias=12.159999999999785 loss=23.116213333333445\n",
            "\n",
            " iteration=1471 bias=12.169999999999785 loss=23.110980000000108\n",
            "\n",
            " iteration=1472 bias=12.179999999999785 loss=23.10594666666678\n",
            "\n",
            " iteration=1473 bias=12.189999999999785 loss=23.101113333333437\n",
            "\n",
            " iteration=1474 bias=12.199999999999784 loss=23.096480000000096\n",
            "\n",
            " iteration=1475 bias=12.209999999999784 loss=23.092046666666768\n",
            "\n",
            " iteration=1476 bias=12.219999999999784 loss=23.08781333333343\n",
            "\n",
            " iteration=1477 bias=12.229999999999784 loss=23.08378000000009\n",
            "\n",
            " iteration=1478 bias=12.239999999999783 loss=23.079946666666743\n",
            "\n",
            " iteration=1479 bias=12.249999999999783 loss=23.07631333333341\n",
            "\n",
            " iteration=1480 bias=12.249999999999783 loss=23.07422000000014\n",
            "\n",
            " iteration=1481 bias=12.259999999999783 loss=23.06825333333346\n",
            "\n",
            " iteration=1482 bias=12.269999999999783 loss=23.062486666666796\n",
            "\n",
            " iteration=1483 bias=12.279999999999783 loss=23.05692000000013\n",
            "\n",
            " iteration=1484 bias=12.289999999999782 loss=23.051553333333445\n",
            "\n",
            " iteration=1485 bias=12.299999999999782 loss=23.046386666666773\n",
            "\n",
            " iteration=1486 bias=12.309999999999782 loss=23.04142000000011\n",
            "\n",
            " iteration=1487 bias=12.319999999999782 loss=23.036653333333444\n",
            "\n",
            " iteration=1488 bias=12.329999999999782 loss=23.03208666666677\n",
            "\n",
            " iteration=1489 bias=12.339999999999781 loss=23.027720000000087\n",
            "\n",
            " iteration=1490 bias=12.349999999999781 loss=23.023553333333428\n",
            "\n",
            " iteration=1491 bias=12.359999999999781 loss=23.019586666666765\n",
            "\n",
            " iteration=1492 bias=12.36999999999978 loss=23.015820000000087\n",
            "\n",
            " iteration=1493 bias=12.37999999999978 loss=23.01225333333341\n",
            "\n",
            " iteration=1494 bias=12.38999999999978 loss=23.008886666666736\n",
            "\n",
            " iteration=1495 bias=12.39999999999978 loss=23.005720000000075\n",
            "\n",
            " iteration=1496 bias=12.40999999999978 loss=23.002753333333406\n",
            "\n",
            " iteration=1497 bias=12.41999999999978 loss=22.999986666666725\n",
            "\n",
            " iteration=1498 bias=12.41999999999978 loss=22.99858666666678\n",
            "\n",
            " iteration=1499 bias=12.42999999999978 loss=22.993486666666783\n",
            "\n",
            " iteration=1500 bias=12.43999999999978 loss=22.98858666666677\n",
            "\n",
            " iteration=1501 bias=12.449999999999779 loss=22.98388666666676\n",
            "\n",
            " iteration=1502 bias=12.459999999999779 loss=22.97938666666676\n",
            "\n",
            " iteration=1503 bias=12.469999999999779 loss=22.97508666666676\n",
            "\n",
            " iteration=1504 bias=12.479999999999778 loss=22.970986666666757\n",
            "\n",
            " iteration=1505 bias=12.489999999999778 loss=22.967086666666752\n",
            "\n",
            " iteration=1506 bias=12.499999999999778 loss=22.963386666666747\n",
            "\n",
            " iteration=1507 bias=12.509999999999778 loss=22.95988666666674\n",
            "\n",
            " iteration=1508 bias=12.519999999999778 loss=22.95658666666673\n",
            "\n",
            " iteration=1509 bias=12.529999999999777 loss=22.953486666666727\n",
            "\n",
            " iteration=1510 bias=12.539999999999777 loss=22.950586666666727\n",
            "\n",
            " iteration=1511 bias=12.549999999999777 loss=22.94788666666672\n",
            "\n",
            " iteration=1512 bias=12.559999999999777 loss=22.94538666666672\n",
            "\n",
            " iteration=1513 bias=12.569999999999776 loss=22.943086666666712\n",
            "\n",
            " iteration=1514 bias=12.579999999999776 loss=22.940986666666713\n",
            "\n",
            " iteration=1515 bias=12.589999999999776 loss=22.939086666666707\n",
            "\n",
            " iteration=1516 bias=12.589999999999776 loss=22.938380000000098\n",
            "\n",
            " iteration=1517 bias=12.599999999999776 loss=22.934146666666756\n",
            "\n",
            " iteration=1518 bias=12.609999999999776 loss=22.93011333333342\n",
            "\n",
            " iteration=1519 bias=12.619999999999775 loss=22.926280000000087\n",
            "\n",
            " iteration=1520 bias=12.629999999999775 loss=22.922646666666743\n",
            "\n",
            " iteration=1521 bias=12.639999999999775 loss=22.9192133333334\n",
            "\n",
            " iteration=1522 bias=12.649999999999775 loss=22.915980000000065\n",
            "\n",
            " iteration=1523 bias=12.659999999999775 loss=22.912946666666738\n",
            "\n",
            " iteration=1524 bias=12.669999999999774 loss=22.910113333333396\n",
            "\n",
            " iteration=1525 bias=12.679999999999774 loss=22.907480000000053\n",
            "\n",
            " iteration=1526 bias=12.689999999999774 loss=22.905046666666728\n",
            "\n",
            " iteration=1527 bias=12.699999999999774 loss=22.902813333333388\n",
            "\n",
            " iteration=1528 bias=12.709999999999773 loss=22.90078000000004\n",
            "\n",
            " iteration=1529 bias=12.719999999999773 loss=22.8989466666667\n",
            "\n",
            " iteration=1530 bias=12.729999999999773 loss=22.897313333333365\n",
            "\n",
            " iteration=1531 bias=12.739999999999773 loss=22.895880000000037\n",
            "\n",
            " iteration=1532 bias=12.749999999999773 loss=22.89464666666669\n",
            "\n",
            " iteration=1533 bias=12.759999999999772 loss=22.89361333333335\n",
            "\n",
            " iteration=1534 bias=12.759999999999772 loss=22.893600000000074\n",
            "\n",
            " iteration=1535 bias=12.769999999999772 loss=22.890233333333402\n",
            "\n",
            " iteration=1536 bias=12.779999999999772 loss=22.887066666666733\n",
            "\n",
            " iteration=1537 bias=12.789999999999772 loss=22.88410000000006\n",
            "\n",
            " iteration=1538 bias=12.799999999999772 loss=22.881333333333387\n",
            "\n",
            " iteration=1539 bias=12.809999999999771 loss=22.87876666666672\n",
            "\n",
            " iteration=1540 bias=12.819999999999771 loss=22.876400000000046\n",
            "\n",
            " iteration=1541 bias=12.829999999999771 loss=22.874233333333386\n",
            "\n",
            " iteration=1542 bias=12.83999999999977 loss=22.872266666666707\n",
            "\n",
            " iteration=1543 bias=12.84999999999977 loss=22.870500000000035\n",
            "\n",
            " iteration=1544 bias=12.85999999999977 loss=22.868933333333363\n",
            "\n",
            " iteration=1545 bias=12.86999999999977 loss=22.86756666666669\n",
            "\n",
            " iteration=1546 bias=12.87999999999977 loss=22.866400000000024\n",
            "\n",
            " iteration=1547 bias=12.88999999999977 loss=22.86543333333335\n",
            "\n",
            " iteration=1548 bias=12.89999999999977 loss=22.864666666666682\n",
            "\n",
            " iteration=1549 bias=12.90999999999977 loss=22.86410000000002\n",
            "\n",
            " iteration=1550 bias=12.919999999999769 loss=22.863733333333336\n",
            "\n",
            " iteration=1551 bias=12.929999999999769 loss=22.86356666666666\n",
            "\n",
            "w=1.1000000000000008, b=12.929999999999769\n",
            "Prediction: x=20, Y=>34.92999999999978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "showHistory(X,Y, history,5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "ZC4rlg-Cah8r",
        "outputId": "7f4e9b93-62dc-4c83-99ad-0549c5788a90"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Prediction: x=3, Y=>{predict(100, w,b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyXje8gq-kSd",
        "outputId": "288339f2-48d5-4fcc-f6ac-697324a7cacc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: x=3, Y=>122.92999999999984\n"
          ]
        }
      ]
    }
  ]
}