{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyTR4yzLPi4NPVEurrGtTW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanyelMorales/MLLearneerRepo/blob/main/cats_LinearRegressionWithAnimation_bias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "blepHTTmuPmq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "afd1bca4-5065-4e3e-c7c2-d4ef53ac6fc5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import seaborn as sns\n",
        "\n",
        "xLabel = \"Cat Weight (kg)\"\n",
        "yLabel = \"Cat Length (cm)\"\n",
        "\n",
        "plt.ion()\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "X,Y =np.loadtxt(\"https://raw.githubusercontent.com/DanyelMorales/MLLearneerRepo/main/dataset/cats.txt\",skiprows=1, unpack=True)\n",
        "plt.xlabel(xLabel, fontsize=30)\n",
        "plt.ylabel(yLabel,fontsize=30)\n",
        "ax.plot(X,Y, \"bo\")\n",
        "\n",
        "def graph(X1,Y1, id):\n",
        "    ax.set_xlim(0, id)\n",
        "    ax.cla()\n",
        "    ax.plot(X1,Y1, \"bo\")\n",
        "    line1, = ax.plot(X1,X1, 'b-')\n",
        "    display(fig)\n",
        "    clear_output(wait = True)\n",
        "    plt.pause(0.5)\n",
        "    return line1\n",
        "\n",
        "\n",
        "def updateLine(X1,Y1,theLine):\n",
        "    theLine.set_ydata(Y1)\n",
        "    theLine.set_xdata(X1)\n",
        "    display(fig)\n",
        "    clear_output(wait = True)\n",
        "    #plt.pause(0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MsXmVcG-0kHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showHistory(X,Y,historyW, step=2):\n",
        "  theLine=graph(X,Y,1)\n",
        "  updateLineL = lambda w :  updateLine(X,predict(X,w[0],w[1]),theLine)\n",
        "  for i in range(0,len(historyW),step):\n",
        "    w=historyW[i]\n",
        "    updateLineL(w)\n",
        "  updateLineL(historyW[-1])\n",
        "\n",
        "def predict(X, w,b):\n",
        "  return X * w + b\n",
        "\n",
        "def loss(X,Y,w,b):\n",
        "  error = predict(X,w,b)-Y\n",
        "  squared_error = error ** 2\n",
        "  return np.average(squared_error)\n",
        "\n",
        "def train(iterations, X,Y, lr):\n",
        "  w=b=0\n",
        "  historyW=[]\n",
        "  for i in range(iterations):\n",
        "    historyW.append([w,b])\n",
        "    tmpLoss=loss(X,Y,w,b)\n",
        "    print(f\"\\n iteration={i} bias={b} loss={tmpLoss}\")\n",
        "    if(loss(X,Y,w+lr,b)<tmpLoss):\n",
        "      w+=lr\n",
        "    elif(loss(X,Y,w-lr,b)<tmpLoss):\n",
        "      w-=lr\n",
        "    elif(loss(X,Y,w,b+lr)<tmpLoss):\n",
        "      b+=lr\n",
        "    elif(loss(X,Y,w,b-lr)<tmpLoss):\n",
        "      b-=lr\n",
        "    else:\n",
        "      return [w,b, historyW]\n",
        "  raise Exception(f\"cannot convey after {iterations} iterations\")\n",
        "\n",
        "w,b, history =train(20000, X,Y, 0.01)\n",
        "print(f\"\\nw={w}, b={b}\")\n",
        "print(f\"Prediction: x=8, Y=>{predict(8, w,b)}\")\n"
      ],
      "metadata": {
        "id": "8PAq5GGAQvqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f514e90c-dd7d-48c7-f556-80ed6a2e1296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " iteration=0 bias=0 loss=1479.1666666666667\n",
            "\n",
            " iteration=1 bias=0 loss=1474.7533166666665\n",
            "\n",
            " iteration=2 bias=0 loss=1470.3466\n",
            "\n",
            " iteration=3 bias=0 loss=1465.9465166666666\n",
            "\n",
            " iteration=4 bias=0 loss=1461.5530666666664\n",
            "\n",
            " iteration=5 bias=0 loss=1457.16625\n",
            "\n",
            " iteration=6 bias=0 loss=1452.7860666666668\n",
            "\n",
            " iteration=7 bias=0 loss=1448.4125166666665\n",
            "\n",
            " iteration=8 bias=0 loss=1444.0456000000001\n",
            "\n",
            " iteration=9 bias=0 loss=1439.6853166666667\n",
            "\n",
            " iteration=10 bias=0 loss=1435.3316666666667\n",
            "\n",
            " iteration=11 bias=0 loss=1430.98465\n",
            "\n",
            " iteration=12 bias=0 loss=1426.644266666667\n",
            "\n",
            " iteration=13 bias=0 loss=1422.3105166666667\n",
            "\n",
            " iteration=14 bias=0 loss=1417.9834\n",
            "\n",
            " iteration=15 bias=0 loss=1413.662916666667\n",
            "\n",
            " iteration=16 bias=0 loss=1409.3490666666667\n",
            "\n",
            " iteration=17 bias=0 loss=1405.0418499999998\n",
            "\n",
            " iteration=18 bias=0 loss=1400.741266666667\n",
            "\n",
            " iteration=19 bias=0 loss=1396.4473166666667\n",
            "\n",
            " iteration=20 bias=0 loss=1392.1599999999999\n",
            "\n",
            " iteration=21 bias=0 loss=1387.8793166666667\n",
            "\n",
            " iteration=22 bias=0 loss=1383.6052666666667\n",
            "\n",
            " iteration=23 bias=0 loss=1379.33785\n",
            "\n",
            " iteration=24 bias=0 loss=1375.0770666666667\n",
            "\n",
            " iteration=25 bias=0 loss=1370.8229166666667\n",
            "\n",
            " iteration=26 bias=0 loss=1366.5754\n",
            "\n",
            " iteration=27 bias=0 loss=1362.3345166666668\n",
            "\n",
            " iteration=28 bias=0 loss=1358.1002666666666\n",
            "\n",
            " iteration=29 bias=0 loss=1353.87265\n",
            "\n",
            " iteration=30 bias=0 loss=1349.6516666666666\n",
            "\n",
            " iteration=31 bias=0 loss=1345.4373166666667\n",
            "\n",
            " iteration=32 bias=0 loss=1341.2296\n",
            "\n",
            " iteration=33 bias=0 loss=1337.0285166666665\n",
            "\n",
            " iteration=34 bias=0 loss=1332.8340666666666\n",
            "\n",
            " iteration=35 bias=0 loss=1328.6462499999998\n",
            "\n",
            " iteration=36 bias=0 loss=1324.4650666666664\n",
            "\n",
            " iteration=37 bias=0 loss=1320.2905166666667\n",
            "\n",
            " iteration=38 bias=0 loss=1316.1226\n",
            "\n",
            " iteration=39 bias=0 loss=1311.9613166666663\n",
            "\n",
            " iteration=40 bias=0 loss=1307.8066666666666\n",
            "\n",
            " iteration=41 bias=0 loss=1303.6586499999996\n",
            "\n",
            " iteration=42 bias=0 loss=1299.5172666666665\n",
            "\n",
            " iteration=43 bias=0 loss=1295.3825166666666\n",
            "\n",
            " iteration=44 bias=0 loss=1291.2543999999998\n",
            "\n",
            " iteration=45 bias=0 loss=1287.1329166666667\n",
            "\n",
            " iteration=46 bias=0 loss=1283.0180666666665\n",
            "\n",
            " iteration=47 bias=0 loss=1278.90985\n",
            "\n",
            " iteration=48 bias=0 loss=1274.8082666666667\n",
            "\n",
            " iteration=49 bias=0 loss=1270.7133166666665\n",
            "\n",
            " iteration=50 bias=0 loss=1266.625\n",
            "\n",
            " iteration=51 bias=0 loss=1262.5433166666664\n",
            "\n",
            " iteration=52 bias=0 loss=1258.4682666666665\n",
            "\n",
            " iteration=53 bias=0 loss=1254.39985\n",
            "\n",
            " iteration=54 bias=0 loss=1250.3380666666665\n",
            "\n",
            " iteration=55 bias=0 loss=1246.2829166666666\n",
            "\n",
            " iteration=56 bias=0 loss=1242.2343999999998\n",
            "\n",
            " iteration=57 bias=0 loss=1238.1925166666665\n",
            "\n",
            " iteration=58 bias=0 loss=1234.1572666666666\n",
            "\n",
            " iteration=59 bias=0 loss=1230.1286499999999\n",
            "\n",
            " iteration=60 bias=0 loss=1226.1066666666666\n",
            "\n",
            " iteration=61 bias=0 loss=1222.0913166666667\n",
            "\n",
            " iteration=62 bias=0 loss=1218.0826\n",
            "\n",
            " iteration=63 bias=0 loss=1214.0805166666664\n",
            "\n",
            " iteration=64 bias=0 loss=1210.0850666666665\n",
            "\n",
            " iteration=65 bias=0 loss=1206.0962499999998\n",
            "\n",
            " iteration=66 bias=0 loss=1202.1140666666665\n",
            "\n",
            " iteration=67 bias=0 loss=1198.1385166666666\n",
            "\n",
            " iteration=68 bias=0 loss=1194.1696\n",
            "\n",
            " iteration=69 bias=0 loss=1190.2073166666667\n",
            "\n",
            " iteration=70 bias=0 loss=1186.2516666666666\n",
            "\n",
            " iteration=71 bias=0 loss=1182.3026499999999\n",
            "\n",
            " iteration=72 bias=0 loss=1178.3602666666666\n",
            "\n",
            " iteration=73 bias=0 loss=1174.4245166666665\n",
            "\n",
            " iteration=74 bias=0 loss=1170.4954\n",
            "\n",
            " iteration=75 bias=0 loss=1166.5729166666667\n",
            "\n",
            " iteration=76 bias=0 loss=1162.6570666666664\n",
            "\n",
            " iteration=77 bias=0 loss=1158.74785\n",
            "\n",
            " iteration=78 bias=0 loss=1154.8452666666665\n",
            "\n",
            " iteration=79 bias=0 loss=1150.9493166666664\n",
            "\n",
            " iteration=80 bias=0 loss=1147.0599999999997\n",
            "\n",
            " iteration=81 bias=0 loss=1143.1773166666665\n",
            "\n",
            " iteration=82 bias=0 loss=1139.3012666666666\n",
            "\n",
            " iteration=83 bias=0 loss=1135.43185\n",
            "\n",
            " iteration=84 bias=0 loss=1131.5690666666662\n",
            "\n",
            " iteration=85 bias=0 loss=1127.7129166666664\n",
            "\n",
            " iteration=86 bias=0 loss=1123.8634\n",
            "\n",
            " iteration=87 bias=0 loss=1120.0205166666663\n",
            "\n",
            " iteration=88 bias=0 loss=1116.1842666666664\n",
            "\n",
            " iteration=89 bias=0 loss=1112.3546499999998\n",
            "\n",
            " iteration=90 bias=0 loss=1108.5316666666665\n",
            "\n",
            " iteration=91 bias=0 loss=1104.7153166666667\n",
            "\n",
            " iteration=92 bias=0 loss=1100.9055999999998\n",
            "\n",
            " iteration=93 bias=0 loss=1097.1025166666664\n",
            "\n",
            " iteration=94 bias=0 loss=1093.3060666666663\n",
            "\n",
            " iteration=95 bias=0 loss=1089.5162499999997\n",
            "\n",
            " iteration=96 bias=0 loss=1085.7330666666664\n",
            "\n",
            " iteration=97 bias=0 loss=1081.9565166666664\n",
            "\n",
            " iteration=98 bias=0 loss=1078.1865999999998\n",
            "\n",
            " iteration=99 bias=0 loss=1074.4233166666666\n",
            "\n",
            " iteration=100 bias=0 loss=1070.6666666666663\n",
            "\n",
            " iteration=101 bias=0 loss=1066.91665\n",
            "\n",
            " iteration=102 bias=0 loss=1063.1732666666664\n",
            "\n",
            " iteration=103 bias=0 loss=1059.4365166666662\n",
            "\n",
            " iteration=104 bias=0 loss=1055.7063999999998\n",
            "\n",
            " iteration=105 bias=0 loss=1051.9829166666664\n",
            "\n",
            " iteration=106 bias=0 loss=1048.2660666666663\n",
            "\n",
            " iteration=107 bias=0 loss=1044.5558499999997\n",
            "\n",
            " iteration=108 bias=0 loss=1040.8522666666665\n",
            "\n",
            " iteration=109 bias=0 loss=1037.1553166666665\n",
            "\n",
            " iteration=110 bias=0 loss=1033.4649999999997\n",
            "\n",
            " iteration=111 bias=0 loss=1029.7813166666663\n",
            "\n",
            " iteration=112 bias=0 loss=1026.1042666666665\n",
            "\n",
            " iteration=113 bias=0 loss=1022.4338499999998\n",
            "\n",
            " iteration=114 bias=0 loss=1018.7700666666665\n",
            "\n",
            " iteration=115 bias=0 loss=1015.1129166666664\n",
            "\n",
            " iteration=116 bias=0 loss=1011.4623999999995\n",
            "\n",
            " iteration=117 bias=0 loss=1007.8185166666664\n",
            "\n",
            " iteration=118 bias=0 loss=1004.1812666666664\n",
            "\n",
            " iteration=119 bias=0 loss=1000.5506499999997\n",
            "\n",
            " iteration=120 bias=0 loss=996.9266666666663\n",
            "\n",
            " iteration=121 bias=0 loss=993.3093166666663\n",
            "\n",
            " iteration=122 bias=0 loss=989.6985999999997\n",
            "\n",
            " iteration=123 bias=0 loss=986.0945166666664\n",
            "\n",
            " iteration=124 bias=0 loss=982.4970666666665\n",
            "\n",
            " iteration=125 bias=0 loss=978.9062499999997\n",
            "\n",
            " iteration=126 bias=0 loss=975.3220666666665\n",
            "\n",
            " iteration=127 bias=0 loss=971.7445166666662\n",
            "\n",
            " iteration=128 bias=0 loss=968.1735999999996\n",
            "\n",
            " iteration=129 bias=0 loss=964.6093166666664\n",
            "\n",
            " iteration=130 bias=0 loss=961.0516666666664\n",
            "\n",
            " iteration=131 bias=0 loss=957.5006499999996\n",
            "\n",
            " iteration=132 bias=0 loss=953.9562666666662\n",
            "\n",
            " iteration=133 bias=0 loss=950.4185166666663\n",
            "\n",
            " iteration=134 bias=0 loss=946.8873999999997\n",
            "\n",
            " iteration=135 bias=0 loss=943.3629166666664\n",
            "\n",
            " iteration=136 bias=0 loss=939.8450666666662\n",
            "\n",
            " iteration=137 bias=0 loss=936.3338499999996\n",
            "\n",
            " iteration=138 bias=0 loss=932.8292666666663\n",
            "\n",
            " iteration=139 bias=0 loss=929.3313166666663\n",
            "\n",
            " iteration=140 bias=0 loss=925.8399999999996\n",
            "\n",
            " iteration=141 bias=0 loss=922.3553166666665\n",
            "\n",
            " iteration=142 bias=0 loss=918.8772666666664\n",
            "\n",
            " iteration=143 bias=0 loss=915.4058499999996\n",
            "\n",
            " iteration=144 bias=0 loss=911.9410666666663\n",
            "\n",
            " iteration=145 bias=0 loss=908.4829166666664\n",
            "\n",
            " iteration=146 bias=0 loss=905.0313999999997\n",
            "\n",
            " iteration=147 bias=0 loss=901.5865166666664\n",
            "\n",
            " iteration=148 bias=0 loss=898.1482666666662\n",
            "\n",
            " iteration=149 bias=0 loss=894.7166499999997\n",
            "\n",
            " iteration=150 bias=0 loss=891.2916666666664\n",
            "\n",
            " iteration=151 bias=0 loss=887.8733166666663\n",
            "\n",
            " iteration=152 bias=0 loss=884.4615999999996\n",
            "\n",
            " iteration=153 bias=0 loss=881.0565166666662\n",
            "\n",
            " iteration=154 bias=0 loss=877.6580666666663\n",
            "\n",
            " iteration=155 bias=0 loss=874.2662499999997\n",
            "\n",
            " iteration=156 bias=0 loss=870.8810666666662\n",
            "\n",
            " iteration=157 bias=0 loss=867.5025166666661\n",
            "\n",
            " iteration=158 bias=0 loss=864.1305999999996\n",
            "\n",
            " iteration=159 bias=0 loss=860.7653166666661\n",
            "\n",
            " iteration=160 bias=0 loss=857.4066666666662\n",
            "\n",
            " iteration=161 bias=0 loss=854.0546499999996\n",
            "\n",
            " iteration=162 bias=0 loss=850.7092666666663\n",
            "\n",
            " iteration=163 bias=0 loss=847.3705166666663\n",
            "\n",
            " iteration=164 bias=0 loss=844.0383999999995\n",
            "\n",
            " iteration=165 bias=0 loss=840.7129166666663\n",
            "\n",
            " iteration=166 bias=0 loss=837.3940666666663\n",
            "\n",
            " iteration=167 bias=0 loss=834.0818499999995\n",
            "\n",
            " iteration=168 bias=0 loss=830.7762666666663\n",
            "\n",
            " iteration=169 bias=0 loss=827.4773166666663\n",
            "\n",
            " iteration=170 bias=0 loss=824.1849999999996\n",
            "\n",
            " iteration=171 bias=0 loss=820.8993166666663\n",
            "\n",
            " iteration=172 bias=0 loss=817.6202666666662\n",
            "\n",
            " iteration=173 bias=0 loss=814.3478499999995\n",
            "\n",
            " iteration=174 bias=0 loss=811.0820666666663\n",
            "\n",
            " iteration=175 bias=0 loss=807.8229166666662\n",
            "\n",
            " iteration=176 bias=0 loss=804.5703999999996\n",
            "\n",
            " iteration=177 bias=0 loss=801.3245166666662\n",
            "\n",
            " iteration=178 bias=0 loss=798.0852666666664\n",
            "\n",
            " iteration=179 bias=0 loss=794.8526499999997\n",
            "\n",
            " iteration=180 bias=0 loss=791.6266666666661\n",
            "\n",
            " iteration=181 bias=0 loss=788.4073166666662\n",
            "\n",
            " iteration=182 bias=0 loss=785.1945999999995\n",
            "\n",
            " iteration=183 bias=0 loss=781.9885166666662\n",
            "\n",
            " iteration=184 bias=0 loss=778.7890666666663\n",
            "\n",
            " iteration=185 bias=0 loss=775.5962499999996\n",
            "\n",
            " iteration=186 bias=0 loss=772.4100666666662\n",
            "\n",
            " iteration=187 bias=0 loss=769.2305166666662\n",
            "\n",
            " iteration=188 bias=0 loss=766.0575999999995\n",
            "\n",
            " iteration=189 bias=0 loss=762.8913166666662\n",
            "\n",
            " iteration=190 bias=0 loss=759.7316666666662\n",
            "\n",
            " iteration=191 bias=0 loss=756.5786499999995\n",
            "\n",
            " iteration=192 bias=0 loss=753.4322666666661\n",
            "\n",
            " iteration=193 bias=0 loss=750.2925166666661\n",
            "\n",
            " iteration=194 bias=0 loss=747.1593999999996\n",
            "\n",
            " iteration=195 bias=0 loss=744.0329166666662\n",
            "\n",
            " iteration=196 bias=0 loss=740.9130666666662\n",
            "\n",
            " iteration=197 bias=0 loss=737.7998499999995\n",
            "\n",
            " iteration=198 bias=0 loss=734.6932666666662\n",
            "\n",
            " iteration=199 bias=0 loss=731.593316666666\n",
            "\n",
            " iteration=200 bias=0 loss=728.4999999999995\n",
            "\n",
            " iteration=201 bias=0 loss=725.4133166666664\n",
            "\n",
            " iteration=202 bias=0 loss=722.3332666666662\n",
            "\n",
            " iteration=203 bias=0 loss=719.2598499999998\n",
            "\n",
            " iteration=204 bias=0 loss=716.1930666666666\n",
            "\n",
            " iteration=205 bias=0 loss=713.1329166666665\n",
            "\n",
            " iteration=206 bias=0 loss=710.0794\n",
            "\n",
            " iteration=207 bias=0 loss=707.0325166666667\n",
            "\n",
            " iteration=208 bias=0 loss=703.9922666666668\n",
            "\n",
            " iteration=209 bias=0 loss=700.9586500000001\n",
            "\n",
            " iteration=210 bias=0 loss=697.9316666666668\n",
            "\n",
            " iteration=211 bias=0 loss=694.911316666667\n",
            "\n",
            " iteration=212 bias=0 loss=691.8976000000004\n",
            "\n",
            " iteration=213 bias=0 loss=688.8905166666669\n",
            "\n",
            " iteration=214 bias=0 loss=685.8900666666672\n",
            "\n",
            " iteration=215 bias=0 loss=682.8962500000006\n",
            "\n",
            " iteration=216 bias=0 loss=679.9090666666673\n",
            "\n",
            " iteration=217 bias=0 loss=676.9285166666674\n",
            "\n",
            " iteration=218 bias=0 loss=673.9546000000007\n",
            "\n",
            " iteration=219 bias=0 loss=670.9873166666674\n",
            "\n",
            " iteration=220 bias=0 loss=668.0266666666676\n",
            "\n",
            " iteration=221 bias=0 loss=665.0726500000009\n",
            "\n",
            " iteration=222 bias=0 loss=662.1252666666676\n",
            "\n",
            " iteration=223 bias=0 loss=659.1845166666676\n",
            "\n",
            " iteration=224 bias=0 loss=656.2504000000012\n",
            "\n",
            " iteration=225 bias=0 loss=653.3229166666679\n",
            "\n",
            " iteration=226 bias=0 loss=650.4020666666679\n",
            "\n",
            " iteration=227 bias=0 loss=647.4878500000012\n",
            "\n",
            " iteration=228 bias=0 loss=644.580266666668\n",
            "\n",
            " iteration=229 bias=0 loss=641.6793166666681\n",
            "\n",
            " iteration=230 bias=0 loss=638.7850000000014\n",
            "\n",
            " iteration=231 bias=0 loss=635.8973166666682\n",
            "\n",
            " iteration=232 bias=0 loss=633.0162666666683\n",
            "\n",
            " iteration=233 bias=0 loss=630.1418500000017\n",
            "\n",
            " iteration=234 bias=0 loss=627.2740666666683\n",
            "\n",
            " iteration=235 bias=0 loss=624.4129166666685\n",
            "\n",
            " iteration=236 bias=0 loss=621.5584000000018\n",
            "\n",
            " iteration=237 bias=0 loss=618.7105166666686\n",
            "\n",
            " iteration=238 bias=0 loss=615.8692666666685\n",
            "\n",
            " iteration=239 bias=0 loss=613.034650000002\n",
            "\n",
            " iteration=240 bias=0 loss=610.2066666666686\n",
            "\n",
            " iteration=241 bias=0 loss=607.3853166666687\n",
            "\n",
            " iteration=242 bias=0 loss=604.5706000000022\n",
            "\n",
            " iteration=243 bias=0 loss=601.7625166666688\n",
            "\n",
            " iteration=244 bias=0 loss=598.9610666666689\n",
            "\n",
            " iteration=245 bias=0 loss=596.1662500000023\n",
            "\n",
            " iteration=246 bias=0 loss=593.378066666669\n",
            "\n",
            " iteration=247 bias=0 loss=590.5965166666691\n",
            "\n",
            " iteration=248 bias=0 loss=587.8216000000025\n",
            "\n",
            " iteration=249 bias=0 loss=585.0533166666693\n",
            "\n",
            " iteration=250 bias=0 loss=582.2916666666692\n",
            "\n",
            " iteration=251 bias=0 loss=579.5366500000026\n",
            "\n",
            " iteration=252 bias=0 loss=576.7882666666693\n",
            "\n",
            " iteration=253 bias=0 loss=574.0465166666694\n",
            "\n",
            " iteration=254 bias=0 loss=571.3114000000028\n",
            "\n",
            " iteration=255 bias=0 loss=568.5829166666695\n",
            "\n",
            " iteration=256 bias=0 loss=565.8610666666696\n",
            "\n",
            " iteration=257 bias=0 loss=563.145850000003\n",
            "\n",
            " iteration=258 bias=0 loss=560.4372666666696\n",
            "\n",
            " iteration=259 bias=0 loss=557.7353166666696\n",
            "\n",
            " iteration=260 bias=0 loss=555.040000000003\n",
            "\n",
            " iteration=261 bias=0 loss=552.3513166666698\n",
            "\n",
            " iteration=262 bias=0 loss=549.6692666666698\n",
            "\n",
            " iteration=263 bias=0 loss=546.9938500000031\n",
            "\n",
            " iteration=264 bias=0 loss=544.32506666667\n",
            "\n",
            " iteration=265 bias=0 loss=541.66291666667\n",
            "\n",
            " iteration=266 bias=0 loss=539.0074000000035\n",
            "\n",
            " iteration=267 bias=0 loss=536.3585166666701\n",
            "\n",
            " iteration=268 bias=0 loss=533.7162666666701\n",
            "\n",
            " iteration=269 bias=0 loss=531.0806500000035\n",
            "\n",
            " iteration=270 bias=0 loss=528.4516666666702\n",
            "\n",
            " iteration=271 bias=0 loss=525.8293166666703\n",
            "\n",
            " iteration=272 bias=0 loss=523.2136000000037\n",
            "\n",
            " iteration=273 bias=0 loss=520.6045166666704\n",
            "\n",
            " iteration=274 bias=0 loss=518.0020666666704\n",
            "\n",
            " iteration=275 bias=0 loss=515.4062500000038\n",
            "\n",
            " iteration=276 bias=0 loss=512.8170666666705\n",
            "\n",
            " iteration=277 bias=0 loss=510.2345166666706\n",
            "\n",
            " iteration=278 bias=0 loss=507.6586000000039\n",
            "\n",
            " iteration=279 bias=0 loss=505.08931666667064\n",
            "\n",
            " iteration=280 bias=0 loss=502.5266666666707\n",
            "\n",
            " iteration=281 bias=0 loss=499.97065000000407\n",
            "\n",
            " iteration=282 bias=0 loss=497.4212666666708\n",
            "\n",
            " iteration=283 bias=0 loss=494.87851666667075\n",
            "\n",
            " iteration=284 bias=0 loss=492.34240000000426\n",
            "\n",
            " iteration=285 bias=0 loss=489.8129166666709\n",
            "\n",
            " iteration=286 bias=0 loss=487.29006666667095\n",
            "\n",
            " iteration=287 bias=0 loss=484.7738500000043\n",
            "\n",
            " iteration=288 bias=0 loss=482.26426666667095\n",
            "\n",
            " iteration=289 bias=0 loss=479.7613166666711\n",
            "\n",
            " iteration=290 bias=0 loss=477.2650000000044\n",
            "\n",
            " iteration=291 bias=0 loss=474.77531666667113\n",
            "\n",
            " iteration=292 bias=0 loss=472.2922666666712\n",
            "\n",
            " iteration=293 bias=0 loss=469.81585000000456\n",
            "\n",
            " iteration=294 bias=0 loss=467.3460666666712\n",
            "\n",
            " iteration=295 bias=0 loss=464.8829166666713\n",
            "\n",
            " iteration=296 bias=0 loss=462.42640000000466\n",
            "\n",
            " iteration=297 bias=0 loss=459.97651666667133\n",
            "\n",
            " iteration=298 bias=0 loss=457.5332666666714\n",
            "\n",
            " iteration=299 bias=0 loss=455.0966500000048\n",
            "\n",
            " iteration=300 bias=0 loss=452.6666666666715\n",
            "\n",
            " iteration=301 bias=0 loss=450.24331666667155\n",
            "\n",
            " iteration=302 bias=0 loss=447.8266000000049\n",
            "\n",
            " iteration=303 bias=0 loss=445.41651666667167\n",
            "\n",
            " iteration=304 bias=0 loss=443.0130666666717\n",
            "\n",
            " iteration=305 bias=0 loss=440.61625000000504\n",
            "\n",
            " iteration=306 bias=0 loss=438.22606666667167\n",
            "\n",
            " iteration=307 bias=0 loss=435.84251666667177\n",
            "\n",
            " iteration=308 bias=0 loss=433.46560000000517\n",
            "\n",
            " iteration=309 bias=0 loss=431.09531666667186\n",
            "\n",
            " iteration=310 bias=0 loss=428.73166666667186\n",
            "\n",
            " iteration=311 bias=0 loss=426.37465000000526\n",
            "\n",
            " iteration=312 bias=0 loss=424.02426666667196\n",
            "\n",
            " iteration=313 bias=0 loss=421.68051666667196\n",
            "\n",
            " iteration=314 bias=0 loss=419.3434000000054\n",
            "\n",
            " iteration=315 bias=0 loss=417.012916666672\n",
            "\n",
            " iteration=316 bias=0 loss=414.6890666666721\n",
            "\n",
            " iteration=317 bias=0 loss=412.37185000000545\n",
            "\n",
            " iteration=318 bias=0 loss=410.0612666666722\n",
            "\n",
            " iteration=319 bias=0 loss=407.75731666667224\n",
            "\n",
            " iteration=320 bias=0 loss=405.4600000000055\n",
            "\n",
            " iteration=321 bias=0 loss=403.1693166666723\n",
            "\n",
            " iteration=322 bias=0 loss=400.8852666666723\n",
            "\n",
            " iteration=323 bias=0 loss=398.6078500000057\n",
            "\n",
            " iteration=324 bias=0 loss=396.3370666666724\n",
            "\n",
            " iteration=325 bias=0 loss=394.0729166666724\n",
            "\n",
            " iteration=326 bias=0 loss=391.8154000000057\n",
            "\n",
            " iteration=327 bias=0 loss=389.56451666667243\n",
            "\n",
            " iteration=328 bias=0 loss=387.3202666666725\n",
            "\n",
            " iteration=329 bias=0 loss=385.0826500000058\n",
            "\n",
            " iteration=330 bias=0 loss=382.85166666667254\n",
            "\n",
            " iteration=331 bias=0 loss=380.6273166666726\n",
            "\n",
            " iteration=332 bias=0 loss=378.4096000000059\n",
            "\n",
            " iteration=333 bias=0 loss=376.1985166666727\n",
            "\n",
            " iteration=334 bias=0 loss=373.9940666666726\n",
            "\n",
            " iteration=335 bias=0 loss=371.79625000000607\n",
            "\n",
            " iteration=336 bias=0 loss=369.6050666666727\n",
            "\n",
            " iteration=337 bias=0 loss=367.4205166666727\n",
            "\n",
            " iteration=338 bias=0 loss=365.2426000000061\n",
            "\n",
            " iteration=339 bias=0 loss=363.07131666667283\n",
            "\n",
            " iteration=340 bias=0 loss=360.90666666667283\n",
            "\n",
            " iteration=341 bias=0 loss=358.7486500000062\n",
            "\n",
            " iteration=342 bias=0 loss=356.59726666667285\n",
            "\n",
            " iteration=343 bias=0 loss=354.45251666667286\n",
            "\n",
            " iteration=344 bias=0 loss=352.3144000000063\n",
            "\n",
            " iteration=345 bias=0 loss=350.18291666667295\n",
            "\n",
            " iteration=346 bias=0 loss=348.0580666666729\n",
            "\n",
            " iteration=347 bias=0 loss=345.93985000000634\n",
            "\n",
            " iteration=348 bias=0 loss=343.828266666673\n",
            "\n",
            " iteration=349 bias=0 loss=341.72331666667304\n",
            "\n",
            " iteration=350 bias=0 loss=339.6250000000064\n",
            "\n",
            " iteration=351 bias=0 loss=337.5333166666731\n",
            "\n",
            " iteration=352 bias=0 loss=335.44826666667313\n",
            "\n",
            " iteration=353 bias=0 loss=333.3698500000065\n",
            "\n",
            " iteration=354 bias=0 loss=331.2980666666732\n",
            "\n",
            " iteration=355 bias=0 loss=329.2329166666732\n",
            "\n",
            " iteration=356 bias=0 loss=327.17440000000653\n",
            "\n",
            " iteration=357 bias=0 loss=325.1225166666732\n",
            "\n",
            " iteration=358 bias=0 loss=323.07726666667327\n",
            "\n",
            " iteration=359 bias=0 loss=321.03865000000667\n",
            "\n",
            " iteration=360 bias=0 loss=319.0066666666733\n",
            "\n",
            " iteration=361 bias=0 loss=316.9813166666734\n",
            "\n",
            " iteration=362 bias=0 loss=314.9626000000067\n",
            "\n",
            " iteration=363 bias=0 loss=312.9505166666734\n",
            "\n",
            " iteration=364 bias=0 loss=310.9450666666734\n",
            "\n",
            " iteration=365 bias=0 loss=308.9462500000068\n",
            "\n",
            " iteration=366 bias=0 loss=306.95406666667344\n",
            "\n",
            " iteration=367 bias=0 loss=304.96851666667345\n",
            "\n",
            " iteration=368 bias=0 loss=302.9896000000068\n",
            "\n",
            " iteration=369 bias=0 loss=301.0173166666735\n",
            "\n",
            " iteration=370 bias=0 loss=299.0516666666735\n",
            "\n",
            " iteration=371 bias=0 loss=297.09265000000687\n",
            "\n",
            " iteration=372 bias=0 loss=295.14026666667354\n",
            "\n",
            " iteration=373 bias=0 loss=293.19451666667356\n",
            "\n",
            " iteration=374 bias=0 loss=291.255400000007\n",
            "\n",
            " iteration=375 bias=0 loss=289.3229166666736\n",
            "\n",
            " iteration=376 bias=0 loss=287.39706666667365\n",
            "\n",
            " iteration=377 bias=0 loss=285.477850000007\n",
            "\n",
            " iteration=378 bias=0 loss=283.5652666666736\n",
            "\n",
            " iteration=379 bias=0 loss=281.65931666667365\n",
            "\n",
            " iteration=380 bias=0 loss=279.76000000000704\n",
            "\n",
            " iteration=381 bias=0 loss=277.86731666667373\n",
            "\n",
            " iteration=382 bias=0 loss=275.9812666666737\n",
            "\n",
            " iteration=383 bias=0 loss=274.1018500000071\n",
            "\n",
            " iteration=384 bias=0 loss=272.22906666667376\n",
            "\n",
            " iteration=385 bias=0 loss=270.36291666667375\n",
            "\n",
            " iteration=386 bias=0 loss=268.50340000000716\n",
            "\n",
            " iteration=387 bias=0 loss=266.6505166666738\n",
            "\n",
            " iteration=388 bias=0 loss=264.8042666666738\n",
            "\n",
            " iteration=389 bias=0 loss=262.96465000000717\n",
            "\n",
            " iteration=390 bias=0 loss=261.1316666666739\n",
            "\n",
            " iteration=391 bias=0 loss=259.30531666667383\n",
            "\n",
            " iteration=392 bias=0 loss=257.4856000000072\n",
            "\n",
            " iteration=393 bias=0 loss=255.6725166666739\n",
            "\n",
            " iteration=394 bias=0 loss=253.8660666666739\n",
            "\n",
            " iteration=395 bias=0 loss=252.06625000000722\n",
            "\n",
            " iteration=396 bias=0 loss=250.27306666667388\n",
            "\n",
            " iteration=397 bias=0 loss=248.48651666667396\n",
            "\n",
            " iteration=398 bias=0 loss=246.70660000000726\n",
            "\n",
            " iteration=399 bias=0 loss=244.93331666667396\n",
            "\n",
            " iteration=400 bias=0 loss=243.16666666667393\n",
            "\n",
            " iteration=401 bias=0 loss=241.4066500000072\n",
            "\n",
            " iteration=402 bias=0 loss=239.6532666666739\n",
            "\n",
            " iteration=403 bias=0 loss=237.90651666667392\n",
            "\n",
            " iteration=404 bias=0 loss=236.16640000000726\n",
            "\n",
            " iteration=405 bias=0 loss=234.43291666667395\n",
            "\n",
            " iteration=406 bias=0 loss=232.70606666667393\n",
            "\n",
            " iteration=407 bias=0 loss=230.9858500000073\n",
            "\n",
            " iteration=408 bias=0 loss=229.27226666667397\n",
            "\n",
            " iteration=409 bias=0 loss=227.565316666674\n",
            "\n",
            " iteration=410 bias=0 loss=225.86500000000729\n",
            "\n",
            " iteration=411 bias=0 loss=224.17131666667396\n",
            "\n",
            " iteration=412 bias=0 loss=222.48426666667396\n",
            "\n",
            " iteration=413 bias=0 loss=220.80385000000737\n",
            "\n",
            " iteration=414 bias=0 loss=219.130066666674\n",
            "\n",
            " iteration=415 bias=0 loss=217.46291666667398\n",
            "\n",
            " iteration=416 bias=0 loss=215.80240000000734\n",
            "\n",
            " iteration=417 bias=0 loss=214.14851666667403\n",
            "\n",
            " iteration=418 bias=0 loss=212.501266666674\n",
            "\n",
            " iteration=419 bias=0 loss=210.86065000000735\n",
            "\n",
            " iteration=420 bias=0 loss=209.22666666667405\n",
            "\n",
            " iteration=421 bias=0 loss=207.599316666674\n",
            "\n",
            " iteration=422 bias=0 loss=205.9786000000074\n",
            "\n",
            " iteration=423 bias=0 loss=204.36451666667404\n",
            "\n",
            " iteration=424 bias=0 loss=202.75706666667406\n",
            "\n",
            " iteration=425 bias=0 loss=201.1562500000074\n",
            "\n",
            " iteration=426 bias=0 loss=199.56206666667404\n",
            "\n",
            " iteration=427 bias=0 loss=197.97451666667405\n",
            "\n",
            " iteration=428 bias=0 loss=196.39360000000738\n",
            "\n",
            " iteration=429 bias=0 loss=194.81931666667404\n",
            "\n",
            " iteration=430 bias=0 loss=193.25166666667405\n",
            "\n",
            " iteration=431 bias=0 loss=191.6906500000074\n",
            "\n",
            " iteration=432 bias=0 loss=190.13626666667406\n",
            "\n",
            " iteration=433 bias=0 loss=188.58851666667408\n",
            "\n",
            " iteration=434 bias=0 loss=187.0474000000074\n",
            "\n",
            " iteration=435 bias=0 loss=185.51291666667407\n",
            "\n",
            " iteration=436 bias=0 loss=183.98506666667404\n",
            "\n",
            " iteration=437 bias=0 loss=182.4638500000074\n",
            "\n",
            " iteration=438 bias=0 loss=180.94926666667405\n",
            "\n",
            " iteration=439 bias=0 loss=179.44131666667408\n",
            "\n",
            " iteration=440 bias=0 loss=177.94000000000742\n",
            "\n",
            " iteration=441 bias=0 loss=176.44531666667407\n",
            "\n",
            " iteration=442 bias=0 loss=174.95726666667406\n",
            "\n",
            " iteration=443 bias=0 loss=173.4758500000074\n",
            "\n",
            " iteration=444 bias=0 loss=172.00106666667406\n",
            "\n",
            " iteration=445 bias=0 loss=170.53291666667403\n",
            "\n",
            " iteration=446 bias=0 loss=169.0714000000074\n",
            "\n",
            " iteration=447 bias=0 loss=167.61651666667407\n",
            "\n",
            " iteration=448 bias=0 loss=166.16826666667404\n",
            "\n",
            " iteration=449 bias=0 loss=164.7266500000074\n",
            "\n",
            " iteration=450 bias=0 loss=163.29166666667405\n",
            "\n",
            " iteration=451 bias=0 loss=161.86331666667402\n",
            "\n",
            " iteration=452 bias=0 loss=160.44160000000736\n",
            "\n",
            " iteration=453 bias=0 loss=159.02651666667404\n",
            "\n",
            " iteration=454 bias=0 loss=157.61806666667403\n",
            "\n",
            " iteration=455 bias=0 loss=156.21625000000736\n",
            "\n",
            " iteration=456 bias=0 loss=154.82106666667403\n",
            "\n",
            " iteration=457 bias=0 loss=153.43251666667402\n",
            "\n",
            " iteration=458 bias=0 loss=152.05060000000734\n",
            "\n",
            " iteration=459 bias=0 loss=150.675316666674\n",
            "\n",
            " iteration=460 bias=0 loss=149.306666666674\n",
            "\n",
            " iteration=461 bias=0 loss=147.9446500000073\n",
            "\n",
            " iteration=462 bias=0 loss=146.58926666667398\n",
            "\n",
            " iteration=463 bias=0 loss=145.24051666667398\n",
            "\n",
            " iteration=464 bias=0 loss=143.8984000000073\n",
            "\n",
            " iteration=465 bias=0 loss=142.56291666667394\n",
            "\n",
            " iteration=466 bias=0 loss=141.23406666667395\n",
            "\n",
            " iteration=467 bias=0 loss=139.91185000000726\n",
            "\n",
            " iteration=468 bias=0 loss=138.59626666667393\n",
            "\n",
            " iteration=469 bias=0 loss=137.2873166666739\n",
            "\n",
            " iteration=470 bias=0 loss=135.98500000000726\n",
            "\n",
            " iteration=471 bias=0 loss=134.6893166666739\n",
            "\n",
            " iteration=472 bias=0 loss=133.4002666666739\n",
            "\n",
            " iteration=473 bias=0 loss=132.11785000000722\n",
            "\n",
            " iteration=474 bias=0 loss=130.84206666667387\n",
            "\n",
            " iteration=475 bias=0 loss=129.57291666667388\n",
            "\n",
            " iteration=476 bias=0 loss=128.31040000000715\n",
            "\n",
            " iteration=477 bias=0 loss=127.0545166666738\n",
            "\n",
            " iteration=478 bias=0 loss=125.80526666667383\n",
            "\n",
            " iteration=479 bias=0 loss=124.56265000000717\n",
            "\n",
            " iteration=480 bias=0 loss=123.32666666667382\n",
            "\n",
            " iteration=481 bias=0 loss=122.0973166666738\n",
            "\n",
            " iteration=482 bias=0 loss=120.8746000000071\n",
            "\n",
            " iteration=483 bias=0 loss=119.65851666667375\n",
            "\n",
            " iteration=484 bias=0 loss=118.44906666667377\n",
            "\n",
            " iteration=485 bias=0 loss=117.24625000000704\n",
            "\n",
            " iteration=486 bias=0 loss=116.05006666667373\n",
            "\n",
            " iteration=487 bias=0 loss=114.86051666667372\n",
            "\n",
            " iteration=488 bias=0 loss=113.67760000000703\n",
            "\n",
            " iteration=489 bias=0 loss=112.50131666667369\n",
            "\n",
            " iteration=490 bias=0 loss=111.33166666667366\n",
            "\n",
            " iteration=491 bias=0 loss=110.16865000000699\n",
            "\n",
            " iteration=492 bias=0 loss=109.01226666667363\n",
            "\n",
            " iteration=493 bias=0 loss=107.86251666667361\n",
            "\n",
            " iteration=494 bias=0 loss=106.71940000000694\n",
            "\n",
            " iteration=495 bias=0 loss=105.5829166666736\n",
            "\n",
            " iteration=496 bias=0 loss=104.45306666667358\n",
            "\n",
            " iteration=497 bias=0 loss=103.3298500000069\n",
            "\n",
            " iteration=498 bias=0 loss=102.21326666667353\n",
            "\n",
            " iteration=499 bias=0 loss=101.10331666667354\n",
            "\n",
            " iteration=500 bias=0 loss=100.00000000000682\n",
            "\n",
            " iteration=501 bias=0 loss=98.90331666667346\n",
            "\n",
            " iteration=502 bias=0 loss=97.81326666667348\n",
            "\n",
            " iteration=503 bias=0 loss=96.72985000000678\n",
            "\n",
            " iteration=504 bias=0 loss=95.65306666667344\n",
            "\n",
            " iteration=505 bias=0 loss=94.58291666667344\n",
            "\n",
            " iteration=506 bias=0 loss=93.51940000000673\n",
            "\n",
            " iteration=507 bias=0 loss=92.46251666667337\n",
            "\n",
            " iteration=508 bias=0 loss=91.41226666667335\n",
            "\n",
            " iteration=509 bias=0 loss=90.36865000000665\n",
            "\n",
            " iteration=510 bias=0 loss=89.33166666667331\n",
            "\n",
            " iteration=511 bias=0 loss=88.30131666667329\n",
            "\n",
            " iteration=512 bias=0 loss=87.27760000000661\n",
            "\n",
            " iteration=513 bias=0 loss=86.26051666667327\n",
            "\n",
            " iteration=514 bias=0 loss=85.25006666667323\n",
            "\n",
            " iteration=515 bias=0 loss=84.24625000000655\n",
            "\n",
            " iteration=516 bias=0 loss=83.24906666667319\n",
            "\n",
            " iteration=517 bias=0 loss=82.25851666667315\n",
            "\n",
            " iteration=518 bias=0 loss=81.27460000000647\n",
            "\n",
            " iteration=519 bias=0 loss=80.29731666667311\n",
            "\n",
            " iteration=520 bias=0 loss=79.32666666667309\n",
            "\n",
            " iteration=521 bias=0 loss=78.36265000000641\n",
            "\n",
            " iteration=522 bias=0 loss=77.40526666667304\n",
            "\n",
            " iteration=523 bias=0 loss=76.45451666667302\n",
            "\n",
            " iteration=524 bias=0 loss=75.51040000000633\n",
            "\n",
            " iteration=525 bias=0 loss=74.57291666667297\n",
            "\n",
            " iteration=526 bias=0 loss=73.64206666667295\n",
            "\n",
            " iteration=527 bias=0 loss=72.71785000000625\n",
            "\n",
            " iteration=528 bias=0 loss=71.8002666666729\n",
            "\n",
            " iteration=529 bias=0 loss=70.88931666667288\n",
            "\n",
            " iteration=530 bias=0 loss=69.98500000000618\n",
            "\n",
            " iteration=531 bias=0 loss=69.08731666667282\n",
            "\n",
            " iteration=532 bias=0 loss=68.1962666666728\n",
            "\n",
            " iteration=533 bias=0 loss=67.31185000000609\n",
            "\n",
            " iteration=534 bias=0 loss=66.43406666667273\n",
            "\n",
            " iteration=535 bias=0 loss=65.5629166666727\n",
            "\n",
            " iteration=536 bias=0 loss=64.698400000006\n",
            "\n",
            " iteration=537 bias=0 loss=63.84051666667267\n",
            "\n",
            " iteration=538 bias=0 loss=62.98926666667264\n",
            "\n",
            " iteration=539 bias=0 loss=62.144650000005925\n",
            "\n",
            " iteration=540 bias=0 loss=61.306666666672555\n",
            "\n",
            " iteration=541 bias=0 loss=60.47531666667253\n",
            "\n",
            " iteration=542 bias=0 loss=59.65060000000586\n",
            "\n",
            " iteration=543 bias=0 loss=58.83251666667248\n",
            "\n",
            " iteration=544 bias=0 loss=58.02106666667243\n",
            "\n",
            " iteration=545 bias=0 loss=57.21625000000577\n",
            "\n",
            " iteration=546 bias=0 loss=56.41806666667241\n",
            "\n",
            " iteration=547 bias=0 loss=55.62651666667236\n",
            "\n",
            " iteration=548 bias=0 loss=54.84160000000565\n",
            "\n",
            " iteration=549 bias=0 loss=54.063316666672286\n",
            "\n",
            " iteration=550 bias=0 loss=53.29166666667228\n",
            "\n",
            " iteration=551 bias=0 loss=52.52665000000556\n",
            "\n",
            " iteration=552 bias=0 loss=51.76826666667219\n",
            "\n",
            " iteration=553 bias=0 loss=51.016516666672175\n",
            "\n",
            " iteration=554 bias=0 loss=50.271400000005485\n",
            "\n",
            " iteration=555 bias=0 loss=49.532916666672094\n",
            "\n",
            " iteration=556 bias=0 loss=48.80106666667205\n",
            "\n",
            " iteration=557 bias=0 loss=48.07585000000535\n",
            "\n",
            " iteration=558 bias=0 loss=47.35726666667201\n",
            "\n",
            " iteration=559 bias=0 loss=46.64531666667196\n",
            "\n",
            " iteration=560 bias=0 loss=45.94000000000525\n",
            "\n",
            " iteration=561 bias=0 loss=45.241316666671906\n",
            "\n",
            " iteration=562 bias=0 loss=44.549266666671876\n",
            "\n",
            " iteration=563 bias=0 loss=43.86385000000516\n",
            "\n",
            " iteration=564 bias=0 loss=43.18506666667178\n",
            "\n",
            " iteration=565 bias=0 loss=42.51291666667174\n",
            "\n",
            " iteration=566 bias=0 loss=41.84740000000506\n",
            "\n",
            " iteration=567 bias=0 loss=41.18851666667168\n",
            "\n",
            " iteration=568 bias=0 loss=40.53626666667163\n",
            "\n",
            " iteration=569 bias=0 loss=39.890650000004946\n",
            "\n",
            " iteration=570 bias=0 loss=39.25166666667158\n",
            "\n",
            " iteration=571 bias=0 loss=38.61931666667153\n",
            "\n",
            " iteration=572 bias=0 loss=37.99360000000481\n",
            "\n",
            " iteration=573 bias=0 loss=37.37451666667145\n",
            "\n",
            " iteration=574 bias=0 loss=36.76206666667142\n",
            "\n",
            " iteration=575 bias=0 loss=36.15625000000471\n",
            "\n",
            " iteration=576 bias=0 loss=35.557066666671325\n",
            "\n",
            " iteration=577 bias=0 loss=34.964516666671294\n",
            "\n",
            " iteration=578 bias=0 loss=34.3786000000046\n",
            "\n",
            " iteration=579 bias=0 loss=33.79931666667122\n",
            "\n",
            " iteration=580 bias=0 loss=33.22666666667117\n",
            "\n",
            " iteration=581 bias=0 loss=32.66065000000446\n",
            "\n",
            " iteration=582 bias=0 loss=32.10126666667111\n",
            "\n",
            " iteration=583 bias=0 loss=31.548516666671052\n",
            "\n",
            " iteration=584 bias=0 loss=31.00240000000434\n",
            "\n",
            " iteration=585 bias=0 loss=30.462916666670974\n",
            "\n",
            " iteration=586 bias=0 loss=29.93006666667095\n",
            "\n",
            " iteration=587 bias=0 loss=29.403850000004223\n",
            "\n",
            " iteration=588 bias=0 loss=28.884266666670836\n",
            "\n",
            " iteration=589 bias=0 loss=28.371316666670797\n",
            "\n",
            " iteration=590 bias=0 loss=27.865000000004102\n",
            "\n",
            " iteration=591 bias=0 loss=27.36531666667072\n",
            "\n",
            " iteration=592 bias=0 loss=26.872266666670658\n",
            "\n",
            " iteration=593 bias=0 loss=26.385850000003966\n",
            "\n",
            " iteration=594 bias=0 loss=25.906066666670593\n",
            "\n",
            " iteration=595 bias=0 loss=25.432916666670536\n",
            "\n",
            " iteration=596 bias=0 loss=24.966400000003816\n",
            "\n",
            " iteration=597 bias=0 loss=24.50651666667045\n",
            "\n",
            " iteration=598 bias=0 loss=24.053266666670414\n",
            "\n",
            " iteration=599 bias=0 loss=23.606650000003693\n",
            "\n",
            " iteration=600 bias=0 loss=23.166666666670306\n",
            "\n",
            " iteration=601 bias=0 loss=22.733316666670273\n",
            "\n",
            " iteration=602 bias=0 loss=22.306600000003566\n",
            "\n",
            " iteration=603 bias=0 loss=21.886516666670172\n",
            "\n",
            " iteration=604 bias=0 loss=21.47306666667012\n",
            "\n",
            " iteration=605 bias=0 loss=21.066250000003407\n",
            "\n",
            " iteration=606 bias=0 loss=20.666066666670037\n",
            "\n",
            " iteration=607 bias=0 loss=20.272516666669983\n",
            "\n",
            " iteration=608 bias=0 loss=19.88560000000326\n",
            "\n",
            " iteration=609 bias=0 loss=19.505316666669888\n",
            "\n",
            " iteration=610 bias=0 loss=19.131666666669844\n",
            "\n",
            " iteration=611 bias=0 loss=18.764650000003122\n",
            "\n",
            " iteration=612 bias=0 loss=18.404266666669734\n",
            "\n",
            " iteration=613 bias=0 loss=18.050516666669687\n",
            "\n",
            " iteration=614 bias=0 loss=17.703400000002983\n",
            "\n",
            " iteration=615 bias=0 loss=17.362916666669587\n",
            "\n",
            " iteration=616 bias=0 loss=17.029066666669532\n",
            "\n",
            " iteration=617 bias=0 loss=16.701850000002825\n",
            "\n",
            " iteration=618 bias=0 loss=16.381266666669443\n",
            "\n",
            " iteration=619 bias=0 loss=16.067316666669385\n",
            "\n",
            " iteration=620 bias=0 loss=15.760000000002663\n",
            "\n",
            " iteration=621 bias=0 loss=15.45931666666928\n",
            "\n",
            " iteration=622 bias=0 loss=15.165266666669234\n",
            "\n",
            " iteration=623 bias=0 loss=14.87785000000251\n",
            "\n",
            " iteration=624 bias=0 loss=14.597066666669122\n",
            "\n",
            " iteration=625 bias=0 loss=14.322916666669073\n",
            "\n",
            " iteration=626 bias=0 loss=14.055400000002358\n",
            "\n",
            " iteration=627 bias=0 loss=13.794516666668963\n",
            "\n",
            " iteration=628 bias=0 loss=13.540266666668906\n",
            "\n",
            " iteration=629 bias=0 loss=13.292650000002189\n",
            "\n",
            " iteration=630 bias=0 loss=13.051666666668808\n",
            "\n",
            " iteration=631 bias=0 loss=12.817316666668745\n",
            "\n",
            " iteration=632 bias=0 loss=12.58960000000202\n",
            "\n",
            " iteration=633 bias=0 loss=12.36851666666864\n",
            "\n",
            " iteration=634 bias=0 loss=12.154066666668585\n",
            "\n",
            " iteration=635 bias=0 loss=11.94625000000186\n",
            "\n",
            " iteration=636 bias=0 loss=11.74506666666847\n",
            "\n",
            " iteration=637 bias=0 loss=11.550516666668413\n",
            "\n",
            " iteration=638 bias=0 loss=11.362600000001697\n",
            "\n",
            " iteration=639 bias=0 loss=11.181316666668302\n",
            "\n",
            " iteration=640 bias=0 loss=11.00666666666824\n",
            "\n",
            " iteration=641 bias=0 loss=10.838650000001516\n",
            "\n",
            " iteration=642 bias=0 loss=10.677266666668132\n",
            "\n",
            " iteration=643 bias=0 loss=10.52251666666807\n",
            "\n",
            " iteration=644 bias=0 loss=10.374400000001343\n",
            "\n",
            " iteration=645 bias=0 loss=10.232916666667956\n",
            "\n",
            " iteration=646 bias=0 loss=10.098066666667895\n",
            "\n",
            " iteration=647 bias=0 loss=9.969850000001168\n",
            "\n",
            " iteration=648 bias=0 loss=9.848266666667774\n",
            "\n",
            " iteration=649 bias=0 loss=9.733316666667713\n",
            "\n",
            " iteration=650 bias=0 loss=9.625000000000991\n",
            "\n",
            " iteration=651 bias=0 loss=9.523316666667595\n",
            "\n",
            " iteration=652 bias=0 loss=9.42826666666753\n",
            "\n",
            " iteration=653 bias=0 loss=9.33985000000081\n",
            "\n",
            " iteration=654 bias=0 loss=9.258066666667412\n",
            "\n",
            " iteration=655 bias=0 loss=9.182916666667351\n",
            "\n",
            " iteration=656 bias=0 loss=9.11440000000062\n",
            "\n",
            " iteration=657 bias=0 loss=9.052516666667222\n",
            "\n",
            " iteration=658 bias=0 loss=8.997266666667166\n",
            "\n",
            " iteration=659 bias=0 loss=8.948650000000434\n",
            "\n",
            " iteration=660 bias=0 loss=8.906666666667036\n",
            "\n",
            " iteration=661 bias=0 loss=8.87131666666698\n",
            "\n",
            " iteration=662 bias=0 loss=8.842600000000248\n",
            "\n",
            " iteration=663 bias=0 loss=8.820516666666848\n",
            "\n",
            " iteration=664 bias=0 loss=8.805066666666782\n",
            "\n",
            " iteration=665 bias=0 loss=8.79625000000005\n",
            "\n",
            " iteration=666 bias=0 loss=8.794066666666657\n",
            "\n",
            " iteration=667 bias=0.01 loss=8.776766666666639\n",
            "\n",
            " iteration=668 bias=0.02 loss=8.75966666666664\n",
            "\n",
            " iteration=669 bias=0.02 loss=8.759650000000033\n",
            "\n",
            " iteration=670 bias=0.03 loss=8.741650000000016\n",
            "\n",
            " iteration=671 bias=0.04 loss=8.72385000000001\n",
            "\n",
            " iteration=672 bias=0.05 loss=8.706249999999992\n",
            "\n",
            " iteration=673 bias=0.060000000000000005 loss=8.68884999999999\n",
            "\n",
            " iteration=674 bias=0.07 loss=8.671649999999975\n",
            "\n",
            " iteration=675 bias=0.08 loss=8.654649999999966\n",
            "\n",
            " iteration=676 bias=0.09 loss=8.637849999999958\n",
            "\n",
            " iteration=677 bias=0.09 loss=8.63676666666669\n",
            "\n",
            " iteration=678 bias=0.09999999999999999 loss=8.619066666666674\n",
            "\n",
            " iteration=679 bias=0.10999999999999999 loss=8.601566666666665\n",
            "\n",
            " iteration=680 bias=0.11999999999999998 loss=8.584266666666649\n",
            "\n",
            " iteration=681 bias=0.12999999999999998 loss=8.567166666666651\n",
            "\n",
            " iteration=682 bias=0.13999999999999999 loss=8.550266666666632\n",
            "\n",
            " iteration=683 bias=0.15 loss=8.533566666666625\n",
            "\n",
            " iteration=684 bias=0.15 loss=8.532516666666691\n",
            "\n",
            " iteration=685 bias=0.16 loss=8.514916666666673\n",
            "\n",
            " iteration=686 bias=0.17 loss=8.497516666666664\n",
            "\n",
            " iteration=687 bias=0.18000000000000002 loss=8.480316666666658\n",
            "\n",
            " iteration=688 bias=0.19000000000000003 loss=8.46331666666664\n",
            "\n",
            " iteration=689 bias=0.20000000000000004 loss=8.446516666666641\n",
            "\n",
            " iteration=690 bias=0.21000000000000005 loss=8.429916666666623\n",
            "\n",
            " iteration=691 bias=0.21000000000000005 loss=8.428900000000022\n",
            "\n",
            " iteration=692 bias=0.22000000000000006 loss=8.411400000000015\n",
            "\n",
            " iteration=693 bias=0.23000000000000007 loss=8.3941\n",
            "\n",
            " iteration=694 bias=0.24000000000000007 loss=8.376999999999997\n",
            "\n",
            " iteration=695 bias=0.25000000000000006 loss=8.36009999999998\n",
            "\n",
            " iteration=696 bias=0.26000000000000006 loss=8.343399999999962\n",
            "\n",
            " iteration=697 bias=0.2700000000000001 loss=8.326899999999965\n",
            "\n",
            " iteration=698 bias=0.2700000000000001 loss=8.325916666666698\n",
            "\n",
            " iteration=699 bias=0.2800000000000001 loss=8.308516666666682\n",
            "\n",
            " iteration=700 bias=0.2900000000000001 loss=8.291316666666672\n",
            "\n",
            " iteration=701 bias=0.3000000000000001 loss=8.274316666666657\n",
            "\n",
            " iteration=702 bias=0.3100000000000001 loss=8.257516666666655\n",
            "\n",
            " iteration=703 bias=0.3200000000000001 loss=8.240916666666639\n",
            "\n",
            " iteration=704 bias=0.3300000000000001 loss=8.224516666666622\n",
            "\n",
            " iteration=705 bias=0.3300000000000001 loss=8.223566666666681\n",
            "\n",
            " iteration=706 bias=0.34000000000000014 loss=8.20626666666668\n",
            "\n",
            " iteration=707 bias=0.35000000000000014 loss=8.189166666666665\n",
            "\n",
            " iteration=708 bias=0.36000000000000015 loss=8.172266666666658\n",
            "\n",
            " iteration=709 bias=0.37000000000000016 loss=8.155566666666642\n",
            "\n",
            " iteration=710 bias=0.38000000000000017 loss=8.139066666666642\n",
            "\n",
            " iteration=711 bias=0.3900000000000002 loss=8.122766666666623\n",
            "\n",
            " iteration=712 bias=0.3900000000000002 loss=8.121850000000022\n",
            "\n",
            " iteration=713 bias=0.4000000000000002 loss=8.104650000000015\n",
            "\n",
            " iteration=714 bias=0.4100000000000002 loss=8.087650000000005\n",
            "\n",
            " iteration=715 bias=0.4200000000000002 loss=8.07084999999999\n",
            "\n",
            " iteration=716 bias=0.4300000000000002 loss=8.054249999999982\n",
            "\n",
            " iteration=717 bias=0.4400000000000002 loss=8.037849999999965\n",
            "\n",
            " iteration=718 bias=0.45000000000000023 loss=8.021649999999964\n",
            "\n",
            " iteration=719 bias=0.45000000000000023 loss=8.020766666666697\n",
            "\n",
            " iteration=720 bias=0.46000000000000024 loss=8.003666666666678\n",
            "\n",
            " iteration=721 bias=0.47000000000000025 loss=7.986766666666672\n",
            "\n",
            " iteration=722 bias=0.48000000000000026 loss=7.970066666666656\n",
            "\n",
            " iteration=723 bias=0.49000000000000027 loss=7.953566666666646\n",
            "\n",
            " iteration=724 bias=0.5000000000000002 loss=7.9372666666666385\n",
            "\n",
            " iteration=725 bias=0.5100000000000002 loss=7.921166666666622\n",
            "\n",
            " iteration=726 bias=0.5100000000000002 loss=7.92031666666668\n",
            "\n",
            " iteration=727 bias=0.5200000000000002 loss=7.90331666666668\n",
            "\n",
            " iteration=728 bias=0.5300000000000002 loss=7.886516666666663\n",
            "\n",
            " iteration=729 bias=0.5400000000000003 loss=7.869916666666657\n",
            "\n",
            " iteration=730 bias=0.5500000000000003 loss=7.85351666666664\n",
            "\n",
            " iteration=731 bias=0.5600000000000003 loss=7.837316666666639\n",
            "\n",
            " iteration=732 bias=0.5700000000000003 loss=7.821316666666623\n",
            "\n",
            " iteration=733 bias=0.5700000000000003 loss=7.820500000000021\n",
            "\n",
            " iteration=734 bias=0.5800000000000003 loss=7.803600000000006\n",
            "\n",
            " iteration=735 bias=0.5900000000000003 loss=7.7869000000000055\n",
            "\n",
            " iteration=736 bias=0.6000000000000003 loss=7.770399999999989\n",
            "\n",
            " iteration=737 bias=0.6100000000000003 loss=7.7540999999999825\n",
            "\n",
            " iteration=738 bias=0.6200000000000003 loss=7.737999999999965\n",
            "\n",
            " iteration=739 bias=0.6300000000000003 loss=7.7220999999999655\n",
            "\n",
            " iteration=740 bias=0.6300000000000003 loss=7.721316666666695\n",
            "\n",
            " iteration=741 bias=0.6400000000000003 loss=7.70451666666668\n",
            "\n",
            " iteration=742 bias=0.6500000000000004 loss=7.687916666666671\n",
            "\n",
            " iteration=743 bias=0.6600000000000004 loss=7.6715166666666645\n",
            "\n",
            " iteration=744 bias=0.6700000000000004 loss=7.655316666666646\n",
            "\n",
            " iteration=745 bias=0.6800000000000004 loss=7.6393166666666374\n",
            "\n",
            " iteration=746 bias=0.6900000000000004 loss=7.623516666666622\n",
            "\n",
            " iteration=747 bias=0.6900000000000004 loss=7.622766666666685\n",
            "\n",
            " iteration=748 bias=0.7000000000000004 loss=7.606066666666686\n",
            "\n",
            " iteration=749 bias=0.7100000000000004 loss=7.58956666666667\n",
            "\n",
            " iteration=750 bias=0.7200000000000004 loss=7.573266666666663\n",
            "\n",
            " iteration=751 bias=0.7300000000000004 loss=7.557166666666659\n",
            "\n",
            " iteration=752 bias=0.7400000000000004 loss=7.541266666666637\n",
            "\n",
            " iteration=753 bias=0.7500000000000004 loss=7.525566666666628\n",
            "\n",
            " iteration=754 bias=0.7500000000000004 loss=7.524850000000028\n",
            "\n",
            " iteration=755 bias=0.7600000000000005 loss=7.508250000000012\n",
            "\n",
            " iteration=756 bias=0.7700000000000005 loss=7.491850000000011\n",
            "\n",
            " iteration=757 bias=0.7800000000000005 loss=7.475649999999995\n",
            "\n",
            " iteration=758 bias=0.7900000000000005 loss=7.459649999999989\n",
            "\n",
            " iteration=759 bias=0.8000000000000005 loss=7.443849999999972\n",
            "\n",
            " iteration=760 bias=0.8100000000000005 loss=7.428249999999967\n",
            "\n",
            " iteration=761 bias=0.8100000000000005 loss=7.427566666666688\n",
            "\n",
            " iteration=762 bias=0.8200000000000005 loss=7.41106666666668\n",
            "\n",
            " iteration=763 bias=0.8300000000000005 loss=7.394766666666662\n",
            "\n",
            " iteration=764 bias=0.8400000000000005 loss=7.378666666666663\n",
            "\n",
            " iteration=765 bias=0.8500000000000005 loss=7.362766666666647\n",
            "\n",
            " iteration=766 bias=0.8600000000000005 loss=7.347066666666639\n",
            "\n",
            " iteration=767 bias=0.8700000000000006 loss=7.331566666666625\n",
            "\n",
            " iteration=768 bias=0.8700000000000006 loss=7.330916666666686\n",
            "\n",
            " iteration=769 bias=0.8800000000000006 loss=7.314516666666687\n",
            "\n",
            " iteration=770 bias=0.8900000000000006 loss=7.298316666666669\n",
            "\n",
            " iteration=771 bias=0.9000000000000006 loss=7.282316666666655\n",
            "\n",
            " iteration=772 bias=0.9100000000000006 loss=7.2665166666666545\n",
            "\n",
            " iteration=773 bias=0.9200000000000006 loss=7.250916666666637\n",
            "\n",
            " iteration=774 bias=0.9300000000000006 loss=7.235516666666629\n",
            "\n",
            " iteration=775 bias=0.9300000000000006 loss=7.234900000000025\n",
            "\n",
            " iteration=776 bias=0.9400000000000006 loss=7.21860000000001\n",
            "\n",
            " iteration=777 bias=0.9500000000000006 loss=7.202500000000011\n",
            "\n",
            " iteration=778 bias=0.9600000000000006 loss=7.186599999999994\n",
            "\n",
            " iteration=779 bias=0.9700000000000006 loss=7.170899999999986\n",
            "\n",
            " iteration=780 bias=0.9800000000000006 loss=7.155399999999976\n",
            "\n",
            " iteration=781 bias=0.9900000000000007 loss=7.140099999999961\n",
            "\n",
            " iteration=782 bias=0.9900000000000007 loss=7.139516666666686\n",
            "\n",
            " iteration=783 bias=1.0000000000000007 loss=7.123316666666678\n",
            "\n",
            " iteration=784 bias=1.0100000000000007 loss=7.107316666666662\n",
            "\n",
            " iteration=785 bias=1.0200000000000007 loss=7.091516666666663\n",
            "\n",
            " iteration=786 bias=1.0300000000000007 loss=7.075916666666646\n",
            "\n",
            " iteration=787 bias=1.0400000000000007 loss=7.060516666666639\n",
            "\n",
            " iteration=788 bias=1.0500000000000007 loss=7.045316666666622\n",
            "\n",
            " iteration=789 bias=1.0500000000000007 loss=7.044766666666685\n",
            "\n",
            " iteration=790 bias=1.0600000000000007 loss=7.028666666666677\n",
            "\n",
            " iteration=791 bias=1.0700000000000007 loss=7.01276666666667\n",
            "\n",
            " iteration=792 bias=1.0800000000000007 loss=6.9970666666666546\n",
            "\n",
            " iteration=793 bias=1.0900000000000007 loss=6.981566666666654\n",
            "\n",
            " iteration=794 bias=1.1000000000000008 loss=6.966266666666638\n",
            "\n",
            " iteration=795 bias=1.1100000000000008 loss=6.95116666666663\n",
            "\n",
            " iteration=796 bias=1.1100000000000008 loss=6.950650000000027\n",
            "\n",
            " iteration=797 bias=1.1200000000000008 loss=6.934650000000011\n",
            "\n",
            " iteration=798 bias=1.1300000000000008 loss=6.918850000000002\n",
            "\n",
            " iteration=799 bias=1.1400000000000008 loss=6.9032499999999954\n",
            "\n",
            " iteration=800 bias=1.1500000000000008 loss=6.887849999999979\n",
            "\n",
            " iteration=801 bias=1.1600000000000008 loss=6.872649999999978\n",
            "\n",
            " iteration=802 bias=1.1700000000000008 loss=6.857649999999961\n",
            "\n",
            " iteration=803 bias=1.1700000000000008 loss=6.85716666666669\n",
            "\n",
            " iteration=804 bias=1.1800000000000008 loss=6.841266666666684\n",
            "\n",
            " iteration=805 bias=1.1900000000000008 loss=6.82556666666667\n",
            "\n",
            " iteration=806 bias=1.2000000000000008 loss=6.810066666666668\n",
            "\n",
            " iteration=807 bias=1.2100000000000009 loss=6.794766666666651\n",
            "\n",
            " iteration=808 bias=1.2200000000000009 loss=6.779666666666635\n",
            "\n",
            " iteration=809 bias=1.2300000000000009 loss=6.764766666666635\n",
            "\n",
            " iteration=810 bias=1.2300000000000009 loss=6.764316666666699\n",
            "\n",
            " iteration=811 bias=1.2400000000000009 loss=6.748516666666684\n",
            "\n",
            " iteration=812 bias=1.2500000000000009 loss=6.732916666666676\n",
            "\n",
            " iteration=813 bias=1.260000000000001 loss=6.717516666666661\n",
            "\n",
            " iteration=814 bias=1.270000000000001 loss=6.70231666666666\n",
            "\n",
            " iteration=815 bias=1.280000000000001 loss=6.687316666666644\n",
            "\n",
            " iteration=816 bias=1.290000000000001 loss=6.672516666666631\n",
            "\n",
            " iteration=817 bias=1.290000000000001 loss=6.672100000000028\n",
            "\n",
            " iteration=818 bias=1.300000000000001 loss=6.656400000000018\n",
            "\n",
            " iteration=819 bias=1.310000000000001 loss=6.640900000000002\n",
            "\n",
            " iteration=820 bias=1.320000000000001 loss=6.625599999999995\n",
            "\n",
            " iteration=821 bias=1.330000000000001 loss=6.61049999999998\n",
            "\n",
            " iteration=822 bias=1.340000000000001 loss=6.595599999999979\n",
            "\n",
            " iteration=823 bias=1.350000000000001 loss=6.580899999999963\n",
            "\n",
            " iteration=824 bias=1.350000000000001 loss=6.580516666666692\n",
            "\n",
            " iteration=825 bias=1.360000000000001 loss=6.564916666666684\n",
            "\n",
            " iteration=826 bias=1.370000000000001 loss=6.549516666666675\n",
            "\n",
            " iteration=827 bias=1.380000000000001 loss=6.534316666666659\n",
            "\n",
            " iteration=828 bias=1.390000000000001 loss=6.5193166666666515\n",
            "\n",
            " iteration=829 bias=1.400000000000001 loss=6.504516666666636\n",
            "\n",
            " iteration=830 bias=1.410000000000001 loss=6.489916666666635\n",
            "\n",
            " iteration=831 bias=1.410000000000001 loss=6.489566666666697\n",
            "\n",
            " iteration=832 bias=1.420000000000001 loss=6.474066666666682\n",
            "\n",
            " iteration=833 bias=1.430000000000001 loss=6.458766666666675\n",
            "\n",
            " iteration=834 bias=1.440000000000001 loss=6.443666666666659\n",
            "\n",
            " iteration=835 bias=1.450000000000001 loss=6.42876666666665\n",
            "\n",
            " iteration=836 bias=1.460000000000001 loss=6.414066666666643\n",
            "\n",
            " iteration=837 bias=1.470000000000001 loss=6.399566666666627\n",
            "\n",
            " iteration=838 bias=1.470000000000001 loss=6.399250000000016\n",
            "\n",
            " iteration=839 bias=1.480000000000001 loss=6.383850000000017\n",
            "\n",
            " iteration=840 bias=1.490000000000001 loss=6.3686500000000015\n",
            "\n",
            " iteration=841 bias=1.500000000000001 loss=6.353649999999995\n",
            "\n",
            " iteration=842 bias=1.5100000000000011 loss=6.338849999999979\n",
            "\n",
            " iteration=843 bias=1.5200000000000011 loss=6.324249999999977\n",
            "\n",
            " iteration=844 bias=1.5300000000000011 loss=6.309849999999962\n",
            "\n",
            " iteration=845 bias=1.5300000000000011 loss=6.309566666666691\n",
            "\n",
            " iteration=846 bias=1.5400000000000011 loss=6.294266666666677\n",
            "\n",
            " iteration=847 bias=1.5500000000000012 loss=6.279166666666676\n",
            "\n",
            " iteration=848 bias=1.5600000000000012 loss=6.26426666666666\n",
            "\n",
            " iteration=849 bias=1.5700000000000012 loss=6.249566666666652\n",
            "\n",
            " iteration=850 bias=1.5800000000000012 loss=6.235066666666636\n",
            "\n",
            " iteration=851 bias=1.5900000000000012 loss=6.220766666666635\n",
            "\n",
            " iteration=852 bias=1.5900000000000012 loss=6.220516666666698\n",
            "\n",
            " iteration=853 bias=1.6000000000000012 loss=6.205316666666683\n",
            "\n",
            " iteration=854 bias=1.6100000000000012 loss=6.190316666666676\n",
            "\n",
            " iteration=855 bias=1.6200000000000012 loss=6.175516666666666\n",
            "\n",
            " iteration=856 bias=1.6300000000000012 loss=6.160916666666651\n",
            "\n",
            " iteration=857 bias=1.6400000000000012 loss=6.146516666666643\n",
            "\n",
            " iteration=858 bias=1.6500000000000012 loss=6.132316666666628\n",
            "\n",
            " iteration=859 bias=1.6500000000000012 loss=6.132100000000023\n",
            "\n",
            " iteration=860 bias=1.6600000000000013 loss=6.117000000000022\n",
            "\n",
            " iteration=861 bias=1.6700000000000013 loss=6.102100000000006\n",
            "\n",
            " iteration=862 bias=1.6800000000000013 loss=6.0874\n",
            "\n",
            " iteration=863 bias=1.6900000000000013 loss=6.07289999999998\n",
            "\n",
            " iteration=864 bias=1.7000000000000013 loss=6.0585999999999745\n",
            "\n",
            " iteration=865 bias=1.7100000000000013 loss=6.044499999999968\n",
            "\n",
            " iteration=866 bias=1.7100000000000013 loss=6.0443166666666945\n",
            "\n",
            " iteration=867 bias=1.7200000000000013 loss=6.029316666666678\n",
            "\n",
            " iteration=868 bias=1.7300000000000013 loss=6.0145166666666805\n",
            "\n",
            " iteration=869 bias=1.7400000000000013 loss=5.999916666666667\n",
            "\n",
            " iteration=870 bias=1.7500000000000013 loss=5.985516666666655\n",
            "\n",
            " iteration=871 bias=1.7600000000000013 loss=5.971316666666639\n",
            "\n",
            " iteration=872 bias=1.7700000000000014 loss=5.957316666666639\n",
            "\n",
            " iteration=873 bias=1.7700000000000014 loss=5.95716666666669\n",
            "\n",
            " iteration=874 bias=1.7800000000000014 loss=5.942266666666684\n",
            "\n",
            " iteration=875 bias=1.7900000000000014 loss=5.927566666666668\n",
            "\n",
            " iteration=876 bias=1.8000000000000014 loss=5.913066666666666\n",
            "\n",
            " iteration=877 bias=1.8100000000000014 loss=5.898766666666653\n",
            "\n",
            " iteration=878 bias=1.8200000000000014 loss=5.884666666666644\n",
            "\n",
            " iteration=879 bias=1.8300000000000014 loss=5.870766666666629\n",
            "\n",
            " iteration=880 bias=1.8300000000000014 loss=5.8706500000000235\n",
            "\n",
            " iteration=881 bias=1.8400000000000014 loss=5.8558500000000215\n",
            "\n",
            " iteration=882 bias=1.8500000000000014 loss=5.841250000000007\n",
            "\n",
            " iteration=883 bias=1.8600000000000014 loss=5.826849999999992\n",
            "\n",
            " iteration=884 bias=1.8700000000000014 loss=5.812649999999991\n",
            "\n",
            " iteration=885 bias=1.8800000000000014 loss=5.798649999999976\n",
            "\n",
            " iteration=886 bias=1.8900000000000015 loss=5.784849999999968\n",
            "\n",
            " iteration=887 bias=1.8900000000000015 loss=5.7847666666666955\n",
            "\n",
            " iteration=888 bias=1.9000000000000015 loss=5.770066666666682\n",
            "\n",
            " iteration=889 bias=1.9100000000000015 loss=5.75556666666668\n",
            "\n",
            " iteration=890 bias=1.9200000000000015 loss=5.7412666666666645\n",
            "\n",
            " iteration=891 bias=1.9300000000000015 loss=5.727166666666657\n",
            "\n",
            " iteration=892 bias=1.9400000000000015 loss=5.713266666666648\n",
            "\n",
            " iteration=893 bias=1.9500000000000015 loss=5.699566666666633\n",
            "\n",
            " iteration=894 bias=1.9500000000000015 loss=5.6995166666666925\n",
            "\n",
            " iteration=895 bias=1.9600000000000015 loss=5.684916666666681\n",
            "\n",
            " iteration=896 bias=1.9700000000000015 loss=5.670516666666667\n",
            "\n",
            " iteration=897 bias=1.9800000000000015 loss=5.656316666666669\n",
            "\n",
            " iteration=898 bias=1.9900000000000015 loss=5.642316666666654\n",
            "\n",
            " iteration=899 bias=2.0000000000000013 loss=5.628516666666644\n",
            "\n",
            " iteration=900 bias=2.010000000000001 loss=5.614916666666628\n",
            "\n",
            " iteration=901 bias=2.010000000000001 loss=5.614900000000024\n",
            "\n",
            " iteration=902 bias=2.020000000000001 loss=5.600400000000022\n",
            "\n",
            " iteration=903 bias=2.0300000000000007 loss=5.586100000000008\n",
            "\n",
            " iteration=904 bias=2.0400000000000005 loss=5.572\n",
            "\n",
            " iteration=905 bias=2.0500000000000003 loss=5.558099999999985\n",
            "\n",
            " iteration=906 bias=2.06 loss=5.5443999999999845\n",
            "\n",
            " iteration=907 bias=2.07 loss=5.53089999999997\n",
            "\n",
            " iteration=908 bias=2.0799999999999996 loss=5.517599999999962\n",
            "\n",
            " iteration=909 bias=2.0799999999999996 loss=5.516516666666689\n",
            "\n",
            " iteration=910 bias=2.0899999999999994 loss=5.502316666666673\n",
            "\n",
            " iteration=911 bias=2.099999999999999 loss=5.488316666666673\n",
            "\n",
            " iteration=912 bias=2.109999999999999 loss=5.474516666666658\n",
            "\n",
            " iteration=913 bias=2.1199999999999988 loss=5.460916666666651\n",
            "\n",
            " iteration=914 bias=2.1299999999999986 loss=5.447516666666634\n",
            "\n",
            " iteration=915 bias=2.1399999999999983 loss=5.434316666666635\n",
            "\n",
            " iteration=916 bias=2.1399999999999983 loss=5.433266666666696\n",
            "\n",
            " iteration=917 bias=2.149999999999998 loss=5.41916666666668\n",
            "\n",
            " iteration=918 bias=2.159999999999998 loss=5.405266666666672\n",
            "\n",
            " iteration=919 bias=2.1699999999999977 loss=5.391566666666658\n",
            "\n",
            " iteration=920 bias=2.1799999999999975 loss=5.378066666666656\n",
            "\n",
            " iteration=921 bias=2.1899999999999973 loss=5.364766666666642\n",
            "\n",
            " iteration=922 bias=2.199999999999997 loss=5.351666666666634\n",
            "\n",
            " iteration=923 bias=2.199999999999997 loss=5.3506500000000266\n",
            "\n",
            " iteration=924 bias=2.209999999999997 loss=5.336650000000012\n",
            "\n",
            " iteration=925 bias=2.2199999999999966 loss=5.322850000000014\n",
            "\n",
            " iteration=926 bias=2.2299999999999964 loss=5.309249999999998\n",
            "\n",
            " iteration=927 bias=2.239999999999996 loss=5.295849999999988\n",
            "\n",
            " iteration=928 bias=2.249999999999996 loss=5.282649999999973\n",
            "\n",
            " iteration=929 bias=2.259999999999996 loss=5.269649999999975\n",
            "\n",
            " iteration=930 bias=2.259999999999996 loss=5.268666666666697\n",
            "\n",
            " iteration=931 bias=2.2699999999999956 loss=5.254766666666682\n",
            "\n",
            " iteration=932 bias=2.2799999999999954 loss=5.241066666666675\n",
            "\n",
            " iteration=933 bias=2.289999999999995 loss=5.22756666666666\n",
            "\n",
            " iteration=934 bias=2.299999999999995 loss=5.214266666666659\n",
            "\n",
            " iteration=935 bias=2.3099999999999947 loss=5.201166666666643\n",
            "\n",
            " iteration=936 bias=2.3199999999999945 loss=5.188266666666636\n",
            "\n",
            " iteration=937 bias=2.3199999999999945 loss=5.187316666666697\n",
            "\n",
            " iteration=938 bias=2.3299999999999943 loss=5.173516666666682\n",
            "\n",
            " iteration=939 bias=2.339999999999994 loss=5.159916666666681\n",
            "\n",
            " iteration=940 bias=2.349999999999994 loss=5.146516666666666\n",
            "\n",
            " iteration=941 bias=2.3599999999999937 loss=5.133316666666658\n",
            "\n",
            " iteration=942 bias=2.3699999999999934 loss=5.1203166666666435\n",
            "\n",
            " iteration=943 bias=2.3799999999999932 loss=5.107516666666641\n",
            "\n",
            " iteration=944 bias=2.3799999999999932 loss=5.106600000000037\n",
            "\n",
            " iteration=945 bias=2.389999999999993 loss=5.0929000000000215\n",
            "\n",
            " iteration=946 bias=2.399999999999993 loss=5.079400000000014\n",
            "\n",
            " iteration=947 bias=2.4099999999999926 loss=5.066099999999999\n",
            "\n",
            " iteration=948 bias=2.4199999999999924 loss=5.052999999999997\n",
            "\n",
            " iteration=949 bias=2.429999999999992 loss=5.040099999999982\n",
            "\n",
            " iteration=950 bias=2.439999999999992 loss=5.027399999999975\n",
            "\n",
            " iteration=951 bias=2.439999999999992 loss=5.026516666666699\n",
            "\n",
            " iteration=952 bias=2.4499999999999917 loss=5.012916666666683\n",
            "\n",
            " iteration=953 bias=2.4599999999999915 loss=4.999516666666684\n",
            "\n",
            " iteration=954 bias=2.4699999999999913 loss=4.98631666666667\n",
            "\n",
            " iteration=955 bias=2.479999999999991 loss=4.973316666666658\n",
            "\n",
            " iteration=956 bias=2.489999999999991 loss=4.960516666666645\n",
            "\n",
            " iteration=957 bias=2.4999999999999907 loss=4.947916666666646\n",
            "\n",
            " iteration=958 bias=2.4999999999999907 loss=4.9470666666667045\n",
            "\n",
            " iteration=959 bias=2.5099999999999905 loss=4.93356666666669\n",
            "\n",
            " iteration=960 bias=2.5199999999999902 loss=4.9202666666666826\n",
            "\n",
            " iteration=961 bias=2.52999999999999 loss=4.907166666666668\n",
            "\n",
            " iteration=962 bias=2.53999999999999 loss=4.894266666666666\n",
            "\n",
            " iteration=963 bias=2.5499999999999896 loss=4.881566666666651\n",
            "\n",
            " iteration=964 bias=2.5599999999999894 loss=4.869066666666645\n",
            "\n",
            " iteration=965 bias=2.5599999999999894 loss=4.868250000000039\n",
            "\n",
            " iteration=966 bias=2.569999999999989 loss=4.854850000000023\n",
            "\n",
            " iteration=967 bias=2.579999999999989 loss=4.841650000000022\n",
            "\n",
            " iteration=968 bias=2.5899999999999888 loss=4.828650000000007\n",
            "\n",
            " iteration=969 bias=2.5999999999999885 loss=4.815849999999999\n",
            "\n",
            " iteration=970 bias=2.6099999999999883 loss=4.803249999999984\n",
            "\n",
            " iteration=971 bias=2.619999999999988 loss=4.790849999999982\n",
            "\n",
            " iteration=972 bias=2.619999999999988 loss=4.79006666666671\n",
            "\n",
            " iteration=973 bias=2.629999999999988 loss=4.7767666666666955\n",
            "\n",
            " iteration=974 bias=2.6399999999999877 loss=4.763666666666688\n",
            "\n",
            " iteration=975 bias=2.6499999999999875 loss=4.7507666666666735\n",
            "\n",
            " iteration=976 bias=2.6599999999999873 loss=4.738066666666671\n",
            "\n",
            " iteration=977 bias=2.669999999999987 loss=4.725566666666656\n",
            "\n",
            " iteration=978 bias=2.679999999999987 loss=4.713266666666649\n",
            "\n",
            " iteration=979 bias=2.679999999999987 loss=4.712516666666708\n",
            "\n",
            " iteration=980 bias=2.6899999999999866 loss=4.699316666666694\n",
            "\n",
            " iteration=981 bias=2.6999999999999864 loss=4.686316666666694\n",
            "\n",
            " iteration=982 bias=2.709999999999986 loss=4.673516666666679\n",
            "\n",
            " iteration=983 bias=2.719999999999986 loss=4.66091666666667\n",
            "\n",
            " iteration=984 bias=2.7299999999999858 loss=4.648516666666654\n",
            "\n",
            " iteration=985 bias=2.7399999999999856 loss=4.636316666666656\n",
            "\n",
            " iteration=986 bias=2.7399999999999856 loss=4.6356000000000455\n",
            "\n",
            " iteration=987 bias=2.7499999999999853 loss=4.622500000000031\n",
            "\n",
            " iteration=988 bias=2.759999999999985 loss=4.609600000000023\n",
            "\n",
            " iteration=989 bias=2.769999999999985 loss=4.5969000000000095\n",
            "\n",
            " iteration=990 bias=2.7799999999999847 loss=4.584400000000007\n",
            "\n",
            " iteration=991 bias=2.7899999999999845 loss=4.572099999999993\n",
            "\n",
            " iteration=992 bias=2.7999999999999843 loss=4.5599999999999845\n",
            "\n",
            " iteration=993 bias=2.7999999999999843 loss=4.559316666666712\n",
            "\n",
            " iteration=994 bias=2.809999999999984 loss=4.546316666666698\n",
            "\n",
            " iteration=995 bias=2.819999999999984 loss=4.533516666666695\n",
            "\n",
            " iteration=996 bias=2.8299999999999836 loss=4.520916666666681\n",
            "\n",
            " iteration=997 bias=2.8399999999999834 loss=4.508516666666673\n",
            "\n",
            " iteration=998 bias=2.849999999999983 loss=4.496316666666659\n",
            "\n",
            " iteration=999 bias=2.859999999999983 loss=4.484316666666657\n",
            "\n",
            " iteration=1000 bias=2.859999999999983 loss=4.483666666666718\n",
            "\n",
            " iteration=1001 bias=2.869999999999983 loss=4.4707666666667025\n",
            "\n",
            " iteration=1002 bias=2.8799999999999826 loss=4.458066666666695\n",
            "\n",
            " iteration=1003 bias=2.8899999999999824 loss=4.445566666666681\n",
            "\n",
            " iteration=1004 bias=2.899999999999982 loss=4.433266666666678\n",
            "\n",
            " iteration=1005 bias=2.909999999999982 loss=4.421166666666664\n",
            "\n",
            " iteration=1006 bias=2.9199999999999817 loss=4.409266666666656\n",
            "\n",
            " iteration=1007 bias=2.9199999999999817 loss=4.408650000000047\n",
            "\n",
            " iteration=1008 bias=2.9299999999999815 loss=4.395850000000032\n",
            "\n",
            " iteration=1009 bias=2.9399999999999813 loss=4.383250000000032\n",
            "\n",
            " iteration=1010 bias=2.949999999999981 loss=4.370850000000018\n",
            "\n",
            " iteration=1011 bias=2.959999999999981 loss=4.358650000000007\n",
            "\n",
            " iteration=1012 bias=2.9699999999999807 loss=4.346649999999992\n",
            "\n",
            " iteration=1013 bias=2.9799999999999804 loss=4.334849999999993\n",
            "\n",
            " iteration=1014 bias=2.9799999999999804 loss=4.334266666666719\n",
            "\n",
            " iteration=1015 bias=2.9899999999999802 loss=4.321566666666705\n",
            "\n",
            " iteration=1016 bias=2.99999999999998 loss=4.3090666666666975\n",
            "\n",
            " iteration=1017 bias=3.00999999999998 loss=4.296766666666683\n",
            "\n",
            " iteration=1018 bias=3.0199999999999796 loss=4.284666666666681\n",
            "\n",
            " iteration=1019 bias=3.0299999999999794 loss=4.272766666666666\n",
            "\n",
            " iteration=1020 bias=3.039999999999979 loss=4.261066666666658\n",
            "\n",
            " iteration=1021 bias=3.039999999999979 loss=4.260516666666718\n",
            "\n",
            " iteration=1022 bias=3.049999999999979 loss=4.247916666666705\n",
            "\n",
            " iteration=1023 bias=3.0599999999999787 loss=4.235516666666702\n",
            "\n",
            " iteration=1024 bias=3.0699999999999785 loss=4.223316666666688\n",
            "\n",
            " iteration=1025 bias=3.0799999999999783 loss=4.21131666666668\n",
            "\n",
            " iteration=1026 bias=3.089999999999978 loss=4.199516666666665\n",
            "\n",
            " iteration=1027 bias=3.099999999999978 loss=4.187916666666663\n",
            "\n",
            " iteration=1028 bias=3.099999999999978 loss=4.187400000000057\n",
            "\n",
            " iteration=1029 bias=3.1099999999999777 loss=4.174900000000043\n",
            "\n",
            " iteration=1030 bias=3.1199999999999775 loss=4.162600000000034\n",
            "\n",
            " iteration=1031 bias=3.1299999999999772 loss=4.150500000000021\n",
            "\n",
            " iteration=1032 bias=3.139999999999977 loss=4.138600000000017\n",
            "\n",
            " iteration=1033 bias=3.149999999999977 loss=4.126900000000004\n",
            "\n",
            " iteration=1034 bias=3.1599999999999766 loss=4.115399999999997\n",
            "\n",
            " iteration=1035 bias=3.1599999999999766 loss=4.114916666666722\n",
            "\n",
            " iteration=1036 bias=3.1699999999999764 loss=4.102516666666707\n",
            "\n",
            " iteration=1037 bias=3.179999999999976 loss=4.090316666666708\n",
            "\n",
            " iteration=1038 bias=3.189999999999976 loss=4.0783166666666935\n",
            "\n",
            " iteration=1039 bias=3.1999999999999758 loss=4.066516666666683\n",
            "\n",
            " iteration=1040 bias=3.2099999999999755 loss=4.054916666666669\n",
            "\n",
            " iteration=1041 bias=3.2199999999999753 loss=4.043516666666669\n",
            "\n",
            " iteration=1042 bias=3.2199999999999753 loss=4.043066666666724\n",
            "\n",
            " iteration=1043 bias=3.229999999999975 loss=4.030766666666711\n",
            "\n",
            " iteration=1044 bias=3.239999999999975 loss=4.018666666666703\n",
            "\n",
            " iteration=1045 bias=3.2499999999999747 loss=4.00676666666669\n",
            "\n",
            " iteration=1046 bias=3.2599999999999745 loss=3.9950666666666863\n",
            "\n",
            " iteration=1047 bias=3.2699999999999743 loss=3.9835666666666723\n",
            "\n",
            " iteration=1048 bias=3.279999999999974 loss=3.972266666666664\n",
            "\n",
            " iteration=1049 bias=3.279999999999974 loss=3.971850000000059\n",
            "\n",
            " iteration=1050 bias=3.289999999999974 loss=3.959650000000044\n",
            "\n",
            " iteration=1051 bias=3.2999999999999736 loss=3.9476500000000416\n",
            "\n",
            " iteration=1052 bias=3.3099999999999734 loss=3.935850000000027\n",
            "\n",
            " iteration=1053 bias=3.319999999999973 loss=3.92425000000002\n",
            "\n",
            " iteration=1054 bias=3.329999999999973 loss=3.9128500000000046\n",
            "\n",
            " iteration=1055 bias=3.3399999999999728 loss=3.9016500000000023\n",
            "\n",
            " iteration=1056 bias=3.3399999999999728 loss=3.90126666666673\n",
            "\n",
            " iteration=1057 bias=3.3499999999999726 loss=3.889166666666716\n",
            "\n",
            " iteration=1058 bias=3.3599999999999723 loss=3.8772666666667077\n",
            "\n",
            " iteration=1059 bias=3.369999999999972 loss=3.8655666666666932\n",
            "\n",
            " iteration=1060 bias=3.379999999999972 loss=3.8540666666666907\n",
            "\n",
            " iteration=1061 bias=3.3899999999999717 loss=3.8427666666666767\n",
            "\n",
            " iteration=1062 bias=3.3999999999999715 loss=3.8316666666666688\n",
            "\n",
            " iteration=1063 bias=3.3999999999999715 loss=3.831316666666726\n",
            "\n",
            " iteration=1064 bias=3.4099999999999713 loss=3.8193166666667118\n",
            "\n",
            " iteration=1065 bias=3.419999999999971 loss=3.8075166666667113\n",
            "\n",
            " iteration=1066 bias=3.429999999999971 loss=3.7959166666666975\n",
            "\n",
            " iteration=1067 bias=3.4399999999999706 loss=3.7845166666666863\n",
            "\n",
            " iteration=1068 bias=3.4499999999999704 loss=3.773316666666672\n",
            "\n",
            " iteration=1069 bias=3.45999999999997 loss=3.762316666666672\n",
            "\n",
            " iteration=1070 bias=3.45999999999997 loss=3.7620000000000644\n",
            "\n",
            " iteration=1071 bias=3.46999999999997 loss=3.750100000000051\n",
            "\n",
            " iteration=1072 bias=3.47999999999997 loss=3.738400000000043\n",
            "\n",
            " iteration=1073 bias=3.4899999999999696 loss=3.726900000000029\n",
            "\n",
            " iteration=1074 bias=3.4999999999999694 loss=3.715600000000025\n",
            "\n",
            " iteration=1075 bias=3.509999999999969 loss=3.704500000000012\n",
            "\n",
            " iteration=1076 bias=3.519999999999969 loss=3.6936000000000035\n",
            "\n",
            " iteration=1077 bias=3.519999999999969 loss=3.693316666666732\n",
            "\n",
            " iteration=1078 bias=3.5299999999999687 loss=3.6815166666667163\n",
            "\n",
            " iteration=1079 bias=3.5399999999999685 loss=3.669916666666714\n",
            "\n",
            " iteration=1080 bias=3.5499999999999683 loss=3.6585166666666997\n",
            "\n",
            " iteration=1081 bias=3.559999999999968 loss=3.6473166666666916\n",
            "\n",
            " iteration=1082 bias=3.569999999999968 loss=3.6363166666666777\n",
            "\n",
            " iteration=1083 bias=3.5799999999999677 loss=3.625516666666675\n",
            "\n",
            " iteration=1084 bias=3.5799999999999677 loss=3.625266666666736\n",
            "\n",
            " iteration=1085 bias=3.5899999999999674 loss=3.613566666666722\n",
            "\n",
            " iteration=1086 bias=3.5999999999999672 loss=3.602066666666713\n",
            "\n",
            " iteration=1087 bias=3.609999999999967 loss=3.590766666666699\n",
            "\n",
            " iteration=1088 bias=3.619999999999967 loss=3.5796666666666965\n",
            "\n",
            " iteration=1089 bias=3.6299999999999666 loss=3.5687666666666824\n",
            "\n",
            " iteration=1090 bias=3.6399999999999664 loss=3.558066666666674\n",
            "\n",
            " iteration=1091 bias=3.6399999999999664 loss=3.5578500000000677\n",
            "\n",
            " iteration=1092 bias=3.649999999999966 loss=3.5462500000000525\n",
            "\n",
            " iteration=1093 bias=3.659999999999966 loss=3.534850000000052\n",
            "\n",
            " iteration=1094 bias=3.6699999999999657 loss=3.5236500000000386\n",
            "\n",
            " iteration=1095 bias=3.6799999999999655 loss=3.512650000000028\n",
            "\n",
            " iteration=1096 bias=3.6899999999999653 loss=3.5018500000000135\n",
            "\n",
            " iteration=1097 bias=3.699999999999965 loss=3.4912500000000137\n",
            "\n",
            " iteration=1098 bias=3.699999999999965 loss=3.4910666666667365\n",
            "\n",
            " iteration=1099 bias=3.709999999999965 loss=3.4795666666667224\n",
            "\n",
            " iteration=1100 bias=3.7199999999999647 loss=3.4682666666667146\n",
            "\n",
            " iteration=1101 bias=3.7299999999999645 loss=3.4571666666667014\n",
            "\n",
            " iteration=1102 bias=3.7399999999999642 loss=3.4462666666666983\n",
            "\n",
            " iteration=1103 bias=3.749999999999964 loss=3.4355666666666838\n",
            "\n",
            " iteration=1104 bias=3.759999999999964 loss=3.4250666666666754\n",
            "\n",
            " iteration=1105 bias=3.759999999999964 loss=3.424916666666736\n",
            "\n",
            " iteration=1106 bias=3.7699999999999636 loss=3.4135166666667227\n",
            "\n",
            " iteration=1107 bias=3.7799999999999634 loss=3.402316666666719\n",
            "\n",
            " iteration=1108 bias=3.789999999999963 loss=3.391316666666705\n",
            "\n",
            " iteration=1109 bias=3.799999999999963 loss=3.3805166666666975\n",
            "\n",
            " iteration=1110 bias=3.8099999999999627 loss=3.369916666666684\n",
            "\n",
            " iteration=1111 bias=3.8199999999999625 loss=3.3595166666666803\n",
            "\n",
            " iteration=1112 bias=3.8199999999999625 loss=3.3594000000000737\n",
            "\n",
            " iteration=1113 bias=3.8299999999999623 loss=3.3481000000000596\n",
            "\n",
            " iteration=1114 bias=3.839999999999962 loss=3.3370000000000526\n",
            "\n",
            " iteration=1115 bias=3.849999999999962 loss=3.326100000000038\n",
            "\n",
            " iteration=1116 bias=3.8599999999999617 loss=3.3154000000000354\n",
            "\n",
            " iteration=1117 bias=3.8699999999999615 loss=3.3049000000000213\n",
            "\n",
            " iteration=1118 bias=3.8799999999999613 loss=3.2946000000000133\n",
            "\n",
            " iteration=1119 bias=3.8799999999999613 loss=3.2945166666667363\n",
            "\n",
            " iteration=1120 bias=3.889999999999961 loss=3.283316666666723\n",
            "\n",
            " iteration=1121 bias=3.899999999999961 loss=3.2723166666667214\n",
            "\n",
            " iteration=1122 bias=3.9099999999999606 loss=3.261516666666708\n",
            "\n",
            " iteration=1123 bias=3.9199999999999604 loss=3.2509166666666975\n",
            "\n",
            " iteration=1124 bias=3.92999999999996 loss=3.2405166666666836\n",
            "\n",
            " iteration=1125 bias=3.93999999999996 loss=3.2303166666666825\n",
            "\n",
            " iteration=1126 bias=3.93999999999996 loss=3.2302666666667417\n",
            "\n",
            " iteration=1127 bias=3.9499999999999598 loss=3.219166666666728\n",
            "\n",
            " iteration=1128 bias=3.9599999999999596 loss=3.20826666666672\n",
            "\n",
            " iteration=1129 bias=3.9699999999999593 loss=3.197566666666706\n",
            "\n",
            " iteration=1130 bias=3.979999999999959 loss=3.1870666666667025\n",
            "\n",
            " iteration=1131 bias=3.989999999999959 loss=3.1767666666666887\n",
            "\n",
            " iteration=1132 bias=3.9999999999999587 loss=3.1666666666666807\n",
            "\n",
            " iteration=1133 bias=3.9999999999999587 loss=3.166650000000075\n",
            "\n",
            " iteration=1134 bias=4.009999999999959 loss=3.1556500000000653\n",
            "\n",
            " iteration=1135 bias=4.019999999999959 loss=3.1448500000000514\n",
            "\n",
            " iteration=1136 bias=4.0299999999999585 loss=3.134250000000043\n",
            "\n",
            " iteration=1137 bias=4.039999999999958 loss=3.123850000000035\n",
            "\n",
            " iteration=1138 bias=4.049999999999958 loss=3.1136500000000216\n",
            "\n",
            " iteration=1139 bias=4.059999999999958 loss=3.1036500000000125\n",
            "\n",
            " iteration=1140 bias=4.069999999999958 loss=3.0938500000000047\n",
            "\n",
            " iteration=1141 bias=4.069999999999958 loss=3.092766666666732\n",
            "\n",
            " iteration=1142 bias=4.079999999999957 loss=3.0820666666667242\n",
            "\n",
            " iteration=1143 bias=4.089999999999957 loss=3.0715666666667167\n",
            "\n",
            " iteration=1144 bias=4.099999999999957 loss=3.0612666666667003\n",
            "\n",
            " iteration=1145 bias=4.109999999999957 loss=3.0511666666666923\n",
            "\n",
            " iteration=1146 bias=4.119999999999957 loss=3.0412666666666843\n",
            "\n",
            " iteration=1147 bias=4.129999999999956 loss=3.0315666666666683\n",
            "\n",
            " iteration=1148 bias=4.129999999999956 loss=3.03051666666673\n",
            "\n",
            " iteration=1149 bias=4.139999999999956 loss=3.019916666666726\n",
            "\n",
            " iteration=1150 bias=4.149999999999956 loss=3.0095166666667126\n",
            "\n",
            " iteration=1151 bias=4.159999999999956 loss=2.999316666666701\n",
            "\n",
            " iteration=1152 bias=4.1699999999999555 loss=2.9893166666666975\n",
            "\n",
            " iteration=1153 bias=4.179999999999955 loss=2.979516666666687\n",
            "\n",
            " iteration=1154 bias=4.189999999999955 loss=2.9699166666666756\n",
            "\n",
            " iteration=1155 bias=4.189999999999955 loss=2.968900000000066\n",
            "\n",
            " iteration=1156 bias=4.199999999999955 loss=2.958400000000058\n",
            "\n",
            " iteration=1157 bias=4.209999999999955 loss=2.9481000000000464\n",
            "\n",
            " iteration=1158 bias=4.2199999999999545 loss=2.938000000000035\n",
            "\n",
            " iteration=1159 bias=4.229999999999954 loss=2.9281000000000272\n",
            "\n",
            " iteration=1160 bias=4.239999999999954 loss=2.9184000000000183\n",
            "\n",
            " iteration=1161 bias=4.249999999999954 loss=2.9089000000000076\n",
            "\n",
            " iteration=1162 bias=4.249999999999954 loss=2.907916666666736\n",
            "\n",
            " iteration=1163 bias=4.259999999999954 loss=2.897516666666723\n",
            "\n",
            " iteration=1164 bias=4.269999999999953 loss=2.887316666666715\n",
            "\n",
            " iteration=1165 bias=4.279999999999953 loss=2.8773166666667067\n",
            "\n",
            " iteration=1166 bias=4.289999999999953 loss=2.867516666666693\n",
            "\n",
            " iteration=1167 bias=4.299999999999953 loss=2.8579166666666835\n",
            "\n",
            " iteration=1168 bias=4.3099999999999525 loss=2.8485166666666757\n",
            "\n",
            " iteration=1169 bias=4.3099999999999525 loss=2.8475666666667365\n",
            "\n",
            " iteration=1170 bias=4.319999999999952 loss=2.8372666666667286\n",
            "\n",
            " iteration=1171 bias=4.329999999999952 loss=2.8271666666667166\n",
            "\n",
            " iteration=1172 bias=4.339999999999952 loss=2.8172666666667054\n",
            "\n",
            " iteration=1173 bias=4.349999999999952 loss=2.807566666666697\n",
            "\n",
            " iteration=1174 bias=4.3599999999999515 loss=2.798066666666689\n",
            "\n",
            " iteration=1175 bias=4.369999999999951 loss=2.788766666666677\n",
            "\n",
            " iteration=1176 bias=4.369999999999951 loss=2.787850000000065\n",
            "\n",
            " iteration=1177 bias=4.379999999999951 loss=2.7776500000000617\n",
            "\n",
            " iteration=1178 bias=4.389999999999951 loss=2.767650000000051\n",
            "\n",
            " iteration=1179 bias=4.399999999999951 loss=2.7578500000000368\n",
            "\n",
            " iteration=1180 bias=4.40999999999995 loss=2.748250000000033\n",
            "\n",
            " iteration=1181 bias=4.41999999999995 loss=2.7388500000000224\n",
            "\n",
            " iteration=1182 bias=4.42999999999995 loss=2.729650000000009\n",
            "\n",
            " iteration=1183 bias=4.42999999999995 loss=2.7287666666667376\n",
            "\n",
            " iteration=1184 bias=4.43999999999995 loss=2.718666666666729\n",
            "\n",
            " iteration=1185 bias=4.4499999999999496 loss=2.708766666666713\n",
            "\n",
            " iteration=1186 bias=4.459999999999949 loss=2.699066666666706\n",
            "\n",
            " iteration=1187 bias=4.469999999999949 loss=2.689566666666698\n",
            "\n",
            " iteration=1188 bias=4.479999999999949 loss=2.68026666666669\n",
            "\n",
            " iteration=1189 bias=4.489999999999949 loss=2.6711666666666822\n",
            "\n",
            " iteration=1190 bias=4.489999999999949 loss=2.6703166666667406\n",
            "\n",
            " iteration=1191 bias=4.4999999999999485 loss=2.660316666666727\n",
            "\n",
            " iteration=1192 bias=4.509999999999948 loss=2.6505166666667193\n",
            "\n",
            " iteration=1193 bias=4.519999999999948 loss=2.6409166666667105\n",
            "\n",
            " iteration=1194 bias=4.529999999999948 loss=2.6315166666666974\n",
            "\n",
            " iteration=1195 bias=4.539999999999948 loss=2.622316666666688\n",
            "\n",
            " iteration=1196 bias=4.549999999999947 loss=2.61331666666668\n",
            "\n",
            " iteration=1197 bias=4.549999999999947 loss=2.612500000000074\n",
            "\n",
            " iteration=1198 bias=4.559999999999947 loss=2.6026000000000655\n",
            "\n",
            " iteration=1199 bias=4.569999999999947 loss=2.592900000000058\n",
            "\n",
            " iteration=1200 bias=4.579999999999947 loss=2.5834000000000423\n",
            "\n",
            " iteration=1201 bias=4.589999999999947 loss=2.574100000000034\n",
            "\n",
            " iteration=1202 bias=4.599999999999946 loss=2.565000000000026\n",
            "\n",
            " iteration=1203 bias=4.609999999999946 loss=2.5561000000000105\n",
            "\n",
            " iteration=1204 bias=4.609999999999946 loss=2.5553166666667386\n",
            "\n",
            " iteration=1205 bias=4.619999999999946 loss=2.5455166666667344\n",
            "\n",
            " iteration=1206 bias=4.629999999999946 loss=2.5359166666667208\n",
            "\n",
            " iteration=1207 bias=4.6399999999999455 loss=2.5265166666667094\n",
            "\n",
            " iteration=1208 bias=4.649999999999945 loss=2.5173166666667055\n",
            "\n",
            " iteration=1209 bias=4.659999999999945 loss=2.5083166666666954\n",
            "\n",
            " iteration=1210 bias=4.669999999999945 loss=2.499516666666684\n",
            "\n",
            " iteration=1211 bias=4.669999999999945 loss=2.498766666666741\n",
            "\n",
            " iteration=1212 bias=4.679999999999945 loss=2.489066666666733\n",
            "\n",
            " iteration=1213 bias=4.689999999999944 loss=2.4795666666667215\n",
            "\n",
            " iteration=1214 bias=4.699999999999944 loss=2.4702666666667095\n",
            "\n",
            " iteration=1215 bias=4.709999999999944 loss=2.4611666666667014\n",
            "\n",
            " iteration=1216 bias=4.719999999999944 loss=2.4522666666666937\n",
            "\n",
            " iteration=1217 bias=4.729999999999944 loss=2.443566666666682\n",
            "\n",
            " iteration=1218 bias=4.729999999999944 loss=2.4428500000000777\n",
            "\n",
            " iteration=1219 bias=4.739999999999943 loss=2.4332500000000645\n",
            "\n",
            " iteration=1220 bias=4.749999999999943 loss=2.4238500000000562\n",
            "\n",
            " iteration=1221 bias=4.759999999999943 loss=2.414650000000048\n",
            "\n",
            " iteration=1222 bias=4.769999999999943 loss=2.405650000000034\n",
            "\n",
            " iteration=1223 bias=4.7799999999999425 loss=2.396850000000025\n",
            "\n",
            " iteration=1224 bias=4.789999999999942 loss=2.3882500000000166\n",
            "\n",
            " iteration=1225 bias=4.789999999999942 loss=2.387566666666744\n",
            "\n",
            " iteration=1226 bias=4.799999999999942 loss=2.3780666666667356\n",
            "\n",
            " iteration=1227 bias=4.809999999999942 loss=2.368766666666724\n",
            "\n",
            " iteration=1228 bias=4.819999999999942 loss=2.359666666666713\n",
            "\n",
            " iteration=1229 bias=4.8299999999999415 loss=2.3507666666667046\n",
            "\n",
            " iteration=1230 bias=4.839999999999941 loss=2.342066666666696\n",
            "\n",
            " iteration=1231 bias=4.849999999999941 loss=2.3335666666666843\n",
            "\n",
            " iteration=1232 bias=4.849999999999941 loss=2.3329166666667405\n",
            "\n",
            " iteration=1233 bias=4.859999999999941 loss=2.3235166666667357\n",
            "\n",
            " iteration=1234 bias=4.869999999999941 loss=2.3143166666667248\n",
            "\n",
            " iteration=1235 bias=4.87999999999994 loss=2.305316666666711\n",
            "\n",
            " iteration=1236 bias=4.88999999999994 loss=2.2965166666667067\n",
            "\n",
            " iteration=1237 bias=4.89999999999994 loss=2.2879166666666966\n",
            "\n",
            " iteration=1238 bias=4.90999999999994 loss=2.279516666666683\n",
            "\n",
            " iteration=1239 bias=4.90999999999994 loss=2.278900000000078\n",
            "\n",
            " iteration=1240 bias=4.9199999999999395 loss=2.2696000000000693\n",
            "\n",
            " iteration=1241 bias=4.929999999999939 loss=2.260500000000054\n",
            "\n",
            " iteration=1242 bias=4.939999999999939 loss=2.2516000000000473\n",
            "\n",
            " iteration=1243 bias=4.949999999999939 loss=2.2429000000000383\n",
            "\n",
            " iteration=1244 bias=4.959999999999939 loss=2.2344000000000306\n",
            "\n",
            " iteration=1245 bias=4.9699999999999385 loss=2.2261000000000224\n",
            "\n",
            " iteration=1246 bias=4.9699999999999385 loss=2.225516666666748\n",
            "\n",
            " iteration=1247 bias=4.979999999999938 loss=2.2163166666667347\n",
            "\n",
            " iteration=1248 bias=4.989999999999938 loss=2.207316666666726\n",
            "\n",
            " iteration=1249 bias=4.999999999999938 loss=2.198516666666718\n",
            "\n",
            " iteration=1250 bias=5.009999999999938 loss=2.1899166666667043\n",
            "\n",
            " iteration=1251 bias=5.019999999999937 loss=2.181516666666695\n",
            "\n",
            " iteration=1252 bias=5.029999999999937 loss=2.1733166666666865\n",
            "\n",
            " iteration=1253 bias=5.029999999999937 loss=2.1727666666667473\n",
            "\n",
            " iteration=1254 bias=5.039999999999937 loss=2.163666666666739\n",
            "\n",
            " iteration=1255 bias=5.049999999999937 loss=2.154766666666731\n",
            "\n",
            " iteration=1256 bias=5.0599999999999365 loss=2.146066666666716\n",
            "\n",
            " iteration=1257 bias=5.069999999999936 loss=2.1375666666667077\n",
            "\n",
            " iteration=1258 bias=5.079999999999936 loss=2.1292666666666995\n",
            "\n",
            " iteration=1259 bias=5.089999999999936 loss=2.1211666666666837\n",
            "\n",
            " iteration=1260 bias=5.089999999999936 loss=2.1206500000000785\n",
            "\n",
            " iteration=1261 bias=5.099999999999936 loss=2.1116500000000737\n",
            "\n",
            " iteration=1262 bias=5.1099999999999355 loss=2.1028500000000605\n",
            "\n",
            " iteration=1263 bias=5.119999999999935 loss=2.09425000000005\n",
            "\n",
            " iteration=1264 bias=5.129999999999935 loss=2.085850000000045\n",
            "\n",
            " iteration=1265 bias=5.139999999999935 loss=2.0776500000000344\n",
            "\n",
            " iteration=1266 bias=5.149999999999935 loss=2.0696500000000237\n",
            "\n",
            " iteration=1267 bias=5.149999999999935 loss=2.069166666666747\n",
            "\n",
            " iteration=1268 bias=5.159999999999934 loss=2.0602666666667386\n",
            "\n",
            " iteration=1269 bias=5.169999999999934 loss=2.051566666666728\n",
            "\n",
            " iteration=1270 bias=5.179999999999934 loss=2.0430666666667165\n",
            "\n",
            " iteration=1271 bias=5.189999999999934 loss=2.034766666666708\n",
            "\n",
            " iteration=1272 bias=5.199999999999934 loss=2.0266666666666997\n",
            "\n",
            " iteration=1273 bias=5.209999999999933 loss=2.018766666666689\n",
            "\n",
            " iteration=1274 bias=5.209999999999933 loss=2.0183166666667502\n",
            "\n",
            " iteration=1275 bias=5.219999999999933 loss=2.009516666666737\n",
            "\n",
            " iteration=1276 bias=5.229999999999933 loss=2.0009166666667286\n",
            "\n",
            " iteration=1277 bias=5.239999999999933 loss=1.9925166666667204\n",
            "\n",
            " iteration=1278 bias=5.2499999999999325 loss=1.9843166666667071\n",
            "\n",
            " iteration=1279 bias=5.259999999999932 loss=1.9763166666666983\n",
            "\n",
            " iteration=1280 bias=5.269999999999932 loss=1.9685166666666893\n",
            "\n",
            " iteration=1281 bias=5.269999999999932 loss=1.9681000000000832\n",
            "\n",
            " iteration=1282 bias=5.279999999999932 loss=1.959400000000075\n",
            "\n",
            " iteration=1283 bias=5.289999999999932 loss=1.9509000000000636\n",
            "\n",
            " iteration=1284 bias=5.299999999999931 loss=1.942600000000052\n",
            "\n",
            " iteration=1285 bias=5.309999999999931 loss=1.9345000000000436\n",
            "\n",
            " iteration=1286 bias=5.319999999999931 loss=1.9266000000000352\n",
            "\n",
            " iteration=1287 bias=5.329999999999931 loss=1.9189000000000238\n",
            "\n",
            " iteration=1288 bias=5.329999999999931 loss=1.9185166666667464\n",
            "\n",
            " iteration=1289 bias=5.339999999999931 loss=1.909916666666741\n",
            "\n",
            " iteration=1290 bias=5.34999999999993 loss=1.9015166666667298\n",
            "\n",
            " iteration=1291 bias=5.35999999999993 loss=1.8933166666667176\n",
            "\n",
            " iteration=1292 bias=5.36999999999993 loss=1.8853166666667123\n",
            "\n",
            " iteration=1293 bias=5.37999999999993 loss=1.877516666666702\n",
            "\n",
            " iteration=1294 bias=5.3899999999999295 loss=1.8699166666666889\n",
            "\n",
            " iteration=1295 bias=5.3899999999999295 loss=1.8695666666667503\n",
            "\n",
            " iteration=1296 bias=5.399999999999929 loss=1.8610666666667413\n",
            "\n",
            " iteration=1297 bias=5.409999999999929 loss=1.8527666666667268\n",
            "\n",
            " iteration=1298 bias=5.419999999999929 loss=1.8446666666667193\n",
            "\n",
            " iteration=1299 bias=5.429999999999929 loss=1.8367666666667104\n",
            "\n",
            " iteration=1300 bias=5.4399999999999284 loss=1.8290666666667021\n",
            "\n",
            " iteration=1301 bias=5.449999999999928 loss=1.8215666666666948\n",
            "\n",
            " iteration=1302 bias=5.449999999999928 loss=1.8212500000000862\n",
            "\n",
            " iteration=1303 bias=5.459999999999928 loss=1.8128500000000731\n",
            "\n",
            " iteration=1304 bias=5.469999999999928 loss=1.8046500000000647\n",
            "\n",
            " iteration=1305 bias=5.479999999999928 loss=1.7966500000000565\n",
            "\n",
            " iteration=1306 bias=5.489999999999927 loss=1.7888500000000433\n",
            "\n",
            " iteration=1307 bias=5.499999999999927 loss=1.7812500000000338\n",
            "\n",
            " iteration=1308 bias=5.509999999999927 loss=1.773850000000025\n",
            "\n",
            " iteration=1309 bias=5.509999999999927 loss=1.7735666666667524\n",
            "\n",
            " iteration=1310 bias=5.519999999999927 loss=1.7652666666667438\n",
            "\n",
            " iteration=1311 bias=5.5299999999999265 loss=1.7571666666667358\n",
            "\n",
            " iteration=1312 bias=5.539999999999926 loss=1.7492666666667211\n",
            "\n",
            " iteration=1313 bias=5.549999999999926 loss=1.7415666666667127\n",
            "\n",
            " iteration=1314 bias=5.559999999999926 loss=1.7340666666667037\n",
            "\n",
            " iteration=1315 bias=5.569999999999926 loss=1.7267666666666892\n",
            "\n",
            " iteration=1316 bias=5.569999999999926 loss=1.7265166666667504\n",
            "\n",
            " iteration=1317 bias=5.5799999999999255 loss=1.7183166666667453\n",
            "\n",
            " iteration=1318 bias=5.589999999999925 loss=1.7103166666667324\n",
            "\n",
            " iteration=1319 bias=5.599999999999925 loss=1.7025166666667213\n",
            "\n",
            " iteration=1320 bias=5.609999999999925 loss=1.694916666666716\n",
            "\n",
            " iteration=1321 bias=5.619999999999925 loss=1.687516666666706\n",
            "\n",
            " iteration=1322 bias=5.629999999999924 loss=1.6803166666666947\n",
            "\n",
            " iteration=1323 bias=5.629999999999924 loss=1.6801000000000854\n",
            "\n",
            " iteration=1324 bias=5.639999999999924 loss=1.6720000000000772\n",
            "\n",
            " iteration=1325 bias=5.649999999999924 loss=1.6641000000000659\n",
            "\n",
            " iteration=1326 bias=5.659999999999924 loss=1.6564000000000547\n",
            "\n",
            " iteration=1327 bias=5.6699999999999235 loss=1.6489000000000458\n",
            "\n",
            " iteration=1328 bias=5.679999999999923 loss=1.6416000000000377\n",
            "\n",
            " iteration=1329 bias=5.689999999999923 loss=1.6345000000000265\n",
            "\n",
            " iteration=1330 bias=5.689999999999923 loss=1.6343166666667546\n",
            "\n",
            " iteration=1331 bias=5.699999999999923 loss=1.6263166666667417\n",
            "\n",
            " iteration=1332 bias=5.709999999999923 loss=1.6185166666667332\n",
            "\n",
            " iteration=1333 bias=5.7199999999999225 loss=1.610916666666725\n",
            "\n",
            " iteration=1334 bias=5.729999999999922 loss=1.603516666666712\n",
            "\n",
            " iteration=1335 bias=5.739999999999922 loss=1.5963166666667024\n",
            "\n",
            " iteration=1336 bias=5.749999999999922 loss=1.5893166666666938\n",
            "\n",
            " iteration=1337 bias=5.749999999999922 loss=1.589166666666754\n",
            "\n",
            " iteration=1338 bias=5.759999999999922 loss=1.5812666666667454\n",
            "\n",
            " iteration=1339 bias=5.769999999999921 loss=1.573566666666734\n",
            "\n",
            " iteration=1340 bias=5.779999999999921 loss=1.5660666666667231\n",
            "\n",
            " iteration=1341 bias=5.789999999999921 loss=1.5587666666667144\n",
            "\n",
            " iteration=1342 bias=5.799999999999921 loss=1.5516666666667058\n",
            "\n",
            " iteration=1343 bias=5.809999999999921 loss=1.5447666666666946\n",
            "\n",
            " iteration=1344 bias=5.809999999999921 loss=1.5446500000000842\n",
            "\n",
            " iteration=1345 bias=5.81999999999992 loss=1.5368500000000787\n",
            "\n",
            " iteration=1346 bias=5.82999999999992 loss=1.5292500000000677\n",
            "\n",
            " iteration=1347 bias=5.83999999999992 loss=1.5218500000000548\n",
            "\n",
            " iteration=1348 bias=5.84999999999992 loss=1.5146500000000493\n",
            "\n",
            " iteration=1349 bias=5.8599999999999195 loss=1.507650000000039\n",
            "\n",
            " iteration=1350 bias=5.869999999999919 loss=1.5008500000000264\n",
            "\n",
            " iteration=1351 bias=5.869999999999919 loss=1.500766666666754\n",
            "\n",
            " iteration=1352 bias=5.879999999999919 loss=1.4930666666667456\n",
            "\n",
            " iteration=1353 bias=5.889999999999919 loss=1.485566666666731\n",
            "\n",
            " iteration=1354 bias=5.899999999999919 loss=1.478266666666723\n",
            "\n",
            " iteration=1355 bias=5.909999999999918 loss=1.4711666666667143\n",
            "\n",
            " iteration=1356 bias=5.919999999999918 loss=1.4642666666667055\n",
            "\n",
            " iteration=1357 bias=5.929999999999918 loss=1.4575666666666978\n",
            "\n",
            " iteration=1358 bias=5.929999999999918 loss=1.4575166666667565\n",
            "\n",
            " iteration=1359 bias=5.939999999999918 loss=1.4499166666667433\n",
            "\n",
            " iteration=1360 bias=5.949999999999918 loss=1.4425166666667348\n",
            "\n",
            " iteration=1361 bias=5.959999999999917 loss=1.4353166666667265\n",
            "\n",
            " iteration=1362 bias=5.969999999999917 loss=1.4283166666667135\n",
            "\n",
            " iteration=1363 bias=5.979999999999917 loss=1.4215166666667038\n",
            "\n",
            " iteration=1364 bias=5.989999999999917 loss=1.4149166666666952\n",
            "\n",
            " iteration=1365 bias=5.989999999999917 loss=1.414900000000089\n",
            "\n",
            " iteration=1366 bias=5.9999999999999165 loss=1.4074000000000801\n",
            "\n",
            " iteration=1367 bias=6.009999999999916 loss=1.400100000000072\n",
            "\n",
            " iteration=1368 bias=6.019999999999916 loss=1.3930000000000582\n",
            "\n",
            " iteration=1369 bias=6.029999999999916 loss=1.3861000000000494\n",
            "\n",
            " iteration=1370 bias=6.039999999999916 loss=1.3794000000000406\n",
            "\n",
            " iteration=1371 bias=6.0499999999999154 loss=1.3729000000000262\n",
            "\n",
            " iteration=1372 bias=6.059999999999915 loss=1.366600000000018\n",
            "\n",
            " iteration=1373 bias=6.059999999999915 loss=1.365516666666748\n",
            "\n",
            " iteration=1374 bias=6.069999999999915 loss=1.3583166666667355\n",
            "\n",
            " iteration=1375 bias=6.079999999999915 loss=1.3513166666667245\n",
            "\n",
            " iteration=1376 bias=6.089999999999915 loss=1.3445166666667188\n",
            "\n",
            " iteration=1377 bias=6.099999999999914 loss=1.3379166666667084\n",
            "\n",
            " iteration=1378 bias=6.109999999999914 loss=1.3315166666666973\n",
            "\n",
            " iteration=1379 bias=6.119999999999914 loss=1.3253166666666847\n",
            "\n",
            " iteration=1380 bias=6.119999999999914 loss=1.3242666666667466\n",
            "\n",
            " iteration=1381 bias=6.129999999999914 loss=1.3171666666667357\n",
            "\n",
            " iteration=1382 bias=6.1399999999999135 loss=1.3102666666667244\n",
            "\n",
            " iteration=1383 bias=6.149999999999913 loss=1.3035666666667156\n",
            "\n",
            " iteration=1384 bias=6.159999999999913 loss=1.2970666666667068\n",
            "\n",
            " iteration=1385 bias=6.169999999999913 loss=1.290766666666696\n",
            "\n",
            " iteration=1386 bias=6.179999999999913 loss=1.2846666666666848\n",
            "\n",
            " iteration=1387 bias=6.179999999999913 loss=1.2836500000000781\n",
            "\n",
            " iteration=1388 bias=6.1899999999999125 loss=1.2766500000000693\n",
            "\n",
            " iteration=1389 bias=6.199999999999912 loss=1.269850000000061\n",
            "\n",
            " iteration=1390 bias=6.209999999999912 loss=1.2632500000000482\n",
            "\n",
            " iteration=1391 bias=6.219999999999912 loss=1.2568500000000384\n",
            "\n",
            " iteration=1392 bias=6.229999999999912 loss=1.2506500000000298\n",
            "\n",
            " iteration=1393 bias=6.239999999999911 loss=1.2446500000000211\n",
            "\n",
            " iteration=1394 bias=6.239999999999911 loss=1.243666666666748\n",
            "\n",
            " iteration=1395 bias=6.249999999999911 loss=1.2367666666667367\n",
            "\n",
            " iteration=1396 bias=6.259999999999911 loss=1.2300666666667257\n",
            "\n",
            " iteration=1397 bias=6.269999999999911 loss=1.223566666666717\n",
            "\n",
            " iteration=1398 bias=6.2799999999999105 loss=1.217266666666708\n",
            "\n",
            " iteration=1399 bias=6.28999999999991 loss=1.2111666666666971\n",
            "\n",
            " iteration=1400 bias=6.29999999999991 loss=1.2052666666666862\n",
            "\n",
            " iteration=1401 bias=6.29999999999991 loss=1.2043166666667473\n",
            "\n",
            " iteration=1402 bias=6.30999999999991 loss=1.1975166666667365\n",
            "\n",
            " iteration=1403 bias=6.31999999999991 loss=1.1909166666667241\n",
            "\n",
            " iteration=1404 bias=6.3299999999999095 loss=1.1845166666667182\n",
            "\n",
            " iteration=1405 bias=6.339999999999909 loss=1.1783166666667078\n",
            "\n",
            " iteration=1406 bias=6.349999999999909 loss=1.1723166666666953\n",
            "\n",
            " iteration=1407 bias=6.359999999999909 loss=1.1665166666666844\n",
            "\n",
            " iteration=1408 bias=6.359999999999909 loss=1.1656000000000808\n",
            "\n",
            " iteration=1409 bias=6.369999999999909 loss=1.1589000000000669\n",
            "\n",
            " iteration=1410 bias=6.379999999999908 loss=1.1524000000000585\n",
            "\n",
            " iteration=1411 bias=6.389999999999908 loss=1.1461000000000499\n",
            "\n",
            " iteration=1412 bias=6.399999999999908 loss=1.1400000000000408\n",
            "\n",
            " iteration=1413 bias=6.409999999999908 loss=1.1341000000000327\n",
            "\n",
            " iteration=1414 bias=6.419999999999908 loss=1.128400000000019\n",
            "\n",
            " iteration=1415 bias=6.419999999999908 loss=1.1275166666667455\n",
            "\n",
            " iteration=1416 bias=6.429999999999907 loss=1.120916666666737\n",
            "\n",
            " iteration=1417 bias=6.439999999999907 loss=1.114516666666728\n",
            "\n",
            " iteration=1418 bias=6.449999999999907 loss=1.1083166666667157\n",
            "\n",
            " iteration=1419 bias=6.459999999999907 loss=1.1023166666667061\n",
            "\n",
            " iteration=1420 bias=6.4699999999999065 loss=1.096516666666697\n",
            "\n",
            " iteration=1421 bias=6.479999999999906 loss=1.0909166666666883\n",
            "\n",
            " iteration=1422 bias=6.479999999999906 loss=1.0900666666667485\n",
            "\n",
            " iteration=1423 bias=6.489999999999906 loss=1.08356666666674\n",
            "\n",
            " iteration=1424 bias=6.499999999999906 loss=1.0772666666667263\n",
            "\n",
            " iteration=1425 bias=6.509999999999906 loss=1.0711666666667174\n",
            "\n",
            " iteration=1426 bias=6.519999999999905 loss=1.0652666666667086\n",
            "\n",
            " iteration=1427 bias=6.529999999999905 loss=1.0595666666666947\n",
            "\n",
            " iteration=1428 bias=6.539999999999905 loss=1.0540666666666867\n",
            "\n",
            " iteration=1429 bias=6.539999999999905 loss=1.0532500000000826\n",
            "\n",
            " iteration=1430 bias=6.549999999999905 loss=1.0468500000000704\n",
            "\n",
            " iteration=1431 bias=6.559999999999905 loss=1.0406500000000596\n",
            "\n",
            " iteration=1432 bias=6.569999999999904 loss=1.0346500000000531\n",
            "\n",
            " iteration=1433 bias=6.579999999999904 loss=1.028850000000043\n",
            "\n",
            " iteration=1434 bias=6.589999999999904 loss=1.0232500000000317\n",
            "\n",
            " iteration=1435 bias=6.599999999999904 loss=1.0178500000000197\n",
            "\n",
            " iteration=1436 bias=6.599999999999904 loss=1.0170666666667476\n",
            "\n",
            " iteration=1437 bias=6.6099999999999035 loss=1.0107666666667368\n",
            "\n",
            " iteration=1438 bias=6.619999999999903 loss=1.0046666666667257\n",
            "\n",
            " iteration=1439 bias=6.629999999999903 loss=0.9987666666667168\n",
            "\n",
            " iteration=1440 bias=6.639999999999903 loss=0.9930666666667078\n",
            "\n",
            " iteration=1441 bias=6.649999999999903 loss=0.9875666666666968\n",
            "\n",
            " iteration=1442 bias=6.659999999999902 loss=0.9822666666666859\n",
            "\n",
            " iteration=1443 bias=6.659999999999902 loss=0.981516666666746\n",
            "\n",
            " iteration=1444 bias=6.669999999999902 loss=0.9753166666667371\n",
            "\n",
            " iteration=1445 bias=6.679999999999902 loss=0.9693166666667284\n",
            "\n",
            " iteration=1446 bias=6.689999999999902 loss=0.963516666666716\n",
            "\n",
            " iteration=1447 bias=6.699999999999902 loss=0.9579166666667062\n",
            "\n",
            " iteration=1448 bias=6.709999999999901 loss=0.9525166666666972\n",
            "\n",
            " iteration=1449 bias=6.719999999999901 loss=0.9473166666666882\n",
            "\n",
            " iteration=1450 bias=6.719999999999901 loss=0.9466000000000818\n",
            "\n",
            " iteration=1451 bias=6.729999999999901 loss=0.9405000000000708\n",
            "\n",
            " iteration=1452 bias=6.739999999999901 loss=0.9346000000000599\n",
            "\n",
            " iteration=1453 bias=6.7499999999999005 loss=0.928900000000051\n",
            "\n",
            " iteration=1454 bias=6.7599999999999 loss=0.9234000000000417\n",
            "\n",
            " iteration=1455 bias=6.7699999999999 loss=0.9181000000000309\n",
            "\n",
            " iteration=1456 bias=6.7799999999999 loss=0.9130000000000199\n",
            "\n",
            " iteration=1457 bias=6.7799999999999 loss=0.912316666666748\n",
            "\n",
            " iteration=1458 bias=6.7899999999999 loss=0.9063166666667373\n",
            "\n",
            " iteration=1459 bias=6.7999999999998995 loss=0.900516666666725\n",
            "\n",
            " iteration=1460 bias=6.809999999999899 loss=0.8949166666667185\n",
            "\n",
            " iteration=1461 bias=6.819999999999899 loss=0.8895166666667081\n",
            "\n",
            " iteration=1462 bias=6.829999999999899 loss=0.8843166666666958\n",
            "\n",
            " iteration=1463 bias=6.839999999999899 loss=0.8793166666666848\n",
            "\n",
            " iteration=1464 bias=6.839999999999899 loss=0.8786666666667479\n",
            "\n",
            " iteration=1465 bias=6.849999999999898 loss=0.8727666666667343\n",
            "\n",
            " iteration=1466 bias=6.859999999999898 loss=0.8670666666667257\n",
            "\n",
            " iteration=1467 bias=6.869999999999898 loss=0.8615666666667168\n",
            "\n",
            " iteration=1468 bias=6.879999999999898 loss=0.8562666666667078\n",
            "\n",
            " iteration=1469 bias=6.8899999999998975 loss=0.8511666666666994\n",
            "\n",
            " iteration=1470 bias=6.899999999999897 loss=0.8462666666666862\n",
            "\n",
            " iteration=1471 bias=6.899999999999897 loss=0.8456500000000794\n",
            "\n",
            " iteration=1472 bias=6.909999999999897 loss=0.8398500000000704\n",
            "\n",
            " iteration=1473 bias=6.919999999999897 loss=0.8342500000000613\n",
            "\n",
            " iteration=1474 bias=6.929999999999897 loss=0.828850000000049\n",
            "\n",
            " iteration=1475 bias=6.9399999999998965 loss=0.8236500000000396\n",
            "\n",
            " iteration=1476 bias=6.949999999999896 loss=0.8186500000000304\n",
            "\n",
            " iteration=1477 bias=6.959999999999896 loss=0.8138500000000212\n",
            "\n",
            " iteration=1478 bias=6.959999999999896 loss=0.8132666666667484\n",
            "\n",
            " iteration=1479 bias=6.969999999999896 loss=0.8075666666667395\n",
            "\n",
            " iteration=1480 bias=6.979999999999896 loss=0.8020666666667262\n",
            "\n",
            " iteration=1481 bias=6.989999999999895 loss=0.7967666666667172\n",
            "\n",
            " iteration=1482 bias=6.999999999999895 loss=0.7916666666667082\n",
            "\n",
            " iteration=1483 bias=7.009999999999895 loss=0.7867666666666949\n",
            "\n",
            " iteration=1484 bias=7.019999999999895 loss=0.7820666666666863\n",
            "\n",
            " iteration=1485 bias=7.019999999999895 loss=0.7815166666667487\n",
            "\n",
            " iteration=1486 bias=7.0299999999998946 loss=0.7759166666667366\n",
            "\n",
            " iteration=1487 bias=7.039999999999894 loss=0.7705166666667259\n",
            "\n",
            " iteration=1488 bias=7.049999999999894 loss=0.7653166666667192\n",
            "\n",
            " iteration=1489 bias=7.059999999999894 loss=0.7603166666667088\n",
            "\n",
            " iteration=1490 bias=7.069999999999894 loss=0.7555166666666979\n",
            "\n",
            " iteration=1491 bias=7.0799999999998935 loss=0.7509166666666859\n",
            "\n",
            " iteration=1492 bias=7.0799999999998935 loss=0.7504000000000807\n",
            "\n",
            " iteration=1493 bias=7.089999999999893 loss=0.7449000000000696\n",
            "\n",
            " iteration=1494 bias=7.099999999999893 loss=0.7396000000000588\n",
            "\n",
            " iteration=1495 bias=7.109999999999893 loss=0.7345000000000496\n",
            "\n",
            " iteration=1496 bias=7.119999999999893 loss=0.7296000000000405\n",
            "\n",
            " iteration=1497 bias=7.129999999999892 loss=0.7249000000000297\n",
            "\n",
            " iteration=1498 bias=7.139999999999892 loss=0.7204000000000189\n",
            "\n",
            " iteration=1499 bias=7.139999999999892 loss=0.7199166666667454\n",
            "\n",
            " iteration=1500 bias=7.149999999999892 loss=0.7145166666667365\n",
            "\n",
            " iteration=1501 bias=7.159999999999892 loss=0.7093166666667274\n",
            "\n",
            " iteration=1502 bias=7.169999999999892 loss=0.7043166666667154\n",
            "\n",
            " iteration=1503 bias=7.179999999999891 loss=0.6995166666667055\n",
            "\n",
            " iteration=1504 bias=7.189999999999891 loss=0.6949166666666963\n",
            "\n",
            " iteration=1505 bias=7.199999999999891 loss=0.6905166666666872\n",
            "\n",
            " iteration=1506 bias=7.199999999999891 loss=0.6900666666667475\n",
            "\n",
            " iteration=1507 bias=7.209999999999891 loss=0.6847666666667366\n",
            "\n",
            " iteration=1508 bias=7.2199999999998905 loss=0.6796666666667256\n",
            "\n",
            " iteration=1509 bias=7.22999999999989 loss=0.6747666666667166\n",
            "\n",
            " iteration=1510 bias=7.23999999999989 loss=0.6700666666667073\n",
            "\n",
            " iteration=1511 bias=7.24999999999989 loss=0.6655666666666964\n",
            "\n",
            " iteration=1512 bias=7.25999999999989 loss=0.6612666666666857\n",
            "\n",
            " iteration=1513 bias=7.25999999999989 loss=0.6608500000000802\n",
            "\n",
            " iteration=1514 bias=7.269999999999889 loss=0.6556500000000695\n",
            "\n",
            " iteration=1515 bias=7.279999999999889 loss=0.6506500000000576\n",
            "\n",
            " iteration=1516 bias=7.289999999999889 loss=0.6458500000000503\n",
            "\n",
            " iteration=1517 bias=7.299999999999889 loss=0.6412500000000401\n",
            "\n",
            " iteration=1518 bias=7.309999999999889 loss=0.6368500000000282\n",
            "\n",
            " iteration=1519 bias=7.319999999999888 loss=0.6326500000000173\n",
            "\n",
            " iteration=1520 bias=7.319999999999888 loss=0.6322666666667462\n",
            "\n",
            " iteration=1521 bias=7.329999999999888 loss=0.6271666666667335\n",
            "\n",
            " iteration=1522 bias=7.339999999999888 loss=0.6222666666667246\n",
            "\n",
            " iteration=1523 bias=7.349999999999888 loss=0.6175666666667153\n",
            "\n",
            " iteration=1524 bias=7.3599999999998875 loss=0.6130666666667063\n",
            "\n",
            " iteration=1525 bias=7.369999999999887 loss=0.6087666666666974\n",
            "\n",
            " iteration=1526 bias=7.379999999999887 loss=0.6046666666666846\n",
            "\n",
            " iteration=1527 bias=7.379999999999887 loss=0.6043166666667446\n",
            "\n",
            " iteration=1528 bias=7.389999999999887 loss=0.5993166666667354\n",
            "\n",
            " iteration=1529 bias=7.399999999999887 loss=0.5945166666667263\n",
            "\n",
            " iteration=1530 bias=7.4099999999998865 loss=0.5899166666667144\n",
            "\n",
            " iteration=1531 bias=7.419999999999886 loss=0.5855166666667045\n",
            "\n",
            " iteration=1532 bias=7.429999999999886 loss=0.5813166666666953\n",
            "\n",
            " iteration=1533 bias=7.439999999999886 loss=0.577316666666686\n",
            "\n",
            " iteration=1534 bias=7.439999999999886 loss=0.5770000000000797\n",
            "\n",
            " iteration=1535 bias=7.449999999999886 loss=0.5721000000000707\n",
            "\n",
            " iteration=1536 bias=7.459999999999885 loss=0.5674000000000579\n",
            "\n",
            " iteration=1537 bias=7.469999999999885 loss=0.5629000000000487\n",
            "\n",
            " iteration=1538 bias=7.479999999999885 loss=0.5586000000000394\n",
            "\n",
            " iteration=1539 bias=7.489999999999885 loss=0.5545000000000266\n",
            "\n",
            " iteration=1540 bias=7.4999999999998845 loss=0.5506000000000179\n",
            "\n",
            " iteration=1541 bias=7.4999999999998845 loss=0.5503166666667464\n",
            "\n",
            " iteration=1542 bias=7.509999999999884 loss=0.5455166666667347\n",
            "\n",
            " iteration=1543 bias=7.519999999999884 loss=0.540916666666724\n",
            "\n",
            " iteration=1544 bias=7.529999999999884 loss=0.5365166666667166\n",
            "\n",
            " iteration=1545 bias=7.539999999999884 loss=0.5323166666667064\n",
            "\n",
            " iteration=1546 bias=7.5499999999998835 loss=0.5283166666666955\n",
            "\n",
            " iteration=1547 bias=7.559999999999883 loss=0.5245166666666838\n",
            "\n",
            " iteration=1548 bias=7.559999999999883 loss=0.5242666666667449\n",
            "\n",
            " iteration=1549 bias=7.569999999999883 loss=0.5195666666667341\n",
            "\n",
            " iteration=1550 bias=7.579999999999883 loss=0.5150666666667233\n",
            "\n",
            " iteration=1551 bias=7.589999999999883 loss=0.510766666666714\n",
            "\n",
            " iteration=1552 bias=7.599999999999882 loss=0.5066666666667048\n",
            "\n",
            " iteration=1553 bias=7.609999999999882 loss=0.502766666666694\n",
            "\n",
            " iteration=1554 bias=7.619999999999882 loss=0.4990666666666834\n",
            "\n",
            " iteration=1555 bias=7.619999999999882 loss=0.49885000000007657\n",
            "\n",
            " iteration=1556 bias=7.629999999999882 loss=0.4942500000000673\n",
            "\n",
            " iteration=1557 bias=7.6399999999998816 loss=0.4898500000000581\n",
            "\n",
            " iteration=1558 bias=7.649999999999881 loss=0.4856500000000463\n",
            "\n",
            " iteration=1559 bias=7.659999999999881 loss=0.48165000000003655\n",
            "\n",
            " iteration=1560 bias=7.669999999999881 loss=0.47785000000002714\n",
            "\n",
            " iteration=1561 bias=7.679999999999881 loss=0.47425000000001766\n",
            "\n",
            " iteration=1562 bias=7.679999999999881 loss=0.47406666666674463\n",
            "\n",
            " iteration=1563 bias=7.6899999999998805 loss=0.46956666666673375\n",
            "\n",
            " iteration=1564 bias=7.69999999999988 loss=0.46526666666672295\n",
            "\n",
            " iteration=1565 bias=7.70999999999988 loss=0.46116666666671374\n",
            "\n",
            " iteration=1566 bias=7.71999999999988 loss=0.4572666666667044\n",
            "\n",
            " iteration=1567 bias=7.72999999999988 loss=0.45356666666669354\n",
            "\n",
            " iteration=1568 bias=7.739999999999879 loss=0.4500666666666829\n",
            "\n",
            " iteration=1569 bias=7.739999999999879 loss=0.4499166666667438\n",
            "\n",
            " iteration=1570 bias=7.749999999999879 loss=0.4455166666667332\n",
            "\n",
            " iteration=1571 bias=7.759999999999879 loss=0.4413166666667216\n",
            "\n",
            " iteration=1572 bias=7.769999999999879 loss=0.4373166666667138\n",
            "\n",
            " iteration=1573 bias=7.779999999999879 loss=0.4335166666667036\n",
            "\n",
            " iteration=1574 bias=7.789999999999878 loss=0.42991666666669204\n",
            "\n",
            " iteration=1575 bias=7.799999999999878 loss=0.4265166666666813\n",
            "\n",
            " iteration=1576 bias=7.799999999999878 loss=0.4264000000000765\n",
            "\n",
            " iteration=1577 bias=7.809999999999878 loss=0.4221000000000641\n",
            "\n",
            " iteration=1578 bias=7.819999999999878 loss=0.418000000000055\n",
            "\n",
            " iteration=1579 bias=7.8299999999998775 loss=0.41410000000004565\n",
            "\n",
            " iteration=1580 bias=7.839999999999877 loss=0.41040000000003624\n",
            "\n",
            " iteration=1581 bias=7.849999999999877 loss=0.4069000000000273\n",
            "\n",
            " iteration=1582 bias=7.859999999999877 loss=0.4036000000000149\n",
            "\n",
            " iteration=1583 bias=7.859999999999877 loss=0.4035166666667414\n",
            "\n",
            " iteration=1584 bias=7.869999999999877 loss=0.3993166666667321\n",
            "\n",
            " iteration=1585 bias=7.879999999999876 loss=0.3953166666667227\n",
            "\n",
            " iteration=1586 bias=7.889999999999876 loss=0.39151666666671114\n",
            "\n",
            " iteration=1587 bias=7.899999999999876 loss=0.3879166666667013\n",
            "\n",
            " iteration=1588 bias=7.909999999999876 loss=0.3845166666666919\n",
            "\n",
            " iteration=1589 bias=7.919999999999876 loss=0.3813166666666823\n",
            "\n",
            " iteration=1590 bias=7.919999999999876 loss=0.38126666666674264\n",
            "\n",
            " iteration=1591 bias=7.929999999999875 loss=0.37716666666673343\n",
            "\n",
            " iteration=1592 bias=7.939999999999875 loss=0.37326666666672104\n",
            "\n",
            " iteration=1593 bias=7.949999999999875 loss=0.3695666666667117\n",
            "\n",
            " iteration=1594 bias=7.959999999999875 loss=0.36606666666670223\n",
            "\n",
            " iteration=1595 bias=7.9699999999998745 loss=0.36276666666668983\n",
            "\n",
            " iteration=1596 bias=7.979999999999874 loss=0.35966666666668085\n",
            "\n",
            " iteration=1597 bias=7.979999999999874 loss=0.3596500000000757\n",
            "\n",
            " iteration=1598 bias=7.989999999999874 loss=0.35565000000006425\n",
            "\n",
            " iteration=1599 bias=7.999999999999874 loss=0.35185000000005373\n",
            "\n",
            " iteration=1600 bias=8.009999999999874 loss=0.34825000000004575\n",
            "\n",
            " iteration=1601 bias=8.019999999999873 loss=0.34485000000003546\n",
            "\n",
            " iteration=1602 bias=8.029999999999873 loss=0.3416500000000249\n",
            "\n",
            " iteration=1603 bias=8.039999999999873 loss=0.3386500000000134\n",
            "\n",
            " iteration=1604 bias=8.049999999999873 loss=0.3358500000000031\n",
            "\n",
            " iteration=1605 bias=8.049999999999873 loss=0.33476666666673144\n",
            "\n",
            " iteration=1606 bias=8.059999999999873 loss=0.33106666666672074\n",
            "\n",
            " iteration=1607 bias=8.069999999999872 loss=0.32756666666671014\n",
            "\n",
            " iteration=1608 bias=8.079999999999872 loss=0.32426666666670056\n",
            "\n",
            " iteration=1609 bias=8.089999999999872 loss=0.3211666666666911\n",
            "\n",
            " iteration=1610 bias=8.099999999999872 loss=0.3182666666666804\n",
            "\n",
            " iteration=1611 bias=8.109999999999872 loss=0.3155666666666698\n",
            "\n",
            " iteration=1612 bias=8.109999999999872 loss=0.31451666666672984\n",
            "\n",
            " iteration=1613 bias=8.119999999999871 loss=0.31091666666672035\n",
            "\n",
            " iteration=1614 bias=8.129999999999871 loss=0.307516666666709\n",
            "\n",
            " iteration=1615 bias=8.139999999999871 loss=0.30431666666669904\n",
            "\n",
            " iteration=1616 bias=8.14999999999987 loss=0.3013166666666895\n",
            "\n",
            " iteration=1617 bias=8.15999999999987 loss=0.29851666666667986\n",
            "\n",
            " iteration=1618 bias=8.16999999999987 loss=0.29591666666667016\n",
            "\n",
            " iteration=1619 bias=8.16999999999987 loss=0.29490000000006156\n",
            "\n",
            " iteration=1620 bias=8.17999999999987 loss=0.29140000000005206\n",
            "\n",
            " iteration=1621 bias=8.18999999999987 loss=0.2881000000000425\n",
            "\n",
            " iteration=1622 bias=8.19999999999987 loss=0.2850000000000319\n",
            "\n",
            " iteration=1623 bias=8.20999999999987 loss=0.2821000000000213\n",
            "\n",
            " iteration=1624 bias=8.21999999999987 loss=0.27940000000001175\n",
            "\n",
            " iteration=1625 bias=8.229999999999869 loss=0.2769000000000022\n",
            "\n",
            " iteration=1626 bias=8.229999999999869 loss=0.2759166666667286\n",
            "\n",
            " iteration=1627 bias=8.239999999999869 loss=0.2725166666667172\n",
            "\n",
            " iteration=1628 bias=8.249999999999869 loss=0.269316666666709\n",
            "\n",
            " iteration=1629 bias=8.259999999999868 loss=0.26631666666669873\n",
            "\n",
            " iteration=1630 bias=8.269999999999868 loss=0.26351666666668744\n",
            "\n",
            " iteration=1631 bias=8.279999999999868 loss=0.2609166666666768\n",
            "\n",
            " iteration=1632 bias=8.289999999999868 loss=0.2585166666666665\n",
            "\n",
            " iteration=1633 bias=8.289999999999868 loss=0.25756666666672734\n",
            "\n",
            " iteration=1634 bias=8.299999999999867 loss=0.254266666666718\n",
            "\n",
            " iteration=1635 bias=8.309999999999867 loss=0.25116666666670745\n",
            "\n",
            " iteration=1636 bias=8.319999999999867 loss=0.24826666666669783\n",
            "\n",
            " iteration=1637 bias=8.329999999999867 loss=0.24556666666668958\n",
            "\n",
            " iteration=1638 bias=8.339999999999867 loss=0.24306666666667773\n",
            "\n",
            " iteration=1639 bias=8.349999999999866 loss=0.24076666666666713\n",
            "\n",
            " iteration=1640 bias=8.349999999999866 loss=0.2398500000000604\n",
            "\n",
            " iteration=1641 bias=8.359999999999866 loss=0.23665000000005088\n",
            "\n",
            " iteration=1642 bias=8.369999999999866 loss=0.23365000000003958\n",
            "\n",
            " iteration=1643 bias=8.379999999999866 loss=0.23085000000002964\n",
            "\n",
            " iteration=1644 bias=8.389999999999866 loss=0.22825000000002005\n",
            "\n",
            " iteration=1645 bias=8.399999999999865 loss=0.2258500000000103\n",
            "\n",
            " iteration=1646 bias=8.409999999999865 loss=0.22365000000000046\n",
            "\n",
            " iteration=1647 bias=8.409999999999865 loss=0.22276666666672673\n",
            "\n",
            " iteration=1648 bias=8.419999999999865 loss=0.2196666666667159\n",
            "\n",
            " iteration=1649 bias=8.429999999999865 loss=0.2167666666667063\n",
            "\n",
            " iteration=1650 bias=8.439999999999864 loss=0.21406666666669574\n",
            "\n",
            " iteration=1651 bias=8.449999999999864 loss=0.21156666666668392\n",
            "\n",
            " iteration=1652 bias=8.459999999999864 loss=0.2092666666666755\n",
            "\n",
            " iteration=1653 bias=8.469999999999864 loss=0.20716666666666586\n",
            "\n",
            " iteration=1654 bias=8.469999999999864 loss=0.20631666666672543\n",
            "\n",
            " iteration=1655 bias=8.479999999999864 loss=0.203316666666715\n",
            "\n",
            " iteration=1656 bias=8.489999999999863 loss=0.2005166666667065\n",
            "\n",
            " iteration=1657 bias=8.499999999999863 loss=0.19791666666669627\n",
            "\n",
            " iteration=1658 bias=8.509999999999863 loss=0.19551666666668574\n",
            "\n",
            " iteration=1659 bias=8.519999999999863 loss=0.19331666666667457\n",
            "\n",
            " iteration=1660 bias=8.529999999999863 loss=0.19131666666666428\n",
            "\n",
            " iteration=1661 bias=8.529999999999863 loss=0.19050000000005882\n",
            "\n",
            " iteration=1662 bias=8.539999999999862 loss=0.1876000000000482\n",
            "\n",
            " iteration=1663 bias=8.549999999999862 loss=0.18490000000003773\n",
            "\n",
            " iteration=1664 bias=8.559999999999862 loss=0.182400000000028\n",
            "\n",
            " iteration=1665 bias=8.569999999999862 loss=0.18010000000001836\n",
            "\n",
            " iteration=1666 bias=8.579999999999862 loss=0.17800000000000782\n",
            "\n",
            " iteration=1667 bias=8.589999999999861 loss=0.17609999999999726\n",
            "\n",
            " iteration=1668 bias=8.589999999999861 loss=0.17531666666672388\n",
            "\n",
            " iteration=1669 bias=8.599999999999861 loss=0.17251666666671425\n",
            "\n",
            " iteration=1670 bias=8.60999999999986 loss=0.16991666666670321\n",
            "\n",
            " iteration=1671 bias=8.61999999999986 loss=0.16751666666669318\n",
            "\n",
            " iteration=1672 bias=8.62999999999986 loss=0.16531666666668343\n",
            "\n",
            " iteration=1673 bias=8.63999999999986 loss=0.16331666666667366\n",
            "\n",
            " iteration=1674 bias=8.64999999999986 loss=0.16151666666666387\n",
            "\n",
            " iteration=1675 bias=8.64999999999986 loss=0.16076666666672232\n",
            "\n",
            " iteration=1676 bias=8.65999999999986 loss=0.15806666666671262\n",
            "\n",
            " iteration=1677 bias=8.66999999999986 loss=0.15556666666670296\n",
            "\n",
            " iteration=1678 bias=8.67999999999986 loss=0.15326666666669245\n",
            "\n",
            " iteration=1679 bias=8.68999999999986 loss=0.1511666666666819\n",
            "\n",
            " iteration=1680 bias=8.699999999999859 loss=0.14926666666667218\n",
            "\n",
            " iteration=1681 bias=8.709999999999859 loss=0.14756666666666243\n",
            "\n",
            " iteration=1682 bias=8.709999999999859 loss=0.14685000000005555\n",
            "\n",
            " iteration=1683 bias=8.719999999999859 loss=0.1442500000000445\n",
            "\n",
            " iteration=1684 bias=8.729999999999858 loss=0.14185000000003575\n",
            "\n",
            " iteration=1685 bias=8.739999999999858 loss=0.1396500000000255\n",
            "\n",
            " iteration=1686 bias=8.749999999999858 loss=0.13765000000001448\n",
            "\n",
            " iteration=1687 bias=8.759999999999858 loss=0.13585000000000394\n",
            "\n",
            " iteration=1688 bias=8.769999999999857 loss=0.13424999999999365\n",
            "\n",
            " iteration=1689 bias=8.769999999999857 loss=0.13356666666672093\n",
            "\n",
            " iteration=1690 bias=8.779999999999857 loss=0.13106666666671132\n",
            "\n",
            " iteration=1691 bias=8.789999999999857 loss=0.1287666666667009\n",
            "\n",
            " iteration=1692 bias=8.799999999999857 loss=0.1266666666666911\n",
            "\n",
            " iteration=1693 bias=8.809999999999857 loss=0.12476666666668228\n",
            "\n",
            " iteration=1694 bias=8.819999999999856 loss=0.12306666666667088\n",
            "\n",
            " iteration=1695 bias=8.829999999999856 loss=0.12156666666666038\n",
            "\n",
            " iteration=1696 bias=8.829999999999856 loss=0.12091666666672025\n",
            "\n",
            " iteration=1697 bias=8.839999999999856 loss=0.11851666666671057\n",
            "\n",
            " iteration=1698 bias=8.849999999999856 loss=0.1163166666666996\n",
            "\n",
            " iteration=1699 bias=8.859999999999856 loss=0.11431666666668959\n",
            "\n",
            " iteration=1700 bias=8.869999999999855 loss=0.11251666666667977\n",
            "\n",
            " iteration=1701 bias=8.879999999999855 loss=0.11091666666666992\n",
            "\n",
            " iteration=1702 bias=8.889999999999855 loss=0.10951666666665993\n",
            "\n",
            " iteration=1703 bias=8.889999999999855 loss=0.1089000000000529\n",
            "\n",
            " iteration=1704 bias=8.899999999999855 loss=0.10660000000004227\n",
            "\n",
            " iteration=1705 bias=8.909999999999854 loss=0.10450000000003253\n",
            "\n",
            " iteration=1706 bias=8.919999999999854 loss=0.10260000000002206\n",
            "\n",
            " iteration=1707 bias=8.929999999999854 loss=0.10090000000001074\n",
            "\n",
            " iteration=1708 bias=8.939999999999854 loss=0.09940000000000182\n",
            "\n",
            " iteration=1709 bias=8.949999999999854 loss=0.09809999999999193\n",
            "\n",
            " iteration=1710 bias=8.949999999999854 loss=0.09751666666671825\n",
            "\n",
            " iteration=1711 bias=8.959999999999853 loss=0.0953166666667079\n",
            "\n",
            " iteration=1712 bias=8.969999999999853 loss=0.09331666666669886\n",
            "\n",
            " iteration=1713 bias=8.979999999999853 loss=0.09151666666668866\n",
            "\n",
            " iteration=1714 bias=8.989999999999853 loss=0.08991666666667819\n",
            "\n",
            " iteration=1715 bias=8.999999999999853 loss=0.08851666666666734\n",
            "\n",
            " iteration=1716 bias=9.009999999999852 loss=0.08731666666665705\n",
            "\n",
            " iteration=1717 bias=9.009999999999852 loss=0.08676666666671784\n",
            "\n",
            " iteration=1718 bias=9.019999999999852 loss=0.08466666666670732\n",
            "\n",
            " iteration=1719 bias=9.029999999999852 loss=0.08276666666669695\n",
            "\n",
            " iteration=1720 bias=9.039999999999852 loss=0.08106666666668709\n",
            "\n",
            " iteration=1721 bias=9.049999999999851 loss=0.07956666666667724\n",
            "\n",
            " iteration=1722 bias=9.059999999999851 loss=0.07826666666666682\n",
            "\n",
            " iteration=1723 bias=9.069999999999851 loss=0.07716666666665638\n",
            "\n",
            " iteration=1724 bias=9.069999999999851 loss=0.07665000000004958\n",
            "\n",
            " iteration=1725 bias=9.07999999999985 loss=0.07465000000003978\n",
            "\n",
            " iteration=1726 bias=9.08999999999985 loss=0.07285000000002902\n",
            "\n",
            " iteration=1727 bias=9.09999999999985 loss=0.07125000000001891\n",
            "\n",
            " iteration=1728 bias=9.10999999999985 loss=0.069850000000009\n",
            "\n",
            " iteration=1729 bias=9.11999999999985 loss=0.06864999999999905\n",
            "\n",
            " iteration=1730 bias=9.12999999999985 loss=0.06764999999998918\n",
            "\n",
            " iteration=1731 bias=9.12999999999985 loss=0.06716666666671471\n",
            "\n",
            " iteration=1732 bias=9.13999999999985 loss=0.06526666666670483\n",
            "\n",
            " iteration=1733 bias=9.14999999999985 loss=0.063566666666695\n",
            "\n",
            " iteration=1734 bias=9.15999999999985 loss=0.06206666666668459\n",
            "\n",
            " iteration=1735 bias=9.169999999999849 loss=0.060766666666674164\n",
            "\n",
            " iteration=1736 bias=9.179999999999849 loss=0.059666666666664286\n",
            "\n",
            " iteration=1737 bias=9.189999999999849 loss=0.058766666666654345\n",
            "\n",
            " iteration=1738 bias=9.189999999999849 loss=0.058316666666714125\n",
            "\n",
            " iteration=1739 bias=9.199999999999848 loss=0.056516666666703436\n",
            "\n",
            " iteration=1740 bias=9.209999999999848 loss=0.05491666666669409\n",
            "\n",
            " iteration=1741 bias=9.219999999999848 loss=0.05351666666668386\n",
            "\n",
            " iteration=1742 bias=9.229999999999848 loss=0.05231666666667315\n",
            "\n",
            " iteration=1743 bias=9.239999999999847 loss=0.051316666666662715\n",
            "\n",
            " iteration=1744 bias=9.249999999999847 loss=0.05051666666665242\n",
            "\n",
            " iteration=1745 bias=9.249999999999847 loss=0.05010000000004611\n",
            "\n",
            " iteration=1746 bias=9.259999999999847 loss=0.04840000000003623\n",
            "\n",
            " iteration=1747 bias=9.269999999999847 loss=0.046900000000025914\n",
            "\n",
            " iteration=1748 bias=9.279999999999847 loss=0.04560000000001596\n",
            "\n",
            " iteration=1749 bias=9.289999999999846 loss=0.04450000000000658\n",
            "\n",
            " iteration=1750 bias=9.299999999999846 loss=0.04359999999999565\n",
            "\n",
            " iteration=1751 bias=9.309999999999846 loss=0.042899999999985276\n",
            "\n",
            " iteration=1752 bias=9.309999999999846 loss=0.04251666666671178\n",
            "\n",
            " iteration=1753 bias=9.319999999999846 loss=0.04091666666670189\n",
            "\n",
            " iteration=1754 bias=9.329999999999846 loss=0.039516666666691215\n",
            "\n",
            " iteration=1755 bias=9.339999999999845 loss=0.03831666666668112\n",
            "\n",
            " iteration=1756 bias=9.349999999999845 loss=0.03731666666667115\n",
            "\n",
            " iteration=1757 bias=9.359999999999845 loss=0.03651666666666109\n",
            "\n",
            " iteration=1758 bias=9.369999999999845 loss=0.03591666666665104\n",
            "\n",
            " iteration=1759 bias=9.369999999999845 loss=0.03556666666671072\n",
            "\n",
            " iteration=1760 bias=9.379999999999844 loss=0.034066666666700295\n",
            "\n",
            " iteration=1761 bias=9.389999999999844 loss=0.03276666666669036\n",
            "\n",
            " iteration=1762 bias=9.399999999999844 loss=0.031666666666680006\n",
            "\n",
            " iteration=1763 bias=9.409999999999844 loss=0.030766666666669173\n",
            "\n",
            " iteration=1764 bias=9.419999999999844 loss=0.030066666666659678\n",
            "\n",
            " iteration=1765 bias=9.429999999999843 loss=0.029566666666649644\n",
            "\n",
            " iteration=1766 bias=9.429999999999843 loss=0.029250000000042645\n",
            "\n",
            " iteration=1767 bias=9.439999999999843 loss=0.0278500000000324\n",
            "\n",
            " iteration=1768 bias=9.449999999999843 loss=0.026650000000022805\n",
            "\n",
            " iteration=1769 bias=9.459999999999843 loss=0.025650000000012604\n",
            "\n",
            " iteration=1770 bias=9.469999999999843 loss=0.024850000000002263\n",
            "\n",
            " iteration=1771 bias=9.479999999999842 loss=0.0242499999999917\n",
            "\n",
            " iteration=1772 bias=9.489999999999842 loss=0.023849999999981452\n",
            "\n",
            " iteration=1773 bias=9.489999999999842 loss=0.02356666666670844\n",
            "\n",
            " iteration=1774 bias=9.499999999999842 loss=0.022266666666698052\n",
            "\n",
            " iteration=1775 bias=9.509999999999842 loss=0.02116666666668778\n",
            "\n",
            " iteration=1776 bias=9.519999999999841 loss=0.02026666666667774\n",
            "\n",
            " iteration=1777 bias=9.529999999999841 loss=0.019566666666667704\n",
            "\n",
            " iteration=1778 bias=9.539999999999841 loss=0.019066666666657402\n",
            "\n",
            " iteration=1779 bias=9.54999999999984 loss=0.018766666666647072\n",
            "\n",
            " iteration=1780 bias=9.54999999999984 loss=0.018516666666706882\n",
            "\n",
            " iteration=1781 bias=9.55999999999984 loss=0.01731666666669691\n",
            "\n",
            " iteration=1782 bias=9.56999999999984 loss=0.01631666666668643\n",
            "\n",
            " iteration=1783 bias=9.57999999999984 loss=0.01551666666667624\n",
            "\n",
            " iteration=1784 bias=9.58999999999984 loss=0.014916666666666176\n",
            "\n",
            " iteration=1785 bias=9.59999999999984 loss=0.014516666666656033\n",
            "\n",
            " iteration=1786 bias=9.60999999999984 loss=0.014316666666646084\n",
            "\n",
            " iteration=1787 bias=9.60999999999984 loss=0.014100000000038693\n",
            "\n",
            " iteration=1788 bias=9.61999999999984 loss=0.013000000000028648\n",
            "\n",
            " iteration=1789 bias=9.62999999999984 loss=0.012100000000018632\n",
            "\n",
            " iteration=1790 bias=9.639999999999839 loss=0.011400000000008325\n",
            "\n",
            " iteration=1791 bias=9.649999999999839 loss=0.01089999999999802\n",
            "\n",
            " iteration=1792 bias=9.659999999999838 loss=0.010599999999987966\n",
            "\n",
            " iteration=1793 bias=9.669999999999838 loss=0.010499999999977846\n",
            "\n",
            " iteration=1794 bias=9.669999999999838 loss=0.010316666666704341\n",
            "\n",
            " iteration=1795 bias=9.679999999999838 loss=0.009316666666693939\n",
            "\n",
            " iteration=1796 bias=9.689999999999838 loss=0.008516666666684037\n",
            "\n",
            " iteration=1797 bias=9.699999999999838 loss=0.007916666666673821\n",
            "\n",
            " iteration=1798 bias=9.709999999999837 loss=0.0075166666666634145\n",
            "\n",
            " iteration=1799 bias=9.719999999999837 loss=0.007316666666653073\n",
            "\n",
            " iteration=1800 bias=9.729999999999837 loss=0.0073166666666428055\n",
            "\n",
            " iteration=1801 bias=9.729999999999837 loss=0.007166666666702907\n",
            "\n",
            " iteration=1802 bias=9.739999999999837 loss=0.006266666666692761\n",
            "\n",
            " iteration=1803 bias=9.749999999999837 loss=0.005566666666682547\n",
            "\n",
            " iteration=1804 bias=9.759999999999836 loss=0.00506666666667242\n",
            "\n",
            " iteration=1805 bias=9.769999999999836 loss=0.004766666666662484\n",
            "\n",
            " iteration=1806 bias=9.779999999999836 loss=0.004666666666652043\n",
            "\n",
            "w=5.039999999999937, b=9.779999999999836\n",
            "Prediction: x=8, Y=>50.09999999999933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "showHistory(X,Y, history,5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "ZC4rlg-Cah8r",
        "outputId": "dffe4276-57f3-4998-8842-fc46309bb5d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Prediction: x=8, Y=>{predict(8, w,b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyXje8gq-kSd",
        "outputId": "1a38e7d8-7777-40f4-90ce-e1167d67ced8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: x=8, Y=>50.09999999999933\n"
          ]
        }
      ]
    }
  ]
}